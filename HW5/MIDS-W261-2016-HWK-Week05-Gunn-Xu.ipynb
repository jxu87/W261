{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DATASCI W261: Machine Learning at Scale\n",
    "\n",
    "## Angela Gunn &  Jing Xu \n",
    "### angela@egunn.com  \n",
    "### jaling@gmail.com\n",
    "\n",
    "## MIDS - W261 - Section 3  \n",
    "## February 17, 2016  \n",
    "## Assignment : Week 5\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "----\n",
    "## HW 5.0\n",
    "**What is a data warehouse?** \n",
    "\n",
    "A data warehouse is a central repository of integrated data from one or many sources used for reporting and data analysis. In an enterprise setting, it serves as the primary repository of data from sales transactions to product inventories. Modern data warehouses can store:\n",
    "\n",
    "* relational data\n",
    "* semi-structured data like query logs\n",
    "* unstructured data like tweets, titles of web pages\n",
    "\n",
    "Data warehouses form a foundation for business intelligence and data science, and are leveraged to gain a competitive advantage in the marketplace through data mining.\n",
    "\n",
    "**What is a Star schema? When is it used?**\n",
    "\n",
    "A Star Schema is a type of data mart schema that consists of one or more fact tables referencing any number of dimension tables. It gets its name from the tendency of the physical model to resemble a star with the fact table in the center and dimension tables surrounding it.\n",
    "<img src=starschema.png>\n",
    "It is used to handle simpler queries as the join logic is usually simpler to handle than the join logic needed to retrieve data from a highly normalized transactional schemas. There is also more simplified business reporting logic, query performance gains, and faster aggregations compared to other schemas."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-----\n",
    "## HW 5.1\n",
    "**In the database world What is 3NF?**\n",
    "\n",
    "3NF is third normal form, the third step in normalizing a database and builds on the first and second normal forms. It is a normalization process used to reduce the unnecessary duplication of data and making sure every value of of attribute column of a table exists as a value of another attribute column in the same or different table (referential integrity). This is done by ensuring that the data is in second normal form and that all the attributes in the table are determined only by the candidate keys of the table and not by any other non-prime attributes. 3NF is used to improve processing while minimizing storage costs, which is ideal for online transaction processing (OLTP) applications.\n",
    "\n",
    "**Does machine learning use data in 3NF? If so why?**\n",
    "\n",
    "3NF is often used for machine learning - 3NF's structure is ideal for machine processing. The removal of transitive functional dependency avoids inputting features for machine learning that are redundant and non-independent. Relational databases almost always contain structured, normalized data with indexes, and machine learning in this realm uses 3NF data inputs.\n",
    "\n",
    "**In what form does ML consume data?**\n",
    "\n",
    "Although 3NF works well for ML, ML consumes data in a variety of additional forms. Not all ML applications depend on data normalization - 1NF and 2NF can be fed into ML algorithms, although the effectiveness will vary depending on the algorithm and type of data being worked with. ML algorithms are now sophisticated enough to learn from unstructured data as well.\n",
    "\n",
    "**Why would one use log files that are denormalized?**\n",
    "\n",
    "Denormalized log files can be more efficient for queries that draw information from several tables that are stored on disk and require complex joins to complete, depending on the size and type of data. Denormalization is the process of attempting to optimize the read performance of a database by adding redundant data or by grouping data - it is a tradeoff of redundancy/extra space for scalability and read performance. Hadoop MapReduce for example primarily uses denormalized data that is fully contained in a single record to avoid costly joins.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-----\n",
    "## HW 5.2\n",
    "Using MRJob, implement a hashside join (memory-backed map-side) for left, \n",
    "right and inner joins. Run your code on the  data used in HW 4.4: (Recall HW 4.4: Find the most frequent visitor of each page using mrjob and the output of 4.2  (i.e., transfromed log file). In this output please include the webpage URL, webpageID and Visitor ID.)\n",
    "\n",
    "Justify which table you chose as the Left table in this hashside join.\n",
    "\n",
    "Please report the number of rows resulting from:\n",
    "\n",
    "(1) Left joining Table Left with Table Right  \n",
    "(2) Right joining Table Left with Table Right  \n",
    "(3) Inner joining Table Left with Table Right  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Approach:\n",
    "1. Recognition of 2 tables: the LogData and the URL data.  \n",
    "\n",
    "anonymous-msweb.data.pp: first 5 lines  \n",
    ">V,1000,C,10001  \n",
    ">V,1001,C,10001  \n",
    ">V,1002,C,10001  \n",
    ">V,1001,C,10002  \n",
    ">V,1003,C,10002  \n",
    "\n",
    "url.txt: first 2 lines  \n",
    ">A,1287,1,\"International AutoRoute\",\"/autoroute\"  \n",
    ">A,1288,1,\"library\",\"/library\n",
    "\n",
    "2. Using the URLs to be the left table, as it must be stored in memory and it is smaller"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "removed ‘url.txt’\r\n"
     ]
    }
   ],
   "source": [
    "# Create a file with only URL(s), i.e. records starting with 'A'\n",
    "!rm -v url.txt\n",
    "!grep ^A anonymous-msweb.data > url.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###Left Join:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing mrjob_leftjoin_hw52.py\n"
     ]
    }
   ],
   "source": [
    "\n",
    "%%writefile mrjob_leftjoin_hw52.py\n",
    "#!/usr/bin/python\n",
    "## mrjob_leftjoin_hw52.py\n",
    "## Author: Angela Gunn & Jing Xu\n",
    "## Description:Left Join\n",
    "from mrjob.job import MRJob\n",
    "from mrjob.step import MRStep\n",
    "from mrjob.compat import get_jobconf_value\n",
    "\n",
    "import csv\n",
    "\n",
    "def csv_readline(line):\n",
    "    \"\"\"Given a sting CSV line, return a list of strings.\"\"\"\n",
    "    for row in csv.reader([line]):\n",
    "        return row\n",
    "\n",
    "\n",
    "class leftjoin(MRJob):\n",
    "    def steps(self):\n",
    "        return [MRStep(mapper_init = self.mapper_init,\n",
    "                     mapper=self.mapper, mapper_final = self.mapper_final)]\n",
    "    \n",
    "    def mapper_init(self):\n",
    "        #store the URLs\n",
    "        self.lefttable = {}\n",
    "    \n",
    "        with open('url.txt') as f:\n",
    "            for line in f:\n",
    "                cell = csv_readline(line)\n",
    "                self.lefttable[cell[1]] = (cell[4],[]) #url, list of visitors\n",
    "        \n",
    "    def mapper(self, _, line):\n",
    "        #this is the logs\n",
    "        cell = csv_readline(line) \n",
    "        key = cell[1]\n",
    "        self.lefttable[key][1].append(cell[3])\n",
    "        \n",
    "    def mapper_final(self):\n",
    "        for key, values in self.lefttable.iteritems():\n",
    "            url = values[0]\n",
    "            if len(values[1]) > 0:\n",
    "                for u in values[1]: yield key, (url, u)\n",
    "            else:\n",
    "                yield key, (url, \"NONE\")\n",
    "                \n",
    "        \n",
    "if __name__ == '__main__':\n",
    "    leftjoin.run()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:mrjob.runner:\n",
      "WARNING:mrjob.runner:PLEASE NOTE: Starting in mrjob v0.5.0, protocols will be strict by default. It's recommended you run your job with --strict-protocols or set up mrjob.conf as described at https://pythonhosted.org/mrjob/whats-new.html#ready-for-strict-protocols\n",
      "WARNING:mrjob.runner:\n"
     ]
    }
   ],
   "source": [
    "# Running mrjob using a Hadoop Runner in local cluster\n",
    "from mrjob_leftjoin_hw52 import leftjoin\n",
    "import os\n",
    "\n",
    "# Passing Hadoop Streaming parameters to:\n",
    "# partition by leftmost part of composite key\n",
    "# secodary sort by rightmost part of the same composite key\n",
    "\n",
    "mr_job = leftjoin(args=['anonymous-msweb.data.pp', \n",
    "                        '--file', 'url.txt'])\n",
    "\n",
    "output_file = \"output_hw52_left.txt\"\n",
    "try:\n",
    "    os.remove(output_file)\n",
    "except OSError:\n",
    "    pass\n",
    "\n",
    "with mr_job.make_runner() as runner, open(output_file, 'a') as f: \n",
    "    runner.run()\n",
    "    # stream_output: get access of the output \n",
    "    for line in runner.stream_output():\n",
    "        #print mr_job.parse_output_line(line)\n",
    "        f.write(str(line))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of results:\n",
      "   98663\n",
      "-----\n",
      "first 10 rows\n",
      "page_id [url, user]\n",
      "\"1142\"\t[\"/southafrica\", \"10372\"]\n",
      "\"1142\"\t[\"/southafrica\", \"13352\"]\n",
      "\"1142\"\t[\"/southafrica\", \"19019\"]\n",
      "\"1142\"\t[\"/southafrica\", \"24124\"]\n",
      "\"1142\"\t[\"/southafrica\", \"25638\"]\n",
      "\"1142\"\t[\"/southafrica\", \"25798\"]\n",
      "\"1142\"\t[\"/southafrica\", \"26342\"]\n",
      "\"1142\"\t[\"/southafrica\", \"28044\"]\n",
      "\"1142\"\t[\"/southafrica\", \"28821\"]\n",
      "\"1142\"\t[\"/southafrica\", \"29837\"]\n"
     ]
    }
   ],
   "source": [
    "!echo \"Number of results:\"\n",
    "!wc -l < output_hw52_left.txt\n",
    "!echo \"-----\"\n",
    "!echo \"first 10 rows\"\n",
    "!echo \"page_id [url, user]\"\n",
    "!head -10 output_hw52_left.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###Right Join:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing mrjob_rightjoin_hw52.py\n"
     ]
    }
   ],
   "source": [
    "\n",
    "%%writefile mrjob_rightjoin_hw52.py\n",
    "#!/usr/bin/python\n",
    "## mrjob_rightjoin_hw52.py\n",
    "## Author: Angela Gunn  & Jing Xu\n",
    "## Description:right Join\n",
    "from mrjob.job import MRJob\n",
    "from mrjob.step import MRStep\n",
    "from mrjob.compat import get_jobconf_value\n",
    "\n",
    "import csv\n",
    "\n",
    "def csv_readline(line):\n",
    "    \"\"\"Given a sting CSV line, return a list of strings.\"\"\"\n",
    "    for row in csv.reader([line]):\n",
    "        return row\n",
    "\n",
    "\n",
    "class rightjoin(MRJob):\n",
    "    def steps(self):\n",
    "        return [MRStep(mapper_init = self.mapper_init,\n",
    "                     mapper=self.mapper)]\n",
    "    \n",
    "    def mapper_init(self):\n",
    "        #store the URLs\n",
    "        self.lefttable = {}\n",
    "        with open('url.txt') as f:\n",
    "            for line in f:\n",
    "                cell = csv_readline(line)\n",
    "                self.lefttable[cell[1]] = cell[4]\n",
    "        \n",
    "    def mapper(self, _, line):\n",
    "        #this is the logs\n",
    "        cell = csv_readline(line) \n",
    "        if cell[1] in self.lefttable.keys():\n",
    "            yield cell[1], (self.lefttable[cell[1]], cell[3]) #yield the matching rows.     \n",
    "        else:\n",
    "            yield None, (None , cell[3])\n",
    "        \n",
    "if __name__ == '__main__':\n",
    "    rightjoin.run()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:mrjob.runner:\n",
      "WARNING:mrjob.runner:PLEASE NOTE: Starting in mrjob v0.5.0, protocols will be strict by default. It's recommended you run your job with --strict-protocols or set up mrjob.conf as described at https://pythonhosted.org/mrjob/whats-new.html#ready-for-strict-protocols\n",
      "WARNING:mrjob.runner:\n"
     ]
    }
   ],
   "source": [
    "# Running mrjob using a Hadoop Runner in local cluster\n",
    "from mrjob_rightjoin_hw52 import rightjoin\n",
    "import os\n",
    "\n",
    "# Passing Hadoop Streaming parameters to:\n",
    "# partition by leftmost part of composite key\n",
    "# secodary sort by rightmost part of the same composite key\n",
    "\n",
    "mr_job = rightjoin(args=['anonymous-msweb.data.pp', \n",
    "                        '--file', 'url.txt'])\n",
    "\n",
    "output_file = \"output_hw52_right.txt\"\n",
    "try:\n",
    "    os.remove(output_file)\n",
    "except OSError:\n",
    "    pass\n",
    "\n",
    "with mr_job.make_runner() as runner, open(output_file, 'a') as f: \n",
    "    runner.run()\n",
    "    # stream_output: get access of the output \n",
    "    for line in runner.stream_output():\n",
    "        #print mr_job.parse_output_line(line)\n",
    "        f.write(str(line))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of results:\n",
      "   98654\n",
      "-----\n",
      "first 10 rows\n",
      "page_id [url, user]\n",
      "\"1000\"\t[\"/regwiz\", \"10001\"]\n",
      "\"1001\"\t[\"/support\", \"10001\"]\n",
      "\"1002\"\t[\"/athome\", \"10001\"]\n",
      "\"1001\"\t[\"/support\", \"10002\"]\n",
      "\"1003\"\t[\"/kb\", \"10002\"]\n",
      "\"1001\"\t[\"/support\", \"10003\"]\n",
      "\"1003\"\t[\"/kb\", \"10003\"]\n",
      "\"1004\"\t[\"/search\", \"10003\"]\n",
      "\"1005\"\t[\"/norge\", \"10004\"]\n",
      "\"1006\"\t[\"/misc\", \"10005\"]\n"
     ]
    }
   ],
   "source": [
    "!echo \"Number of results:\"\n",
    "!wc -l < output_hw52_right.txt\n",
    "!echo \"-----\"\n",
    "!echo \"first 10 rows\"\n",
    "!echo \"page_id [url, user]\"\n",
    "!head -10 output_hw52_right.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###Inner Join:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting mrjob_join_hw52.py\n"
     ]
    }
   ],
   "source": [
    "\n",
    "%%writefile mrjob_join_hw52.py\n",
    "#!/usr/bin/python\n",
    "## mrjob_join_hw52.py\n",
    "## Author: Angela Gunn  & Jing Xu\n",
    "## Description:Inner Join\n",
    "\n",
    "\n",
    "\n",
    "from mrjob.job import MRJob\n",
    "from mrjob.step import MRStep\n",
    "from mrjob.compat import get_jobconf_value\n",
    "\n",
    "import csv\n",
    "\n",
    "def csv_readline(line):\n",
    "    \"\"\"Given a sting CSV line, return a list of strings.\"\"\"\n",
    "    for row in csv.reader([line]):\n",
    "        return row\n",
    "\n",
    "class innerjoin(MRJob):\n",
    "    def steps(self):\n",
    "        return [MRStep(mapper_init = self.mapper_init,\n",
    "                     mapper=self.mapper)]\n",
    "    \n",
    "    def mapper_init(self):\n",
    "        #store the URLs\n",
    "        self.lefttable = {}\n",
    "        with open('url.txt') as f:\n",
    "            for line in f:\n",
    "                cell = csv_readline(line)\n",
    "                self.lefttable[cell[1]] = cell[4]\n",
    "        \n",
    "    def mapper(self, _, line):\n",
    "        #this is the logs\n",
    "        cell = csv_readline(line) \n",
    "        yield cell[1], (self.lefttable[cell[1]], cell[3]) #yield the matching rows.\n",
    "        \n",
    "        \n",
    "if __name__ == '__main__':\n",
    "    innerjoin.run()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:mrjob.runner:\n",
      "WARNING:mrjob.runner:PLEASE NOTE: Starting in mrjob v0.5.0, protocols will be strict by default. It's recommended you run your job with --strict-protocols or set up mrjob.conf as described at https://pythonhosted.org/mrjob/whats-new.html#ready-for-strict-protocols\n",
      "WARNING:mrjob.runner:\n"
     ]
    }
   ],
   "source": [
    "# Running mrjob using a Hadoop Runner in local cluster\n",
    "from mrjob_join_hw52 import innerjoin\n",
    "import os\n",
    "\n",
    "# Passing Hadoop Streaming parameters to:\n",
    "# partition by leftmost part of composite key\n",
    "# secodary sort by rightmost part of the same composite key\n",
    "\n",
    "mr_job = innerjoin(args=['anonymous-msweb.data.pp', \n",
    "                        '--file', 'url.txt'])\n",
    "\n",
    "output_file = \"output_hw52_inner.txt\"\n",
    "try:\n",
    "    os.remove(output_file)\n",
    "except OSError:\n",
    "    pass\n",
    "\n",
    "with mr_job.make_runner() as runner, open(output_file, 'a') as f: \n",
    "    runner.run()\n",
    "    # stream_output: get access of the output \n",
    "    for line in runner.stream_output():\n",
    "        #print mr_job.parse_output_line(line)\n",
    "        f.write(str(line))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of results:\n",
      "98654\n",
      "-----\n",
      "first 10 rows\n",
      "page_id [url, user]\n",
      "\"1000\"\t[\"/regwiz\", \"10001\"]\n",
      "\"1001\"\t[\"/support\", \"10001\"]\n",
      "\"1002\"\t[\"/athome\", \"10001\"]\n",
      "\"1001\"\t[\"/support\", \"10002\"]\n",
      "\"1003\"\t[\"/kb\", \"10002\"]\n",
      "\"1001\"\t[\"/support\", \"10003\"]\n",
      "\"1003\"\t[\"/kb\", \"10003\"]\n",
      "\"1004\"\t[\"/search\", \"10003\"]\n",
      "\"1005\"\t[\"/norge\", \"10004\"]\n",
      "\"1006\"\t[\"/misc\", \"10005\"]\n"
     ]
    }
   ],
   "source": [
    "!echo \"Number of results:\"\n",
    "!wc -l < output_hw52_inner.txt\n",
    "!echo \"-----\"\n",
    "!echo \"first 10 rows\"\n",
    "!echo \"page_id [url, user]\"\n",
    "!head -10 output_hw52_inner.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "##HW 5.3  EDA of Google n-grams dataset\n",
    "A large subset of the Google n-grams dataset\n",
    "\n",
    "https://aws.amazon.com/datasets/google-books-ngrams/\n",
    "\n",
    "which we have placed in a bucket/folder on Dropbox on s3:\n",
    "\n",
    "   https://www.dropbox.com/sh/tmqpc4o0xswhkvz/AACUifrl6wrMrlK6a3X3lZ9Ea?dl=0 \n",
    "\n",
    "   s3://filtered-5grams/\n",
    "\n",
    "In particular, this bucket contains (~200) files (10Meg each) in the format:\n",
    "\n",
    "\t(ngram) \\t (count) \\t (pages_count) \\t (books_count)\n",
    "\n",
    "For HW 5.3-5.5, for the Google n-grams dataset unit test and regression test your code using the \n",
    "first 10 lines of the following file:\n",
    "\n",
    "    googlebooks-eng-all-5gram-20090715-0-filtered.txt\n",
    "\n",
    "Once you are happy with your test results proceed to generating  your results on the Google n-grams dataset. \n",
    "\n",
    "Do some EDA on this dataset using mrjob, e.g., \n",
    "\n",
    "- Longest 5-gram (number of characters)\n",
    "- Top 10 most frequent words (please use the count information), i.e., unigrams\n",
    "- 20 Most/Least densely appearing words (count/pages_count) sorted in decreasing order of relative frequency \n",
    "- Distribution of 5-gram sizes (character length).  E.g., count (using the count field) up how many times a 5-gram of 50 characters shows up. Plot the data graphically using a histogram."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "####Response to Requirements\n",
    "I tested these jobs locally on a small set from the googlebooks, as requested above. However, I did not realize that was to be part of our output for this question (wasn't there originally), so I do not have the outputs to show here.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###Longest 5-gram"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting mrjob_longest_53.py\n"
     ]
    }
   ],
   "source": [
    "\n",
    "%%writefile mrjob_longest_53.py\n",
    "#!/usr/bin/python\n",
    "## mrjob_longest_53.py\n",
    "## Author: Angela Gunn  & Jing Xu\n",
    "## Description:Find the longest 5-gram\n",
    "\n",
    "from mrjob.job import MRJob\n",
    "from mrjob.step import MRStep\n",
    "from mrjob.compat import get_jobconf_value\n",
    "\n",
    "class longest_ngram(MRJob):\n",
    "    long_ngram = None\n",
    "    long_length = 0\n",
    "    \n",
    "    def steps(self):\n",
    "        return [MRStep(mapper=self.mapper,\n",
    "                         reducer=self.reducer,\n",
    "                         reducer_final=self.reducer_final,\n",
    "                       jobconf={\n",
    "                            \"mapred.map.tasks\":4,\n",
    "                            \"mapred.reduce.tasks\":1,\n",
    "                            })]\n",
    "        \n",
    "    def mapper(self, _, line):\n",
    "        #break out the lengths of the cells\n",
    "        cell = line.split('\\t')\n",
    "        length = len(cell[0])\n",
    "        yield cell[0], length\n",
    "\n",
    "    def reducer(self, key, value):\n",
    "        #Add to globals if biggest\n",
    "        value = list(value)\n",
    "        if sum(value) > self.long_length:\n",
    "            self.long_ngram = key\n",
    "            self.long_length = sum(value)\n",
    "            \n",
    "    def reducer_final(self):\n",
    "        #output biggest\n",
    "        yield self.long_ngram, self.long_length\n",
    "        \n",
    "if __name__ == '__main__':\n",
    "    longest_ngram.run()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "delete: 's3://agunn-w261-hw5/longest53/_SUCCESS'\n",
      "delete: 's3://agunn-w261-hw5/longest53/part-00000'\n",
      "Got unexpected keyword arguments: ssh_tunnel\n",
      "using configs in /home/hduser/.mrjob.conf\n",
      "creating new scratch bucket mrjob-3e5a84f2faf31709\n",
      "using s3://mrjob-3e5a84f2faf31709/tmp/ as our scratch dir on S3\n",
      "creating tmp directory /tmp/mrjob_longest_53.hduser.20160216.183744.356512\n",
      "writing master bootstrap script to /tmp/mrjob_longest_53.hduser.20160216.183744.356512/b.py\n",
      "creating S3 bucket 'mrjob-3e5a84f2faf31709' to use as scratch space\n",
      "Copying non-input files into s3://mrjob-3e5a84f2faf31709/tmp/mrjob_longest_53.hduser.20160216.183744.356512/files/\n",
      "Waiting 5.0s for S3 eventual consistency\n",
      "Creating Elastic MapReduce job flow\n",
      "Job flow created with ID: j-KMZT7NED7BMO\n",
      "Created new job flow j-KMZT7NED7BMO\n",
      "Job launched 31.0s ago, status STARTING: Provisioning Amazon EC2 capacity\n",
      "Job launched 62.0s ago, status STARTING: Provisioning Amazon EC2 capacity\n",
      "Job on job flow j-KMZT7NED7BMO failed with status TERMINATED: Terminated by user request\n",
      "Logs are in s3://mrjob-3e5a84f2faf31709/tmp/logs/j-KMZT7NED7BMO/\n",
      "ec2_key_pair_file not specified, going to S3\n",
      "Scanning S3 logs for probable cause of failure\n",
      "Waiting 5.0s for S3 eventual consistency\n",
      "Terminating job flow: j-KMZT7NED7BMO\n",
      "Traceback (most recent call last):\n",
      "  File \"mrjob_longest_53.py\", line 41, in <module>\n",
      "    longest_ngram.run()\n",
      "  File \"/usr/local/lib/python2.7/dist-packages/mrjob/job.py\", line 461, in run\n",
      "    mr_job.execute()\n",
      "  File \"/usr/local/lib/python2.7/dist-packages/mrjob/job.py\", line 479, in execute\n",
      "    super(MRJob, self).execute()\n",
      "  File \"/usr/local/lib/python2.7/dist-packages/mrjob/launch.py\", line 153, in execute\n",
      "    self.run_job()\n",
      "  File \"/usr/local/lib/python2.7/dist-packages/mrjob/launch.py\", line 216, in run_job\n",
      "    runner.run()\n",
      "  File \"/usr/local/lib/python2.7/dist-packages/mrjob/runner.py\", line 470, in run\n",
      "    self._run()\n",
      "  File \"/usr/local/lib/python2.7/dist-packages/mrjob/emr.py\", line 883, in _run\n",
      "    self._wait_for_job_to_complete()\n",
      "  File \"/usr/local/lib/python2.7/dist-packages/mrjob/emr.py\", line 1768, in _wait_for_job_to_complete\n",
      "    raise Exception(msg)\n",
      "Exception: Job on job flow j-KMZT7NED7BMO failed with status TERMINATED: Terminated by user request\n"
     ]
    }
   ],
   "source": [
    "!s3cmd rm --recursive s3://agunn-w261-hw5/longest53/\n",
    "! python mrjob_longest_53.py -r emr s3://filtered-5grams \\\n",
    "    --output-dir=s3://agunn-w261-hw5/longest53 \\\n",
    "    --no-output \\\n",
    "    --no-strict-protocol"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ERROR: Parameter problem: File HW5/longest53_out.txt already exists. Use either of --force / --continue / --skip-existing or give it a new name.\n",
      "\"AIOPJUMRXUYVASLYHYPSIBEMAPODIKR UFRYDIUUOLBIGASUAURUSREXLISNAYE RNOONDQSRUNSUBUNOUGRABBERYAIRTC UTAHRAPTOREDILEIPMILBDUMMYUVERI SYEVRAHVELOCYALLOSAURUSLINROTSR\"\t159\n"
     ]
    }
   ],
   "source": [
    "!s3cmd get s3://agunn-w261-hw5/longest53/part* HW5/longest53_out.txt\n",
    "!head HW5/longest53_out.txt\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###Top 10 Most Frequent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting mrjob_frequency_53.py\n"
     ]
    }
   ],
   "source": [
    "\n",
    "%%writefile mrjob_frequency_53.py\n",
    "#!/usr/bin/python\n",
    "## mrjob_frequency_53.py\n",
    "## Author: Angela Gunn  & Jing Xu\n",
    "## Description:Find the frequency of a word in the 5-gram\n",
    "\n",
    "from mrjob.job import MRJob\n",
    "from mrjob.step import MRStep\n",
    "from mrjob.compat import get_jobconf_value\n",
    "\n",
    "import re\n",
    "\n",
    "WORD_RE = re.compile(r\"[A-Za-z0-9]+\")\n",
    "\n",
    "class frequency(MRJob):\n",
    "    \n",
    "    def steps(self):\n",
    "        return [\n",
    "            MRStep(mapper=self.mapper,\n",
    "                   combiner=self.combiner,\n",
    "                   reducer=self.reducer,\n",
    "                   jobconf={\n",
    "                            \"mapred.map.tasks\":16,\n",
    "                            \"mapred.reduce.tasks\":8,\n",
    "                            }),\n",
    "             MRStep(mapper=self.mapper_frequent_unigrams,\n",
    "                   reducer=self.reducer_frequent_unigrams,\n",
    "                   jobconf={\n",
    "                            'mapred.output.key.comparator.class': 'org.apache.hadoop.mapred.lib.KeyFieldBasedComparator',\n",
    "                            'mapred.text.key.comparator.options': '-k1,1rn',\n",
    "                            \"mapred.map.tasks\":4,\n",
    "                            \"mapred.reduce.tasks\":1,\n",
    "                            }\n",
    "                   )\n",
    "        ]\n",
    "    \n",
    "    def mapper(self, _, line):\n",
    "        #get the word, and count for output\n",
    "        line.strip()\n",
    "        cell = re.split(\"\\t\",line)\n",
    "        unigrams = cell[0].split()\n",
    "        count = int(cell[1])\n",
    "        for unigram in unigrams:\n",
    "            yield unigram, count\n",
    "    \n",
    "    def combiner(self, unigram, counts):\n",
    "        #combine counts\n",
    "        yield unigram, sum(counts)\n",
    "    \n",
    "    def reducer(self, unigram, counts):\n",
    "        #combine counts\n",
    "        yield unigram, sum(counts)\n",
    "        \n",
    "    def mapper_frequent_unigrams(self, unigram, count):\n",
    "        #Just passing along with count first so that they all get shuffled by the count\n",
    "        yield count, unigram\n",
    "        \n",
    "    def reducer_frequent_unigrams(self, count, unigrams):\n",
    "        #Printing.\n",
    "        for unigram in unigrams:\n",
    "            yield count, unigram\n",
    "        \n",
    "if __name__ == '__main__':\n",
    "    frequency.run()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "delete: 's3://agunn-w261-hw5/frequency53/_SUCCESS'\n",
      "delete: 's3://agunn-w261-hw5/frequency53/part-00000'\n",
      "Got unexpected keyword arguments: ssh_tunnel\n",
      "using configs in /home/hduser/.mrjob.conf\n",
      "creating new scratch bucket mrjob-dd1e164662921bff\n",
      "using s3://mrjob-dd1e164662921bff/tmp/ as our scratch dir on S3\n",
      "creating tmp directory /tmp/mrjob_frequency_53.hduser.20160216.093621.519523\n",
      "writing master bootstrap script to /tmp/mrjob_frequency_53.hduser.20160216.093621.519523/b.py\n",
      "creating S3 bucket 'mrjob-dd1e164662921bff' to use as scratch space\n",
      "Copying non-input files into s3://mrjob-dd1e164662921bff/tmp/mrjob_frequency_53.hduser.20160216.093621.519523/files/\n",
      "Waiting 5.0s for S3 eventual consistency\n",
      "Creating Elastic MapReduce job flow\n",
      "Job flow created with ID: j-14LX8WV79GC9C\n",
      "Created new job flow j-14LX8WV79GC9C\n",
      "Job launched 31.1s ago, status STARTING: Provisioning Amazon EC2 capacity\n",
      "Job launched 62.1s ago, status STARTING: Provisioning Amazon EC2 capacity\n",
      "Job launched 93.0s ago, status STARTING: Provisioning Amazon EC2 capacity\n",
      "Job launched 124.0s ago, status STARTING: Provisioning Amazon EC2 capacity\n",
      "Job launched 155.0s ago, status STARTING: Configuring cluster software\n",
      "Job launched 186.0s ago, status STARTING: Configuring cluster software\n",
      "Job launched 217.1s ago, status RUNNING: Running step\n",
      "Job launched 248.0s ago, status RUNNING: Running step\n",
      "Job launched 279.2s ago, status RUNNING: Running step (mrjob_frequency_53.hduser.20160216.093621.519523: Step 1 of 2)\n",
      "Job launched 310.1s ago, status RUNNING: Running step (mrjob_frequency_53.hduser.20160216.093621.519523: Step 1 of 2)\n",
      "Job launched 341.1s ago, status RUNNING: Running step (mrjob_frequency_53.hduser.20160216.093621.519523: Step 1 of 2)\n",
      "Job launched 372.1s ago, status RUNNING: Running step (mrjob_frequency_53.hduser.20160216.093621.519523: Step 1 of 2)\n",
      "Job launched 403.2s ago, status RUNNING: Running step (mrjob_frequency_53.hduser.20160216.093621.519523: Step 1 of 2)\n",
      "Job launched 434.2s ago, status RUNNING: Running step (mrjob_frequency_53.hduser.20160216.093621.519523: Step 1 of 2)\n",
      "Job launched 465.1s ago, status RUNNING: Running step (mrjob_frequency_53.hduser.20160216.093621.519523: Step 1 of 2)\n",
      "Job launched 496.1s ago, status RUNNING: Running step (mrjob_frequency_53.hduser.20160216.093621.519523: Step 1 of 2)\n",
      "Job launched 527.2s ago, status RUNNING: Running step (mrjob_frequency_53.hduser.20160216.093621.519523: Step 1 of 2)\n",
      "Job launched 558.1s ago, status RUNNING: Running step (mrjob_frequency_53.hduser.20160216.093621.519523: Step 1 of 2)\n",
      "Job launched 589.2s ago, status RUNNING: Running step (mrjob_frequency_53.hduser.20160216.093621.519523: Step 1 of 2)\n",
      "Job launched 620.2s ago, status RUNNING: Running step (mrjob_frequency_53.hduser.20160216.093621.519523: Step 1 of 2)\n",
      "Job launched 651.2s ago, status RUNNING: Running step (mrjob_frequency_53.hduser.20160216.093621.519523: Step 1 of 2)\n",
      "Job launched 682.2s ago, status RUNNING: Running step (mrjob_frequency_53.hduser.20160216.093621.519523: Step 1 of 2)\n",
      "Job launched 713.2s ago, status RUNNING: Running step (mrjob_frequency_53.hduser.20160216.093621.519523: Step 1 of 2)\n",
      "Job launched 744.1s ago, status RUNNING: Running step (mrjob_frequency_53.hduser.20160216.093621.519523: Step 1 of 2)\n",
      "Job launched 775.2s ago, status RUNNING: Running step (mrjob_frequency_53.hduser.20160216.093621.519523: Step 1 of 2)\n",
      "Job launched 806.2s ago, status RUNNING: Running step (mrjob_frequency_53.hduser.20160216.093621.519523: Step 1 of 2)\n",
      "Job launched 837.2s ago, status RUNNING: Running step (mrjob_frequency_53.hduser.20160216.093621.519523: Step 1 of 2)\n",
      "Job launched 868.1s ago, status RUNNING: Running step (mrjob_frequency_53.hduser.20160216.093621.519523: Step 1 of 2)\n",
      "Job launched 899.1s ago, status RUNNING: Running step (mrjob_frequency_53.hduser.20160216.093621.519523: Step 1 of 2)\n",
      "Job launched 930.1s ago, status RUNNING: Running step (mrjob_frequency_53.hduser.20160216.093621.519523: Step 1 of 2)\n",
      "Job launched 961.2s ago, status RUNNING: Running step (mrjob_frequency_53.hduser.20160216.093621.519523: Step 1 of 2)\n",
      "Job launched 992.2s ago, status RUNNING: Running step (mrjob_frequency_53.hduser.20160216.093621.519523: Step 1 of 2)\n",
      "Job launched 1023.3s ago, status RUNNING: Running step (mrjob_frequency_53.hduser.20160216.093621.519523: Step 1 of 2)\n",
      "Job launched 1054.2s ago, status RUNNING: Running step (mrjob_frequency_53.hduser.20160216.093621.519523: Step 1 of 2)\n",
      "Job launched 1085.3s ago, status RUNNING: Running step (mrjob_frequency_53.hduser.20160216.093621.519523: Step 1 of 2)\n",
      "Job launched 1116.3s ago, status RUNNING: Running step (mrjob_frequency_53.hduser.20160216.093621.519523: Step 1 of 2)\n",
      "Job launched 1147.3s ago, status RUNNING: Running step (mrjob_frequency_53.hduser.20160216.093621.519523: Step 1 of 2)\n",
      "Job launched 1178.4s ago, status RUNNING: Running step (mrjob_frequency_53.hduser.20160216.093621.519523: Step 1 of 2)\n",
      "Job launched 1209.4s ago, status RUNNING: Running step (mrjob_frequency_53.hduser.20160216.093621.519523: Step 1 of 2)\n",
      "Job launched 1240.4s ago, status RUNNING: Running step (mrjob_frequency_53.hduser.20160216.093621.519523: Step 1 of 2)\n",
      "Job launched 1271.5s ago, status RUNNING: Running step (mrjob_frequency_53.hduser.20160216.093621.519523: Step 1 of 2)\n",
      "Job launched 1302.5s ago, status RUNNING: Running step (mrjob_frequency_53.hduser.20160216.093621.519523: Step 1 of 2)\n",
      "Job launched 1333.6s ago, status RUNNING: Running step (mrjob_frequency_53.hduser.20160216.093621.519523: Step 2 of 2)\n",
      "Job completed.\n",
      "Running time was 1126.0s (not counting time spent waiting for the EC2 instances)\n",
      "ec2_key_pair_file not specified, going to S3\n",
      "Fetching counters from S3...\n",
      "Waiting 5.0s for S3 eventual consistency\n",
      "Counters from step 1:\n",
      "  File Input Format Counters :\n",
      "    Bytes Read: 2156069116\n",
      "  File Output Format Counters :\n",
      "    Bytes Written: 5251252\n",
      "  FileSystemCounters:\n",
      "    FILE_BYTES_READ: 327020941\n",
      "    FILE_BYTES_WRITTEN: 326229228\n",
      "    HDFS_BYTES_READ: 23640\n",
      "    HDFS_BYTES_WRITTEN: 5251252\n",
      "    S3_BYTES_READ: 2156069116\n",
      "  Job Counters :\n",
      "    Launched map tasks: 195\n",
      "    Launched reduce tasks: 10\n",
      "    Rack-local map tasks: 193\n",
      "    SLOTS_MILLIS_MAPS: 16892189\n",
      "    SLOTS_MILLIS_REDUCES: 5424402\n",
      "    Total time spent by all maps waiting after reserving slots (ms): 0\n",
      "    Total time spent by all reduces waiting after reserving slots (ms): 0\n",
      "  Map-Reduce Framework:\n",
      "    CPU time spent (ms): 7290480\n",
      "    Combine input records: 307583853\n",
      "    Combine output records: 22094566\n",
      "    Map input bytes: 2156069116\n",
      "    Map input records: 58682266\n",
      "    Map output bytes: 3136729760\n",
      "    Map output materialized bytes: 99241264\n",
      "    Map output records: 293411330\n",
      "    Physical memory (bytes) snapshot: 147517882368\n",
      "    Reduce input groups: 343019\n",
      "    Reduce input records: 7922043\n",
      "    Reduce output records: 343019\n",
      "    Reduce shuffle bytes: 99241264\n",
      "    SPLIT_RAW_BYTES: 23640\n",
      "    Spilled Records: 30016609\n",
      "    Total committed heap usage (bytes): 154081427456\n",
      "    Virtual memory (bytes) snapshot: 388289613824\n",
      "Counters from step 2:\n",
      "  File Input Format Counters :\n",
      "    Bytes Read: 5251252\n",
      "  File Output Format Counters :\n",
      "    Bytes Written: 5251252\n",
      "  FileSystemCounters:\n",
      "    FILE_BYTES_READ: 3962134\n",
      "    FILE_BYTES_WRITTEN: 8491120\n",
      "    HDFS_BYTES_READ: 5252540\n",
      "    S3_BYTES_WRITTEN: 5251252\n",
      "  Job Counters :\n",
      "    Data-local map tasks: 7\n",
      "    Launched map tasks: 8\n",
      "    Launched reduce tasks: 1\n",
      "    Rack-local map tasks: 1\n",
      "    SLOTS_MILLIS_MAPS: 79677\n",
      "    SLOTS_MILLIS_REDUCES: 19832\n",
      "    Total time spent by all maps waiting after reserving slots (ms): 0\n",
      "    Total time spent by all reduces waiting after reserving slots (ms): 0\n",
      "  Map-Reduce Framework:\n",
      "    CPU time spent (ms): 27020\n",
      "    Combine input records: 0\n",
      "    Combine output records: 0\n",
      "    Map input bytes: 5251252\n",
      "    Map input records: 343019\n",
      "    Map output bytes: 5251252\n",
      "    Map output materialized bytes: 4286598\n",
      "    Map output records: 343019\n",
      "    Physical memory (bytes) snapshot: 3130085376\n",
      "    Reduce input groups: 41362\n",
      "    Reduce input records: 343019\n",
      "    Reduce output records: 343019\n",
      "    Reduce shuffle bytes: 4286598\n",
      "    SPLIT_RAW_BYTES: 1288\n",
      "    Spilled Records: 686038\n",
      "    Total committed heap usage (bytes): 3846176768\n",
      "    Virtual memory (bytes) snapshot: 17810649088\n",
      "removing tmp directory /tmp/mrjob_frequency_53.hduser.20160216.093621.519523\n",
      "Removing all files in s3://mrjob-dd1e164662921bff/tmp/mrjob_frequency_53.hduser.20160216.093621.519523/\n",
      "Removing all files in s3://mrjob-dd1e164662921bff/tmp/logs/j-14LX8WV79GC9C/\n",
      "Terminating job flow: j-14LX8WV79GC9C\n"
     ]
    }
   ],
   "source": [
    "!s3cmd rm --recursive s3://agunn-w261-hw5/frequency53/\n",
    "! python mrjob_frequency_53.py -r emr s3://filtered-5grams \\\n",
    "    --output-dir=s3://agunn-w261-hw5/frequency53 \\\n",
    "    --no-output \\\n",
    "    --no-strict-protocol"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ERROR: Parameter problem: File HW5/frequency_out.txt already exists. Use either of --force / --continue / --skip-existing or give it a new name.\n",
      "5375699242\t\"the\"\n",
      "3691308874\t\"of\"\n",
      "2221164346\t\"to\"\n",
      "1387638591\t\"in\"\n",
      "1342195425\t\"a\"\n",
      "1135779433\t\"and\"\n",
      "798553959\t\"that\"\n",
      "756296656\t\"is\"\n",
      "688053106\t\"be\"\n",
      "481373389\t\"as\"\n"
     ]
    }
   ],
   "source": [
    "!s3cmd get s3://agunn-w261-hw5/frequency53/part* HW5/frequency_out.txt\n",
    "!head HW5/frequency_out.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###20 Most/Least Densely Words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting mrjob_density_53.py\n"
     ]
    }
   ],
   "source": [
    "\n",
    "%%writefile mrjob_density_53.py\n",
    "#!/usr/bin/python\n",
    "## mrjob_density_53.py\n",
    "## Author: Angela Gunn  & Jing Xu\n",
    "## Description:Find the density of the words in 5gram\n",
    "\n",
    "from mrjob.job import MRJob\n",
    "from mrjob.step import MRStep\n",
    "from mrjob.compat import get_jobconf_value\n",
    "\n",
    "class density_53(MRJob):\n",
    "\n",
    "    \n",
    "    def steps(self):\n",
    "        return [MRStep(mapper=self.mapper,\n",
    "                         reducer=self.reducer, \n",
    "                   jobconf={\n",
    "                    \"mapred.map.tasks\":16,\n",
    "                    \"mapred.reduce.tasks\":8\n",
    "                    }\n",
    "                         ),\n",
    "               MRStep(mapper=self.mapper_max_min,\n",
    "                        reducer=self.reducer_max_min,\n",
    "                   jobconf={\n",
    "                            'mapred.output.key.comparator.class': 'org.apache.hadoop.mapred.lib.KeyFieldBasedComparator',\n",
    "                            'mapred.text.key.comparator.options': '-k1,1rn',\n",
    "                            \"mapred.map.tasks\":4,\n",
    "                            \"mapred.reduce.tasks\":1\n",
    "                            }\n",
    "                     )]\n",
    "        \n",
    "    def mapper(self, _, line):\n",
    "        #output the density (count / pages)\n",
    "        cell = line.split('\\t')\n",
    "        words = cell[0].split()\n",
    "        density = round((int(cell[1]) * 1.0 / int(cell[2])), 3)\n",
    "        for w in words:\n",
    "            yield w.lower(), density\n",
    "            \n",
    "    def combiner(self, unigram, densities):\n",
    "        #combine\n",
    "        densities = [d for d in densities]\n",
    "        yield unigram, min(densities) \n",
    "        yield unigram, max(densities)\n",
    "        \n",
    "    def reducer(self, unigram, densities):\n",
    "        #combine\n",
    "        densities = [d for d in densities]\n",
    "        yield unigram, min(densities)\n",
    "        yield unigram, max(densities)\n",
    "        \n",
    "    def mapper_max_min(self, unigram, density):\n",
    "        #output with density first so grouping can happen\n",
    "        yield density, unigram\n",
    "        \n",
    "    def reducer_max_min(self, density, unigrams):\n",
    "        #final output\n",
    "        for unigram in unigrams:\n",
    "            yield density, unigram\n",
    "            \n",
    "        \n",
    "if __name__ == '__main__':\n",
    "    density_53.run()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "delete: 's3://agunn-w261-hw5/density53/_SUCCESS'\n",
      "delete: 's3://agunn-w261-hw5/density53/part-00000'\n",
      "Got unexpected keyword arguments: ssh_tunnel\n",
      "using configs in /home/hduser/.mrjob.conf\n",
      "creating new scratch bucket mrjob-349aa6d211330c1d\n",
      "using s3://mrjob-349aa6d211330c1d/tmp/ as our scratch dir on S3\n",
      "creating tmp directory /tmp/mrjob_density_53.hduser.20160216.100149.962951\n",
      "writing master bootstrap script to /tmp/mrjob_density_53.hduser.20160216.100149.962951/b.py\n",
      "creating S3 bucket 'mrjob-349aa6d211330c1d' to use as scratch space\n",
      "Copying non-input files into s3://mrjob-349aa6d211330c1d/tmp/mrjob_density_53.hduser.20160216.100149.962951/files/\n",
      "Waiting 5.0s for S3 eventual consistency\n",
      "Creating Elastic MapReduce job flow\n",
      "Job flow created with ID: j-39IAQOO3X49KX\n",
      "Created new job flow j-39IAQOO3X49KX\n",
      "Job launched 31.0s ago, status STARTING: Provisioning Amazon EC2 capacity\n",
      "Job launched 62.0s ago, status STARTING: Provisioning Amazon EC2 capacity\n",
      "Job launched 93.2s ago, status STARTING: Provisioning Amazon EC2 capacity\n",
      "Job launched 124.2s ago, status STARTING: Provisioning Amazon EC2 capacity\n",
      "Job launched 155.3s ago, status STARTING: Configuring cluster software\n",
      "Job launched 186.3s ago, status STARTING: Configuring cluster software\n",
      "Job launched 217.2s ago, status RUNNING: Running step\n",
      "Job launched 248.2s ago, status RUNNING: Running step\n",
      "Job launched 279.2s ago, status RUNNING: Running step (mrjob_density_53.hduser.20160216.100149.962951: Step 1 of 2)\n",
      "Job launched 310.1s ago, status RUNNING: Running step (mrjob_density_53.hduser.20160216.100149.962951: Step 1 of 2)\n",
      "Job launched 341.2s ago, status RUNNING: Running step (mrjob_density_53.hduser.20160216.100149.962951: Step 1 of 2)\n",
      "Job launched 372.1s ago, status RUNNING: Running step (mrjob_density_53.hduser.20160216.100149.962951: Step 1 of 2)\n",
      "Job launched 403.1s ago, status RUNNING: Running step (mrjob_density_53.hduser.20160216.100149.962951: Step 1 of 2)\n",
      "Job launched 434.2s ago, status RUNNING: Running step (mrjob_density_53.hduser.20160216.100149.962951: Step 1 of 2)\n",
      "Job launched 465.1s ago, status RUNNING: Running step (mrjob_density_53.hduser.20160216.100149.962951: Step 1 of 2)\n",
      "Job launched 496.1s ago, status RUNNING: Running step (mrjob_density_53.hduser.20160216.100149.962951: Step 1 of 2)\n",
      "Job launched 527.1s ago, status RUNNING: Running step (mrjob_density_53.hduser.20160216.100149.962951: Step 1 of 2)\n",
      "Job launched 558.1s ago, status RUNNING: Running step (mrjob_density_53.hduser.20160216.100149.962951: Step 1 of 2)\n",
      "Job launched 589.2s ago, status RUNNING: Running step (mrjob_density_53.hduser.20160216.100149.962951: Step 1 of 2)\n",
      "Job launched 620.2s ago, status RUNNING: Running step (mrjob_density_53.hduser.20160216.100149.962951: Step 1 of 2)\n",
      "Job launched 651.1s ago, status RUNNING: Running step (mrjob_density_53.hduser.20160216.100149.962951: Step 1 of 2)\n",
      "Job launched 682.1s ago, status RUNNING: Running step (mrjob_density_53.hduser.20160216.100149.962951: Step 1 of 2)\n",
      "Job launched 713.1s ago, status RUNNING: Running step (mrjob_density_53.hduser.20160216.100149.962951: Step 1 of 2)\n",
      "Job launched 744.0s ago, status RUNNING: Running step (mrjob_density_53.hduser.20160216.100149.962951: Step 1 of 2)\n",
      "Job launched 775.1s ago, status RUNNING: Running step (mrjob_density_53.hduser.20160216.100149.962951: Step 1 of 2)\n",
      "Job launched 806.1s ago, status RUNNING: Running step (mrjob_density_53.hduser.20160216.100149.962951: Step 1 of 2)\n",
      "Job launched 837.1s ago, status RUNNING: Running step (mrjob_density_53.hduser.20160216.100149.962951: Step 1 of 2)\n",
      "Job launched 868.1s ago, status RUNNING: Running step (mrjob_density_53.hduser.20160216.100149.962951: Step 1 of 2)\n",
      "Job launched 899.2s ago, status RUNNING: Running step (mrjob_density_53.hduser.20160216.100149.962951: Step 1 of 2)\n",
      "Job launched 930.2s ago, status RUNNING: Running step (mrjob_density_53.hduser.20160216.100149.962951: Step 1 of 2)\n",
      "Job launched 961.2s ago, status RUNNING: Running step (mrjob_density_53.hduser.20160216.100149.962951: Step 1 of 2)\n",
      "Job launched 992.2s ago, status RUNNING: Running step (mrjob_density_53.hduser.20160216.100149.962951: Step 1 of 2)\n",
      "Job launched 1023.2s ago, status RUNNING: Running step (mrjob_density_53.hduser.20160216.100149.962951: Step 1 of 2)\n",
      "Job launched 1054.2s ago, status RUNNING: Running step (mrjob_density_53.hduser.20160216.100149.962951: Step 1 of 2)\n",
      "Job launched 1085.1s ago, status RUNNING: Running step (mrjob_density_53.hduser.20160216.100149.962951: Step 1 of 2)\n",
      "Job launched 1116.1s ago, status RUNNING: Running step (mrjob_density_53.hduser.20160216.100149.962951: Step 1 of 2)\n",
      "Job launched 1147.1s ago, status RUNNING: Running step (mrjob_density_53.hduser.20160216.100149.962951: Step 1 of 2)\n",
      "Job launched 1178.0s ago, status RUNNING: Running step (mrjob_density_53.hduser.20160216.100149.962951: Step 1 of 2)\n",
      "Job launched 1209.1s ago, status RUNNING: Running step (mrjob_density_53.hduser.20160216.100149.962951: Step 1 of 2)\n",
      "Job launched 1240.1s ago, status RUNNING: Running step (mrjob_density_53.hduser.20160216.100149.962951: Step 1 of 2)\n",
      "Job launched 1271.2s ago, status RUNNING: Running step (mrjob_density_53.hduser.20160216.100149.962951: Step 1 of 2)\n",
      "Job launched 1302.3s ago, status RUNNING: Running step (mrjob_density_53.hduser.20160216.100149.962951: Step 1 of 2)\n",
      "Job launched 1333.3s ago, status RUNNING: Running step (mrjob_density_53.hduser.20160216.100149.962951: Step 1 of 2)\n",
      "Job launched 1364.3s ago, status RUNNING: Running step (mrjob_density_53.hduser.20160216.100149.962951: Step 1 of 2)\n",
      "Job launched 1395.2s ago, status RUNNING: Running step (mrjob_density_53.hduser.20160216.100149.962951: Step 1 of 2)\n",
      "Job launched 1426.2s ago, status RUNNING: Running step (mrjob_density_53.hduser.20160216.100149.962951: Step 1 of 2)\n",
      "Job launched 1457.3s ago, status RUNNING: Running step (mrjob_density_53.hduser.20160216.100149.962951: Step 1 of 2)\n",
      "Job launched 1488.3s ago, status RUNNING: Running step (mrjob_density_53.hduser.20160216.100149.962951: Step 1 of 2)\n",
      "Job launched 1519.4s ago, status RUNNING: Running step (mrjob_density_53.hduser.20160216.100149.962951: Step 1 of 2)\n",
      "Job launched 1550.3s ago, status RUNNING: Running step (mrjob_density_53.hduser.20160216.100149.962951: Step 1 of 2)\n",
      "Job launched 1581.3s ago, status RUNNING: Running step (mrjob_density_53.hduser.20160216.100149.962951: Step 1 of 2)\n",
      "Job launched 1612.4s ago, status RUNNING: Running step (mrjob_density_53.hduser.20160216.100149.962951: Step 1 of 2)\n",
      "Job launched 1643.4s ago, status RUNNING: Running step (mrjob_density_53.hduser.20160216.100149.962951: Step 2 of 2)\n",
      "Job completed.\n",
      "Running time was 1440.0s (not counting time spent waiting for the EC2 instances)\n",
      "ec2_key_pair_file not specified, going to S3\n",
      "Fetching counters from S3...\n",
      "Waiting 5.0s for S3 eventual consistency\n",
      "Counters from step 1:\n",
      "  File Input Format Counters :\n",
      "    Bytes Read: 2156069116\n",
      "  File Output Format Counters :\n",
      "    Bytes Written: 9706995\n",
      "  FileSystemCounters:\n",
      "    FILE_BYTES_READ: 1255761114\n",
      "    FILE_BYTES_WRITTEN: 1750238175\n",
      "    HDFS_BYTES_READ: 23640\n",
      "    HDFS_BYTES_WRITTEN: 9706995\n",
      "    S3_BYTES_READ: 2156069116\n",
      "  Job Counters :\n",
      "    Launched map tasks: 193\n",
      "    Launched reduce tasks: 9\n",
      "    Rack-local map tasks: 191\n",
      "    SLOTS_MILLIS_MAPS: 11409158\n",
      "    SLOTS_MILLIS_REDUCES: 6017724\n",
      "    Total time spent by all maps waiting after reserving slots (ms): 0\n",
      "    Total time spent by all reduces waiting after reserving slots (ms): 0\n",
      "  Map-Reduce Framework:\n",
      "    CPU time spent (ms): 9679060\n",
      "    Combine input records: 0\n",
      "    Combine output records: 0\n",
      "    Map input bytes: 2156069116\n",
      "    Map input records: 58682266\n",
      "    Map output bytes: 3792916430\n",
      "    Map output materialized bytes: 594163686\n",
      "    Map output records: 293411330\n",
      "    Physical memory (bytes) snapshot: 147177684992\n",
      "    Reduce input groups: 269339\n",
      "    Reduce input records: 293411330\n",
      "    Reduce output records: 538678\n",
      "    Reduce shuffle bytes: 594163686\n",
      "    SPLIT_RAW_BYTES: 23640\n",
      "    Spilled Records: 880233990\n",
      "    Total committed heap usage (bytes): 153993871360\n",
      "    Virtual memory (bytes) snapshot: 386591449088\n",
      "Counters from step 2:\n",
      "  File Input Format Counters :\n",
      "    Bytes Read: 9706995\n",
      "  File Output Format Counters :\n",
      "    Bytes Written: 9706995\n",
      "  FileSystemCounters:\n",
      "    FILE_BYTES_READ: 4152620\n",
      "    FILE_BYTES_WRITTEN: 8737535\n",
      "    HDFS_BYTES_READ: 9708267\n",
      "    S3_BYTES_WRITTEN: 9706995\n",
      "  Job Counters :\n",
      "    Data-local map tasks: 8\n",
      "    Launched map tasks: 8\n",
      "    Launched reduce tasks: 1\n",
      "    SLOTS_MILLIS_MAPS: 80278\n",
      "    SLOTS_MILLIS_REDUCES: 22850\n",
      "    Total time spent by all maps waiting after reserving slots (ms): 0\n",
      "    Total time spent by all reduces waiting after reserving slots (ms): 0\n",
      "  Map-Reduce Framework:\n",
      "    CPU time spent (ms): 35420\n",
      "    Combine input records: 0\n",
      "    Combine output records: 0\n",
      "    Map input bytes: 9706995\n",
      "    Map input records: 538678\n",
      "    Map output bytes: 9706995\n",
      "    Map output materialized bytes: 4342719\n",
      "    Map output records: 538678\n",
      "    Physical memory (bytes) snapshot: 3138199552\n",
      "    Reduce input groups: 2286\n",
      "    Reduce input records: 538678\n",
      "    Reduce output records: 538678\n",
      "    Reduce shuffle bytes: 4342719\n",
      "    SPLIT_RAW_BYTES: 1272\n",
      "    Spilled Records: 1077356\n",
      "    Total committed heap usage (bytes): 4046454784\n",
      "    Virtual memory (bytes) snapshot: 17773137920\n",
      "removing tmp directory /tmp/mrjob_density_53.hduser.20160216.100149.962951\n",
      "Removing all files in s3://mrjob-349aa6d211330c1d/tmp/mrjob_density_53.hduser.20160216.100149.962951/\n",
      "Removing all files in s3://mrjob-349aa6d211330c1d/tmp/logs/j-39IAQOO3X49KX/\n",
      "Terminating job flow: j-39IAQOO3X49KX\n"
     ]
    }
   ],
   "source": [
    "!s3cmd rm --recursive s3://agunn-w261-hw5/density53/\n",
    "! python mrjob_density_53.py -r emr s3://filtered-5grams \\\n",
    "    --output-dir=s3://agunn-w261-hw5/density53 \\\n",
    "    --no-output \\\n",
    "    --no-strict-protocol"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "download: 's3://agunn-w261-hw5/density53/part-00000' -> 'HW5/density_out.txt'  [1 of 1]\n",
      " 9706995 of 9706995   100% in    1s     6.16 MB/s  done\n",
      "29.704999999999998\t\"death\"\n",
      "14.912000000000001\t\"write\"\n",
      "14.912000000000001\t\"and\"\n",
      "12.380000000000001\t\"na\"\n",
      "11.557\t\"xxxx\"\n",
      "11.557\t\"xxxx\"\n",
      "10.882\t\"sc\"\n",
      "10.6\t\"beep\"\n",
      "10.022\t\"blah\"\n",
      "9.8040000000000003\t\"whole\"\n",
      "9.8040000000000003\t\"the\"\n",
      "9.7550000000000008\t\"that\"\n",
      "9.7550000000000008\t\"under\"\n",
      "9.7550000000000008\t\"demand\"\n",
      "9.5779999999999994\t\"may\"\n",
      "9.5779999999999994\t\"concurrent\"\n",
      "9.5779999999999994\t\"in\"\n",
      "9.5779999999999994\t\"result\"\n",
      "9.5779999999999994\t\"use\"\n",
      "9.4809999999999999\t\"expenditure\"\n"
     ]
    }
   ],
   "source": [
    "!rm HW5/density_out.txt\n",
    "!s3cmd get s3://agunn-w261-hw5/density53/part* HW5/density_out.txt\n",
    "!head -20  HW5/density_out.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "###Distribution of 5-gram sizes (character length). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting mrjob_distribution_53.py\n"
     ]
    }
   ],
   "source": [
    "\n",
    "%%writefile mrjob_distribution_53.py\n",
    "#!/usr/bin/python\n",
    "## mrjob_distribution_53.py\n",
    "## Author: Angela Gunn  & Jing Xu\n",
    "## Description: Distribution of 5gram lengths\n",
    "\n",
    "from mrjob.job import MRJob\n",
    "from mrjob.step import MRStep\n",
    "from mrjob.compat import get_jobconf_value\n",
    "\n",
    "class distribution_53(MRJob):\n",
    "\n",
    "    \n",
    "    def steps(self):\n",
    "        return [MRStep(mapper=self.mapper,\n",
    "                       combiner = self.combiner,\n",
    "                         reducer=self.reducer, \n",
    "                   jobconf={\n",
    "                            'mapred.output.key.comparator.class': 'org.apache.hadoop.mapred.lib.KeyFieldBasedComparator',\n",
    "                            'mapred.text.key.comparator.options': '-k1,1rn',\n",
    "                            \"mapred.map.tasks\":8,\n",
    "                            \"mapred.reduce.tasks\":1\n",
    "                            }\n",
    "                     )]\n",
    "        \n",
    "    def mapper(self, _, line):\n",
    "        #get length, and count = count\n",
    "        cell = line.split('\\t')\n",
    "        ngram_length = len(cell[0])\n",
    "        count = int(cell[1])\n",
    "        yield ngram_length, count\n",
    "            \n",
    "    def combiner(self, length, count):\n",
    "        #combine\n",
    "        count = sum(count)\n",
    "        yield length, count\n",
    "        \n",
    "    def reducer(self, length, count):\n",
    "        #combine\n",
    "        count = sum(count)\n",
    "        yield length, count\n",
    "\n",
    "\n",
    "        \n",
    "if __name__ == '__main__':\n",
    "    distribution_53.run()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "delete: 's3://agunn-w261-hw5/distribution53/_SUCCESS'\n",
      "delete: 's3://agunn-w261-hw5/distribution53/part-00000'\n",
      "Got unexpected keyword arguments: ssh_tunnel\n",
      "using configs in /home/hduser/.mrjob.conf\n",
      "creating new scratch bucket mrjob-defa45cb0d5f744a\n",
      "using s3://mrjob-defa45cb0d5f744a/tmp/ as our scratch dir on S3\n",
      "creating tmp directory /tmp/mrjob_distribution_53.hduser.20160216.172543.656953\n",
      "writing master bootstrap script to /tmp/mrjob_distribution_53.hduser.20160216.172543.656953/b.py\n",
      "creating S3 bucket 'mrjob-defa45cb0d5f744a' to use as scratch space\n",
      "Copying non-input files into s3://mrjob-defa45cb0d5f744a/tmp/mrjob_distribution_53.hduser.20160216.172543.656953/files/\n",
      "Waiting 5.0s for S3 eventual consistency\n",
      "Creating Elastic MapReduce job flow\n",
      "Job flow created with ID: j-1LZKIY936I47I\n",
      "Created new job flow j-1LZKIY936I47I\n",
      "Job launched 31.1s ago, status STARTING: Provisioning Amazon EC2 capacity\n",
      "Job launched 62.1s ago, status STARTING: Provisioning Amazon EC2 capacity\n",
      "Job launched 93.0s ago, status STARTING: Provisioning Amazon EC2 capacity\n",
      "Job launched 124.1s ago, status STARTING: Provisioning Amazon EC2 capacity\n",
      "Job launched 155.2s ago, status STARTING: Provisioning Amazon EC2 capacity\n",
      "Job launched 186.3s ago, status STARTING: Configuring cluster software\n",
      "Job launched 217.3s ago, status STARTING: Configuring cluster software\n",
      "Job launched 248.5s ago, status RUNNING: Running step\n",
      "Job launched 279.6s ago, status RUNNING: Running step (mrjob_distribution_53.hduser.20160216.172543.656953: Step 1 of 1)\n",
      "Job launched 310.6s ago, status RUNNING: Running step (mrjob_distribution_53.hduser.20160216.172543.656953: Step 1 of 1)\n",
      "Job launched 341.5s ago, status RUNNING: Running step (mrjob_distribution_53.hduser.20160216.172543.656953: Step 1 of 1)\n",
      "Job launched 372.5s ago, status RUNNING: Running step (mrjob_distribution_53.hduser.20160216.172543.656953: Step 1 of 1)\n",
      "Job launched 403.5s ago, status RUNNING: Running step (mrjob_distribution_53.hduser.20160216.172543.656953: Step 1 of 1)\n",
      "Job launched 434.6s ago, status RUNNING: Running step (mrjob_distribution_53.hduser.20160216.172543.656953: Step 1 of 1)\n",
      "Job launched 465.7s ago, status RUNNING: Running step (mrjob_distribution_53.hduser.20160216.172543.656953: Step 1 of 1)\n",
      "Job launched 496.7s ago, status RUNNING: Running step (mrjob_distribution_53.hduser.20160216.172543.656953: Step 1 of 1)\n",
      "Job launched 527.7s ago, status RUNNING: Running step (mrjob_distribution_53.hduser.20160216.172543.656953: Step 1 of 1)\n",
      "Job launched 558.7s ago, status RUNNING: Running step (mrjob_distribution_53.hduser.20160216.172543.656953: Step 1 of 1)\n",
      "Job completed.\n",
      "Running time was 314.0s (not counting time spent waiting for the EC2 instances)\n",
      "ec2_key_pair_file not specified, going to S3\n",
      "Fetching counters from S3...\n",
      "Waiting 5.0s for S3 eventual consistency\n",
      "Counters from step 1:\n",
      "  File Input Format Counters :\n",
      "    Bytes Read: 2156069116\n",
      "  File Output Format Counters :\n",
      "    Bytes Written: 780\n",
      "  FileSystemCounters:\n",
      "    FILE_BYTES_READ: 56489\n",
      "    FILE_BYTES_WRITTEN: 5346327\n",
      "    HDFS_BYTES_READ: 23640\n",
      "    S3_BYTES_READ: 2156069116\n",
      "    S3_BYTES_WRITTEN: 780\n",
      "  Job Counters :\n",
      "    Launched map tasks: 194\n",
      "    Launched reduce tasks: 1\n",
      "    Rack-local map tasks: 192\n",
      "    SLOTS_MILLIS_MAPS: 4309094\n",
      "    SLOTS_MILLIS_REDUCES: 235128\n",
      "    Total time spent by all maps waiting after reserving slots (ms): 0\n",
      "    Total time spent by all reduces waiting after reserving slots (ms): 0\n",
      "  Map-Reduce Framework:\n",
      "    CPU time spent (ms): 2117690\n",
      "    Combine input records: 58682266\n",
      "    Combine output records: 9172\n",
      "    Map input bytes: 2156069116\n",
      "    Map input records: 58682266\n",
      "    Map output bytes: 372255868\n",
      "    Map output materialized bytes: 97070\n",
      "    Map output records: 58682266\n",
      "    Physical memory (bytes) snapshot: 183381745664\n",
      "    Reduce input groups: 80\n",
      "    Reduce input records: 9172\n",
      "    Reduce output records: 80\n",
      "    Reduce shuffle bytes: 97070\n",
      "    SPLIT_RAW_BYTES: 23640\n",
      "    Spilled Records: 18344\n",
      "    Total committed heap usage (bytes): 184212258816\n",
      "    Virtual memory (bytes) snapshot: 373841731584\n",
      "removing tmp directory /tmp/mrjob_distribution_53.hduser.20160216.172543.656953\n",
      "Removing all files in s3://mrjob-defa45cb0d5f744a/tmp/mrjob_distribution_53.hduser.20160216.172543.656953/\n",
      "Removing all files in s3://mrjob-defa45cb0d5f744a/tmp/logs/j-1LZKIY936I47I/\n",
      "Terminating job flow: j-1LZKIY936I47I\n"
     ]
    }
   ],
   "source": [
    "!s3cmd rm --recursive s3://agunn-w261-hw5/distribution53/\n",
    "! python mrjob_distribution_53.py -r emr s3://filtered-5grams \\\n",
    "    --output-dir=s3://agunn-w261-hw5/distribution53 \\\n",
    "    --no-output \\\n",
    "    --no-strict-protocol"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ERROR: Parameter problem: File HW5/distribution_out.txt already exists. Use either of --force / --continue / --skip-existing or give it a new name.\n",
      "------------------------------\n",
      "ngram length   count of length\n",
      "------------------------------\n",
      "159\t182\n",
      "128\t92\n",
      "119\t148\n",
      "106\t90\n",
      "103\t91\n",
      "91\t155\n",
      "90\t84\n",
      "89\t92\n",
      "86\t91\n",
      "84\t421\n"
     ]
    }
   ],
   "source": [
    "!s3cmd get s3://agunn-w261-hw5/distribution53/part* HW5/distribution_out.txt\n",
    "!echo \"------------------------------\"\n",
    "!echo \"ngram length   count of length\"\n",
    "!echo \"------------------------------\"\n",
    "!head HW5/distribution_out.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "###Plot the data graphically using a histogram."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.text.Text at 0x7f2df45fa250>"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAA6wAAAIACAYAAACGkuFlAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nUifZYZUkSZIkdZIdVkmSJElSJ/0Php8AyKxEUdMAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f2e1d425510>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.rcParams['figure.figsize'] = 16, 8  # plotsize \n",
    "\n",
    "df = pd.read_csv('HW5/distribution_out.txt',sep='\\t',header=None)\n",
    "df.columns = ['length','frequency']\n",
    "df = df.sort('length')\n",
    "df = df.set_index('length')\n",
    "my_plot = df.plot(kind='bar',legend=None,title=\"5-gram character length distribution\")\n",
    "my_plot.set_xlabel(\"5-gram length\")\n",
    "my_plot.set_ylabel(\"frequency\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-----\n",
    "##HW 5.4  (over 2Gig of Data)\n",
    "\n",
    "\n",
    "\n",
    ">For the remainder of this assignment you will work with two datasets:\n",
    "\n",
    ">#### 1: unit/systems test data set: SYSTEMS TEST DATASET\n",
    ">Three terms, A,B,C and their corresponding strip-docs of co-occurring terms\n",
    "\n",
    ">DocA {X:20, Y:30, Z:5}\n",
    ">DocB {X:100, Y:20}\n",
    ">DocC {M:5, N:20, Z:5}\n",
    "\n",
    "\n",
    ">#### 2: A large subset of the Google n-grams dataset as was described above\n",
    "\n",
    ">For each HW 5.4 -5.5.1 Please unit test and system test your code with respect \n",
    "to SYSTEMS TEST DATASET and show the results. \n",
    "Please compute the expected answer by hand and show your hand calculations for the \n",
    "SYSTEMS TEST DATASET. Then show the results you get with you system.\n",
    "\n",
    "\n",
    "In this part of the assignment we will focus on developing methods\n",
    "for detecting synonyms, using the Google 5-grams dataset. To accomplish\n",
    "this you must script two main tasks using MRJob:\n",
    "\n",
    "**(1) Build stripes of word co-ocurrence for the top 10,000 using the words ranked from 9001,-10,000 as a basis\n",
    "most frequently appearing words across the entire set of 5-grams,\n",
    "and output to a file in your bucket on s3 (bigram analysis, though the words are non-contiguous).**\n",
    "\n",
    "**(2) Using two (symmetric) comparison methods of your choice \n",
    "(e.g., correlations, distances, similarities), pairwise compare \n",
    "all stripes (vectors), and output to a file in your bucket on s3.**\n",
    "\n",
    "####==Design notes for (1)==\n",
    ">For this task you will be able to modify the pattern we used in HW 3.2\n",
    "(feel free to use the solution as reference). To total the word counts \n",
    "across the 5-grams, output the support from the mappers using the total \n",
    "order inversion pattern:\n",
    "\n",
    "><*word,count>\n",
    "\n",
    ">to ensure that the support arrives before the cooccurrences.\n",
    "\n",
    ">In addition to ensuring the determination of the total word counts,\n",
    "the mapper must also output co-occurrence counts for the pairs of\n",
    "words inside of each 5-gram. Treat these words as a basket,\n",
    "as we have in HW 3, but count all stripes or pairs in both orders,\n",
    "i.e., count both orderings: (word1,word2), and (word2,word1), to preserve\n",
    "symmetry in our output for (2).\n",
    "\n",
    "####==Design notes for (2)==\n",
    ">For this task you will have to determine a method of comparison.\n",
    "Here are a few that you might consider:\n",
    "\n",
    ">- Jaccard\n",
    ">- Cosine similarity\n",
    ">- Spearman correlation\n",
    ">- Euclidean distance\n",
    ">- Taxicab (Manhattan) distance\n",
    ">- Shortest path graph distance (a graph, because our data is symmetric!)\n",
    ">- Pearson correlation\n",
    ">- Kendall correlation\n",
    ">...\n",
    "\n",
    ">However, be cautioned that some comparison methods are more difficult to\n",
    "parallelize than others, and do not perform more associations than is necessary, \n",
    "since your choice of association will be symmetric.\n",
    "\n",
    "*Please use the inverted index (discussed in live session #5) based pattern to compute the pairwise (term-by-term) similarity matrix. *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "####Preparing SYSTEMS_TEST_DATASET and SYSTEMS_TEST_DATASET_freq\n",
    "Note: The frequency file was necessary because the frequency calculations from above expect a different input.  Being a small dataset, this was easy enough to do."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting createtest.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile createtest.py\n",
    "\n",
    "with open('SYSTEMS_TEST_DATASET.txt','w') as f:\n",
    "    f.write('DocA\\t{\"X\":20,\"Y\":30,\"Z\":5}\\n')\n",
    "    f.write('DocB\\t{\"X\":100,\"Y\":20}\\n')\n",
    "    f.write('DocC\\t{\"M\":5,\"N\":20,\"Z\":5}\\n')\n",
    "\n",
    "with open('SYSTEMS_TEST_DATASET_freq.txt','w') as f:\n",
    "    f.write('120\\t\"X\"\\n')\n",
    "    f.write('150\\t\"Y\"\\n')\n",
    "    f.write('10\\t\"Z\"\\n')\n",
    "    f.write('5\\t\"M\"\\n')\n",
    "    f.write('20\\t\"N\"\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DocA\t{\"X\":20,\"Y\":30,\"Z\":5}\n",
      "DocB\t{\"X\":100,\"Y\":20}\n",
      "DocC\t{\"M\":5,\"N\":20,\"Z\":5}\n",
      "120\t\"X\"\n",
      "150\t\"Y\"\n",
      "10\t\"Z\"\n",
      "5\t\"M\"\n",
      "20\t\"N\"\n"
     ]
    }
   ],
   "source": [
    "!python createtest.py\n",
    "!cat SYSTEMS_TEST_DATASET.txt\n",
    "!cat SYSTEMS_TEST_DATASET_freq.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###5.4\n",
    "\n",
    "**(1) Build stripes for the most frequent 10,000 words using cooccurence information based on\n",
    "the words ranked from 1001,-10,000 as a basis/vocabulary (drop stopword-like terms),\n",
    "and output to a file in your bucket on s3 (bigram analysis, though the words are non-contiguous).**\n",
    "\n",
    "\n",
    "\n",
    "####==Design notes for (1)==\n",
    ">For this task you will be able to modify the pattern we used in HW 3.2\n",
    "(feel free to use the solution as reference). To total the word counts \n",
    "across the 5-grams, output the support from the mappers using the total \n",
    "order inversion pattern:\n",
    "\n",
    "><*word,count>\n",
    "\n",
    ">to ensure that the support arrives before the cooccurrences.\n",
    "\n",
    ">In addition to ensuring the determination of the total word counts,\n",
    "the mapper must also output co-occurrence counts for the pairs of\n",
    "words inside of each 5-gram. Treat these words as a basket,\n",
    "as we have in HW 3, but count all stripes or pairs in both orders,\n",
    "i.e., count both orderings: (word1,word2), and (word2,word1), to preserve\n",
    "symmetry in our output for (2).\n",
    "\n",
    "*Please use the inverted index (discussed in live session #5) based pattern to compute the pairwise (term-by-term) similarity matrix. *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "####Please report the size of the cluster used and the amount of time it takes to run for the index construction task and for the synonym calculation task. How many pairs need to be processed (HINT: use the posting list length to calculate directly)? Report your  Cluster configuration!\n",
    "\n",
    "Cluster: \n",
    "    ec2_instance_type: m3.xlarge\n",
    "    ec2_master_instance_type: m1.medium\n",
    "    num_ec2_instances: 4\n",
    "\n",
    "Index Construction: \n",
    "    1051.0s\n",
    "    991 output stripes\n",
    "    \n",
    "Cosine:\n",
    "    137.0s\n",
    "    187434 lines out\n",
    "\n",
    "Jaccard:\n",
    "    196.0s\n",
    "    187434 lines out"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*The below is called bigram_occurrence, but really it is outputting the stripes for the inverse index*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting mrjob_bigram_occurrence.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile mrjob_bigram_occurrence.py\n",
    "#!/usr/bin/python\n",
    "## inverse_index.py\n",
    "## Author: Angela Gunn & Jing Xu\n",
    "## Description: Inverses an Index.\n",
    "\n",
    "\n",
    "from mrjob.job import MRJob\n",
    "from mrjob.step import MRStep\n",
    "import csv\n",
    "from sets import Set\n",
    "import ast\n",
    "import re\n",
    "\n",
    "\n",
    "WORD_RE = re.compile(r\"[A-Za-z0-9]+\")\n",
    "\n",
    "\n",
    "class bigram_occurrence(MRJob):\n",
    "    \n",
    "    doc_dict={} #global list\n",
    "    \n",
    "    def steps(self):\n",
    "        return [MRStep(mapper_init = self.mapper_init,\n",
    "                       mapper=self.mapper_main,\n",
    "                     combiner=self.combiner,\n",
    "                      reducer=self.reducer, \n",
    "                   jobconf={\n",
    "                            'mapred.output.key.comparator.class': 'org.apache.hadoop.mapred.lib.KeyFieldBasedComparator',\n",
    "                            'mapred.text.key.comparator.options': '-k1,1',\n",
    "                            \"mapred.map.tasks\":32,\n",
    "                            \"mapred.reduce.tasks\":16\n",
    "                            })]\n",
    "    \n",
    "    def mapper_init(self):\n",
    "        #load unigrams\n",
    "        self.unigrams = {}\n",
    "        with open('topwords','r') as f:\n",
    "            for line in f:\n",
    "                cells = line.strip().split('\\t')\n",
    "                word = cells[1].replace('\"','').strip()\n",
    "                self.unigrams[word] = int(cells[0])\n",
    "                yield \"*\"+word, int(cells[0])\n",
    "                \n",
    "                \n",
    "    def mapper_main(self, _, line):        \n",
    "        cell = line.strip().split('\\t')\n",
    "        words = WORD_RE.findall(cell[0])\n",
    "        # Filter 5-grams to only those in list\n",
    "        words = [w for w in words if w in self.unigrams.keys()]        \n",
    "        w_len = len(words)\n",
    "        for i in range(0, w_len): #for each word\n",
    "            key = words[i]\n",
    "            H = {}\n",
    "            for j in xrange(0, w_len): #for each word after this\n",
    "                w = words[j]\n",
    "                self.increment_counter(\"mapper\", \"word_pairs\", amount=1)\n",
    "                if key != w: \n",
    "                    H[w] = H.get(w,0) + 1\n",
    "                    self.increment_counter(\"mapper\", \"yield_pairs\", amount=1)\n",
    "            #emit \n",
    "            if len(H) > 0: yield key, H\n",
    "                  \n",
    "    \n",
    "    def combiner(self, key, stripes):\n",
    "        dic = {}\n",
    "        key = key.replace('\"','')\n",
    "        if key[0] == '*':\n",
    "            total = sum(stripes)\n",
    "            yield key, total\n",
    "        else:\n",
    "            for s in stripes:\n",
    "                for k, v in s.iteritems():\n",
    "                    k = k.replace('\"','')\n",
    "                    dic[k] = dic.get(k,0) + int(v)\n",
    "            yield key, dic\n",
    "\n",
    "    def reducer(self, key, stripes):\n",
    "        dic = {}\n",
    "        key = key.replace('\"','')\n",
    "        if key[0] == '*':\n",
    "            total = sum(stripes)\n",
    "            yield key, total\n",
    "        else:\n",
    "            for s in stripes:\n",
    "                for k, v in s.iteritems():\n",
    "                    k = k.replace('\"','')\n",
    "                    dic[k] = dic.get(k,0) + int(v)\n",
    "            self.increment_counter(\"reducer\", \"output_pairs\", amount=1)\n",
    "            yield key, dic\n",
    "        \n",
    "\n",
    "        \n",
    "if __name__ == '__main__':\n",
    "    bigram_occurrence.run()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "delete: 's3://agunn-w261-hw5/bigram54/_SUCCESS'\n",
      "delete: 's3://agunn-w261-hw5/bigram54/part-00000'\n",
      "delete: 's3://agunn-w261-hw5/bigram54/part-00001'\n",
      "delete: 's3://agunn-w261-hw5/bigram54/part-00002'\n",
      "delete: 's3://agunn-w261-hw5/bigram54/part-00003'\n",
      "delete: 's3://agunn-w261-hw5/bigram54/part-00004'\n",
      "delete: 's3://agunn-w261-hw5/bigram54/part-00005'\n",
      "delete: 's3://agunn-w261-hw5/bigram54/part-00006'\n",
      "delete: 's3://agunn-w261-hw5/bigram54/part-00007'\n",
      "delete: 's3://agunn-w261-hw5/bigram54/part-00008'\n",
      "delete: 's3://agunn-w261-hw5/bigram54/part-00009'\n",
      "delete: 's3://agunn-w261-hw5/bigram54/part-00010'\n",
      "delete: 's3://agunn-w261-hw5/bigram54/part-00011'\n",
      "delete: 's3://agunn-w261-hw5/bigram54/part-00012'\n",
      "delete: 's3://agunn-w261-hw5/bigram54/part-00013'\n",
      "delete: 's3://agunn-w261-hw5/bigram54/part-00014'\n",
      "delete: 's3://agunn-w261-hw5/bigram54/part-00015'\n",
      "Got unexpected keyword arguments: ssh_tunnel\n",
      "using configs in /home/hduser/.mrjob.conf\n",
      "creating new scratch bucket mrjob-002ecd56e2c2558d\n",
      "using s3://mrjob-002ecd56e2c2558d/tmp/ as our scratch dir on S3\n",
      "creating tmp directory /tmp/mrjob_bigram_occurrence.hduser.20160217.080152.217245\n",
      "writing master bootstrap script to /tmp/mrjob_bigram_occurrence.hduser.20160217.080152.217245/b.py\n",
      "creating S3 bucket 'mrjob-002ecd56e2c2558d' to use as scratch space\n",
      "Copying non-input files into s3://mrjob-002ecd56e2c2558d/tmp/mrjob_bigram_occurrence.hduser.20160217.080152.217245/files/\n",
      "Waiting 5.0s for S3 eventual consistency\n",
      "Creating Elastic MapReduce job flow\n",
      "Job flow created with ID: j-GNT0DMIN7FFE\n",
      "Created new job flow j-GNT0DMIN7FFE\n",
      "Job launched 30.9s ago, status STARTING: Provisioning Amazon EC2 capacity\n",
      "Job launched 61.9s ago, status STARTING: Provisioning Amazon EC2 capacity\n",
      "Job launched 92.9s ago, status STARTING: Provisioning Amazon EC2 capacity\n",
      "Job launched 123.9s ago, status STARTING: Provisioning Amazon EC2 capacity\n",
      "Job launched 154.9s ago, status STARTING: Provisioning Amazon EC2 capacity\n",
      "Job launched 185.8s ago, status STARTING: Configuring cluster software\n",
      "Job launched 216.8s ago, status STARTING: Configuring cluster software\n",
      "Job launched 247.9s ago, status RUNNING: Running step\n",
      "Job launched 278.9s ago, status RUNNING: Running step (mrjob_bigram_occurrence.hduser.20160217.080152.217245: Step 1 of 1)\n",
      "Job launched 309.9s ago, status RUNNING: Running step (mrjob_bigram_occurrence.hduser.20160217.080152.217245: Step 1 of 1)\n",
      "Job launched 340.8s ago, status RUNNING: Running step (mrjob_bigram_occurrence.hduser.20160217.080152.217245: Step 1 of 1)\n",
      "Job launched 371.9s ago, status RUNNING: Running step (mrjob_bigram_occurrence.hduser.20160217.080152.217245: Step 1 of 1)\n",
      "Job launched 403.1s ago, status RUNNING: Running step (mrjob_bigram_occurrence.hduser.20160217.080152.217245: Step 1 of 1)\n",
      "Job launched 434.0s ago, status RUNNING: Running step (mrjob_bigram_occurrence.hduser.20160217.080152.217245: Step 1 of 1)\n",
      "Job launched 465.1s ago, status RUNNING: Running step (mrjob_bigram_occurrence.hduser.20160217.080152.217245: Step 1 of 1)\n",
      "Job launched 496.2s ago, status RUNNING: Running step (mrjob_bigram_occurrence.hduser.20160217.080152.217245: Step 1 of 1)\n",
      "Job launched 527.2s ago, status RUNNING: Running step (mrjob_bigram_occurrence.hduser.20160217.080152.217245: Step 1 of 1)\n",
      "Job launched 558.3s ago, status RUNNING: Running step (mrjob_bigram_occurrence.hduser.20160217.080152.217245: Step 1 of 1)\n",
      "Job launched 589.4s ago, status RUNNING: Running step (mrjob_bigram_occurrence.hduser.20160217.080152.217245: Step 1 of 1)\n",
      "Job launched 620.3s ago, status RUNNING: Running step (mrjob_bigram_occurrence.hduser.20160217.080152.217245: Step 1 of 1)\n",
      "Job launched 651.4s ago, status RUNNING: Running step (mrjob_bigram_occurrence.hduser.20160217.080152.217245: Step 1 of 1)\n",
      "Job launched 682.3s ago, status RUNNING: Running step (mrjob_bigram_occurrence.hduser.20160217.080152.217245: Step 1 of 1)\n",
      "Job launched 713.4s ago, status RUNNING: Running step (mrjob_bigram_occurrence.hduser.20160217.080152.217245: Step 1 of 1)\n",
      "Job launched 744.4s ago, status RUNNING: Running step (mrjob_bigram_occurrence.hduser.20160217.080152.217245: Step 1 of 1)\n",
      "Job launched 775.3s ago, status RUNNING: Running step (mrjob_bigram_occurrence.hduser.20160217.080152.217245: Step 1 of 1)\n",
      "Job launched 806.3s ago, status RUNNING: Running step (mrjob_bigram_occurrence.hduser.20160217.080152.217245: Step 1 of 1)\n",
      "Job launched 837.3s ago, status RUNNING: Running step (mrjob_bigram_occurrence.hduser.20160217.080152.217245: Step 1 of 1)\n",
      "Job launched 868.4s ago, status RUNNING: Running step (mrjob_bigram_occurrence.hduser.20160217.080152.217245: Step 1 of 1)\n",
      "Job launched 899.3s ago, status RUNNING: Running step (mrjob_bigram_occurrence.hduser.20160217.080152.217245: Step 1 of 1)\n",
      "Job launched 930.3s ago, status RUNNING: Running step (mrjob_bigram_occurrence.hduser.20160217.080152.217245: Step 1 of 1)\n",
      "Job launched 961.3s ago, status RUNNING: Running step (mrjob_bigram_occurrence.hduser.20160217.080152.217245: Step 1 of 1)\n",
      "Job launched 992.2s ago, status RUNNING: Running step (mrjob_bigram_occurrence.hduser.20160217.080152.217245: Step 1 of 1)\n",
      "Job launched 1023.2s ago, status RUNNING: Running step (mrjob_bigram_occurrence.hduser.20160217.080152.217245: Step 1 of 1)\n",
      "Job launched 1054.4s ago, status RUNNING: Running step (mrjob_bigram_occurrence.hduser.20160217.080152.217245: Step 1 of 1)\n",
      "Job launched 1085.3s ago, status RUNNING: Running step (mrjob_bigram_occurrence.hduser.20160217.080152.217245: Step 1 of 1)\n",
      "Job launched 1116.3s ago, status RUNNING: Running step (mrjob_bigram_occurrence.hduser.20160217.080152.217245: Step 1 of 1)\n",
      "Job launched 1147.3s ago, status RUNNING: Running step (mrjob_bigram_occurrence.hduser.20160217.080152.217245: Step 1 of 1)\n",
      "Job launched 1178.3s ago, status RUNNING: Running step (mrjob_bigram_occurrence.hduser.20160217.080152.217245: Step 1 of 1)\n",
      "Job launched 1209.3s ago, status RUNNING: Running step (mrjob_bigram_occurrence.hduser.20160217.080152.217245: Step 1 of 1)\n",
      "Job launched 1240.4s ago, status RUNNING: Running step (mrjob_bigram_occurrence.hduser.20160217.080152.217245: Step 1 of 1)\n",
      "Job launched 1271.4s ago, status RUNNING: Running step (mrjob_bigram_occurrence.hduser.20160217.080152.217245: Step 1 of 1)\n",
      "Job launched 1302.5s ago, status RUNNING: Running step (mrjob_bigram_occurrence.hduser.20160217.080152.217245: Step 1 of 1)\n",
      "Job completed.\n",
      "Running time was 1051.0s (not counting time spent waiting for the EC2 instances)\n",
      "ec2_key_pair_file not specified, going to S3\n",
      "Fetching counters from S3...\n",
      "Waiting 5.0s for S3 eventual consistency\n",
      "Counters from step 1:\n",
      "  File Input Format Counters :\n",
      "    Bytes Read: 2156069116\n",
      "  File Output Format Counters :\n",
      "    Bytes Written: 241718\n",
      "  FileSystemCounters:\n",
      "    FILE_BYTES_READ: 449403\n",
      "    FILE_BYTES_WRITTEN: 10061824\n",
      "    HDFS_BYTES_READ: 23640\n",
      "    S3_BYTES_READ: 2156069116\n",
      "    S3_BYTES_WRITTEN: 241718\n",
      "  Job Counters :\n",
      "    Launched map tasks: 195\n",
      "    Launched reduce tasks: 18\n",
      "    Rack-local map tasks: 193\n",
      "    SLOTS_MILLIS_MAPS: 16790033\n",
      "    SLOTS_MILLIS_REDUCES: 5418480\n",
      "    Total time spent by all maps waiting after reserving slots (ms): 0\n",
      "    Total time spent by all reduces waiting after reserving slots (ms): 0\n",
      "  Map-Reduce Framework:\n",
      "    CPU time spent (ms): 10168260\n",
      "    Combine input records: 219151\n",
      "    Combine output records: 215958\n",
      "    Map input bytes: 2156069116\n",
      "    Map input records: 58682266\n",
      "    Map output bytes: 4246738\n",
      "    Map output materialized bytes: 3850819\n",
      "    Map output records: 219151\n",
      "    Physical memory (bytes) snapshot: 140355371008\n",
      "    Reduce input groups: 1991\n",
      "    Reduce input records: 215958\n",
      "    Reduce output records: 1991\n",
      "    Reduce shuffle bytes: 3850819\n",
      "    SPLIT_RAW_BYTES: 23640\n",
      "    Spilled Records: 431916\n",
      "    Total committed heap usage (bytes): 150166568960\n",
      "    Virtual memory (bytes) snapshot: 402058670080\n",
      "  mapper:\n",
      "    word_pairs: 1477162\n",
      "    yield_pairs: 29480\n",
      "  reducer:\n",
      "    output_pairs: 991\n",
      "removing tmp directory /tmp/mrjob_bigram_occurrence.hduser.20160217.080152.217245\n",
      "Removing all files in s3://mrjob-002ecd56e2c2558d/tmp/mrjob_bigram_occurrence.hduser.20160217.080152.217245/\n",
      "Removing all files in s3://mrjob-002ecd56e2c2558d/tmp/logs/j-GNT0DMIN7FFE/\n",
      "Terminating job flow: j-GNT0DMIN7FFE\n"
     ]
    }
   ],
   "source": [
    "!s3cmd rm --recursive s3://agunn-w261-hw5/bigram54/\n",
    "! python mrjob_bigram_occurrence.py -r emr s3://filtered-5grams \\\n",
    "    --file 's3://agunn-w261-hw5/topwords_touse9000.txt#topwords' \\\n",
    "    --output-dir=s3://agunn-w261-hw5/bigram54 \\\n",
    "    --no-output \\\n",
    "    --no-strict-protocol"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "download: 's3://agunn-w261-hw5/bigram54/part-00000' -> 'HW5/54_out.txt'  [1 of 1]\n",
      "download: 's3://agunn-w261-hw5/bigram54/part-00000' -> 'HW5/54_out.txt'  [1 of 1]\n",
      " 16735 of 16735   100% in    0s    82.05 kB/s  done\n",
      "\"weaken\"\t{\"vigor\": 1, \"critically\": 1, \"uterine\": 1, \"injure\": 1, \"diversion\": 1, \"abolish\": 1, \"reinforce\": 4, \"shrink\": 1, \"undermine\": 2}\n"
     ]
    }
   ],
   "source": [
    "!rm HW5/54_out.txt\n",
    "!s3cmd get s3://agunn-w261-hw5/bigram54/part-00000 HW5/54_out.txt\n",
    "\n",
    "!tail -1 HW5/54_out.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "**(2) Using two (symmetric) comparison methods of your choice \n",
    "(e.g., correlations, distances, similarities), pairwise compare \n",
    "all stripes (vectors), and output to a file in your bucket on s3.**\n",
    "\n",
    "####==Design notes for (2)==\n",
    ">For this task you will have to determine a method of comparison.\n",
    "Here are a few that you might consider:\n",
    "\n",
    ">- Jaccard\n",
    ">- Cosine similarity\n",
    ">- Spearman correlation\n",
    ">- Euclidean distance\n",
    ">- Taxicab (Manhattan) distance\n",
    ">- Shortest path graph distance (a graph, because our data is symmetric!)\n",
    ">- Pearson correlation\n",
    ">- Kendall correlation\n",
    ">...\n",
    "\n",
    ">However, be cautioned that some comparison methods are more difficult to\n",
    "parallelize than others, and do not perform more associations than is necessary, \n",
    "since your choice of association will be symmetric.\n",
    "\n",
    "*Please use the inverted index (discussed in live session #5) based pattern to compute the pairwise (term-by-term) similarity matrix. *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "####Rough Work\n",
    "Mapper1:\n",
    "  \n",
    "     #   coming in with\n",
    "        #   wordA {w1:#, w2:#, w3:#}\n",
    "        #   coming in with\n",
    "        #   wordB {w1:#, w2:#}\n",
    "        #change to \n",
    "        #   wordA {w1:1, w2:1, w3:1}\n",
    "        #   wordB {w1:1, w2:1}\n",
    "        #get lengths\n",
    "        #   LA = sqrt(1^2 + 1^2 + 1^2) = sqrt(3)\n",
    "        #   LB = sqrt(1^2 + 1^2) = sqrt(2)\n",
    "        #yield: \n",
    "        #   wordA {w1:1/LA}..., w2:1/LA, w3:1/LA}\n",
    "        #   wordB {w1:1/LB, w2:1/LB}\n",
    "    \n",
    "Reducer1:\n",
    "     #   coming in with:\n",
    "        #    wordA [{w1:1/LA},{w2:1/LA}] \n",
    "        \n",
    "Mapper2:\n",
    "     #coming in with:\n",
    "        #   wordA {w1:#, w2:#, w3:#}\n",
    "        #yield:\n",
    "        #   w1  {wordA:#}\n",
    "        #   w2  {wordA:#}\n",
    "        #   w3  {wordA:#}\n",
    "        \n",
    "Reducer2:\n",
    "     #coming in with:\n",
    "        #   w1  [{wordA:#},{wordB:#}]\n",
    "        #   w2  [{wordA:#},{wordB:#}]\n",
    "        #   w3  [{wordA:#}]\n",
    "        #put each pair in global dic to calculate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting mrjob_Cosine_Inverted_Index.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile mrjob_Cosine_Inverted_Index.py\n",
    "#!/usr/bin/python\n",
    "## top_pages_43.py\n",
    "## Author: Angela Gunn  & Jing Xu\n",
    "## Description: Finds the top pages from the log\n",
    "\n",
    "\n",
    "from mrjob.job import MRJob\n",
    "from mrjob.step import MRStep\n",
    "import csv\n",
    "from sets import Set\n",
    "import ast, json\n",
    "import math\n",
    "\n",
    "\n",
    "\n",
    "class Cosine_Inverted_Index(MRJob):\n",
    "\n",
    "    global_doc_dict = {}\n",
    "    def steps(self):\n",
    "        return [\n",
    "            MRStep(mapper=self.mapper, reducer= self.reducer,jobconf={\n",
    "                    \"mapred.map.tasks\":16,\n",
    "                    \"mapred.reduce.tasks\":8\n",
    "                    }\n",
    "                  ),\n",
    "            MRStep(mapper=self.mapper2 ,combiner=self.combiner2, reducer=self.reducer2,\n",
    "                   jobconf={\n",
    "                    \"mapred.map.tasks\":8,\n",
    "                    \"mapred.reduce.tasks\":4\n",
    "                    }\n",
    "                  )\n",
    "               ]\n",
    "    \n",
    "    def mapper(self, _, line):\n",
    "        #output doc   (key, count/sqrt)\n",
    "        total_sqrt = 0\n",
    "        total_sq_cnt= 0 \n",
    "        key,terms = line.strip().split('\\t')\n",
    "        key = key.replace('\"', '')\n",
    "        if key[0] != '*':\n",
    "            docs = eval(terms)\n",
    "            #normalise the counts for cosine similarity\n",
    "            for word, count in docs.iteritems():\n",
    "                total_sq_cnt += count**2\n",
    "            total_sqrt = math.sqrt(total_sq_cnt)\n",
    "            for doc,count in docs.iteritems():\n",
    "                yield doc,(key, 1.0*count/total_sqrt)\n",
    "    \n",
    "    def reducer(self,key,value):\n",
    "        #output key, {doc:dist}\n",
    "        doc_list ={}\n",
    "        for doc,dist in value:\n",
    "            doc_list[doc]=dist\n",
    "        yield key, doc_list\n",
    "        \n",
    "    def mapper2(self,key,value):\n",
    "        #output (word, word)  mutiplied key values\n",
    "        keys = value.keys()\n",
    "        for key1 in keys:\n",
    "            for key2 in keys:\n",
    "                if(key1 == key2):\n",
    "                    continue\n",
    "                multiplied_keys = value[key1]*value[key2]\n",
    "                yield(key1,key2),multiplied_keys\n",
    "    \n",
    "    def combiner2(self,key,value):\n",
    "        yield key,sum(value)\n",
    "    \n",
    "    def reducer2(self,key,value):\n",
    "        self.increment_counter(\"reducer\", \"output_pairs\", amount=1)\n",
    "        yield key,sum(value)\n",
    "\n",
    "        \n",
    "if __name__ == '__main__':\n",
    "    Cosine_Inverted_Index.run()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "####Test with SYSTEMS_TEST_DATASET"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:mrjob.runner:\n",
      "WARNING:mrjob.runner:PLEASE NOTE: Starting in mrjob v0.5.0, protocols will be strict by default. It's recommended you run your job with --strict-protocols or set up mrjob.conf as described at https://pythonhosted.org/mrjob/whats-new.html#ready-for-strict-protocols\n",
      "WARNING:mrjob.runner:\n"
     ]
    }
   ],
   "source": [
    "from mrjob_Cosine_Inverted_Index import Cosine_Inverted_Index\n",
    "import os\n",
    "\n",
    "#mr_job = Cosine_Inverted_Index(args=['HW5/54_out.txt'])\n",
    "mr_job = Cosine_Inverted_Index(args=['SYSTEMS_TEST_DATASET.txt'])\n",
    "\n",
    "output_file = \"SYSTEMS_TEST_COSINE.out\"\n",
    "try:\n",
    "    os.remove(output_file)\n",
    "except OSError:\n",
    "    pass\n",
    "\n",
    "with mr_job.make_runner() as runner, open(output_file, 'a') as f: \n",
    "    runner.run()\n",
    "    # stream_output: get access of the output \n",
    "    for line in runner.stream_output():\n",
    "        #print mr_job.parse_output_line(line)\n",
    "        f.write(str(line))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[\"DocA\", \"DocC\"]\t0.0323761954119088\r\n",
      "[\"DocB\", \"DocA\"]\t0.7004041959724748\r\n",
      "[\"DocC\", \"DocA\"]\t0.0323761954119088\r\n",
      "[\"DocA\", \"DocB\"]\t0.7004041959724748\r\n"
     ]
    }
   ],
   "source": [
    "!cat SYSTEMS_TEST_COSINE.out"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "####Test with 5-grams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#code to make things public for future functions\n",
    "!s3cmd setacl s3://agunn-w261-hw5/bigram54/* --acl-public --recursive"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "delete: 's3://agunn-w261-hw5/cosine54/_SUCCESS'\n",
      "delete: 's3://agunn-w261-hw5/cosine54/part-00000'\n",
      "delete: 's3://agunn-w261-hw5/cosine54/part-00001'\n",
      "delete: 's3://agunn-w261-hw5/cosine54/part-00002'\n",
      "delete: 's3://agunn-w261-hw5/cosine54/part-00003'\n",
      "Got unexpected keyword arguments: ssh_tunnel\n",
      "using configs in /home/hduser/.mrjob.conf\n",
      "creating new scratch bucket mrjob-d2c0116cd8f7c1da\n",
      "using s3://mrjob-d2c0116cd8f7c1da/tmp/ as our scratch dir on S3\n",
      "creating tmp directory /tmp/mrjob_Cosine_Inverted_Index.hduser.20160217.082603.736169\n",
      "writing master bootstrap script to /tmp/mrjob_Cosine_Inverted_Index.hduser.20160217.082603.736169/b.py\n",
      "creating S3 bucket 'mrjob-d2c0116cd8f7c1da' to use as scratch space\n",
      "Copying non-input files into s3://mrjob-d2c0116cd8f7c1da/tmp/mrjob_Cosine_Inverted_Index.hduser.20160217.082603.736169/files/\n",
      "Waiting 5.0s for S3 eventual consistency\n",
      "Creating Elastic MapReduce job flow\n",
      "Job flow created with ID: j-1M4ELXM5IGXGU\n",
      "Created new job flow j-1M4ELXM5IGXGU\n",
      "Job launched 31.0s ago, status STARTING: Provisioning Amazon EC2 capacity\n",
      "Job launched 62.0s ago, status STARTING: Provisioning Amazon EC2 capacity\n",
      "Job launched 92.9s ago, status STARTING: Provisioning Amazon EC2 capacity\n",
      "Job launched 124.8s ago, status STARTING: Provisioning Amazon EC2 capacity\n",
      "Job launched 155.9s ago, status STARTING: Provisioning Amazon EC2 capacity\n",
      "Job launched 186.8s ago, status STARTING: Provisioning Amazon EC2 capacity\n",
      "Job launched 217.8s ago, status STARTING: Configuring cluster software\n",
      "Job launched 248.8s ago, status STARTING: Configuring cluster software\n",
      "Job launched 279.7s ago, status BOOTSTRAPPING: Running bootstrap actions\n",
      "Job launched 310.7s ago, status RUNNING: Running step\n",
      "Job launched 341.7s ago, status RUNNING: Running step (mrjob_Cosine_Inverted_Index.hduser.20160217.082603.736169: Step 1 of 2)\n",
      "Job launched 372.7s ago, status RUNNING: Running step (mrjob_Cosine_Inverted_Index.hduser.20160217.082603.736169: Step 1 of 2)\n",
      "Job launched 403.6s ago, status RUNNING: Running step (mrjob_Cosine_Inverted_Index.hduser.20160217.082603.736169: Step 1 of 2)\n",
      "Job launched 434.6s ago, status RUNNING: Running step (mrjob_Cosine_Inverted_Index.hduser.20160217.082603.736169: Step 2 of 2)\n",
      "Job completed.\n",
      "Running time was 137.0s (not counting time spent waiting for the EC2 instances)\n",
      "ec2_key_pair_file not specified, going to S3\n",
      "Fetching counters from S3...\n",
      "Waiting 5.0s for S3 eventual consistency\n",
      "Counters from step 1:\n",
      "  File Input Format Counters :\n",
      "    Bytes Read: 251799\n",
      "  File Output Format Counters :\n",
      "    Bytes Written: 485358\n",
      "  FileSystemCounters:\n",
      "    FILE_BYTES_READ: 324470\n",
      "    FILE_BYTES_WRITTEN: 1493590\n",
      "    HDFS_BYTES_READ: 1932\n",
      "    HDFS_BYTES_WRITTEN: 485358\n",
      "    S3_BYTES_READ: 251799\n",
      "  Job Counters :\n",
      "    Launched map tasks: 21\n",
      "    Launched reduce tasks: 8\n",
      "    Rack-local map tasks: 21\n",
      "    SLOTS_MILLIS_MAPS: 230468\n",
      "    SLOTS_MILLIS_REDUCES: 89836\n",
      "    Total time spent by all maps waiting after reserving slots (ms): 0\n",
      "    Total time spent by all reduces waiting after reserving slots (ms): 0\n",
      "  Map-Reduce Framework:\n",
      "    CPU time spent (ms): 58680\n",
      "    Combine input records: 0\n",
      "    Combine output records: 0\n",
      "    Map input bytes: 241718\n",
      "    Map input records: 1991\n",
      "    Map output bytes: 639779\n",
      "    Map output materialized bytes: 389169\n",
      "    Map output records: 14644\n",
      "    Physical memory (bytes) snapshot: 8946937856\n",
      "    Reduce input groups: 991\n",
      "    Reduce input records: 14644\n",
      "    Reduce output records: 991\n",
      "    Reduce shuffle bytes: 389169\n",
      "    SPLIT_RAW_BYTES: 1932\n",
      "    Spilled Records: 29288\n",
      "    Total committed heap usage (bytes): 11635523584\n",
      "    Virtual memory (bytes) snapshot: 56642785280\n",
      "Counters from step 2:\n",
      "  File Input Format Counters :\n",
      "    Bytes Read: 490226\n",
      "  File Output Format Counters :\n",
      "    Bytes Written: 8353507\n",
      "  FileSystemCounters:\n",
      "    FILE_BYTES_READ: 6000256\n",
      "    FILE_BYTES_WRITTEN: 12969384\n",
      "    HDFS_BYTES_READ: 491747\n",
      "    S3_BYTES_WRITTEN: 8353507\n",
      "  Job Counters :\n",
      "    Data-local map tasks: 9\n",
      "    Launched map tasks: 9\n",
      "    Launched reduce tasks: 4\n",
      "    SLOTS_MILLIS_MAPS: 109561\n",
      "    SLOTS_MILLIS_REDUCES: 66829\n",
      "    Total time spent by all maps waiting after reserving slots (ms): 0\n",
      "    Total time spent by all reduces waiting after reserving slots (ms): 0\n",
      "  Map-Reduce Framework:\n",
      "    CPU time spent (ms): 43390\n",
      "    Combine input records: 257530\n",
      "    Combine output records: 243678\n",
      "    Map input bytes: 485358\n",
      "    Map input records: 991\n",
      "    Map output bytes: 11510434\n",
      "    Map output materialized bytes: 6616910\n",
      "    Map output records: 257530\n",
      "    Physical memory (bytes) snapshot: 3887824896\n",
      "    Reduce input groups: 187434\n",
      "    Reduce input records: 243678\n",
      "    Reduce output records: 187434\n",
      "    Reduce shuffle bytes: 6616910\n",
      "    SPLIT_RAW_BYTES: 1521\n",
      "    Spilled Records: 487356\n",
      "    Total committed heap usage (bytes): 5052039168\n",
      "    Virtual memory (bytes) snapshot: 25813958656\n",
      "  reducer:\n",
      "    output_pairs: 187434\n",
      "removing tmp directory /tmp/mrjob_Cosine_Inverted_Index.hduser.20160217.082603.736169\n",
      "Removing all files in s3://mrjob-d2c0116cd8f7c1da/tmp/mrjob_Cosine_Inverted_Index.hduser.20160217.082603.736169/\n",
      "Removing all files in s3://mrjob-d2c0116cd8f7c1da/tmp/logs/j-1M4ELXM5IGXGU/\n",
      "Terminating job flow: j-1M4ELXM5IGXGU\n"
     ]
    }
   ],
   "source": [
    "!s3cmd rm --recursive s3://agunn-w261-hw5/cosine54/\n",
    "! python mrjob_Cosine_Inverted_Index.py -r emr s3://agunn-w261-hw5/bigram54/part* \\\n",
    "    --output-dir=s3://agunn-w261-hw5/cosine54 \\\n",
    "    --no-output \\\n",
    "    --no-strict-protocol"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "download: 's3://agunn-w261-hw5/cosine54/part-00000' -> 'HW5/cosine54_out.txt'  [1 of 1]\n",
      "download: 's3://agunn-w261-hw5/cosine54/part-00000' -> 'HW5/cosine54_out.txt'  [1 of 1]\n",
      " 2087595 of 2087595   100% in    0s     2.84 MB/s  done\n",
      "[\"AB\", \"Archaeology\"]\t0.0048318200697723777\n",
      "[\"AB\", \"Banking\"]\t0.00085993106191805887\n",
      "[\"AB\", \"Brown\"]\t0.0010843072791358313\n",
      "[\"AB\", \"Colleges\"]\t0.00015764451732376358\n",
      "[\"AB\", \"Crime\"]\t0.0021306323807380781\n",
      "[\"AB\", \"Electrical\"]\t0.0055941241338269753\n",
      "[\"AB\", \"Excellency\"]\t0.0077924502102389211\n",
      "[\"AB\", \"Female\"]\t0.0007356246980488243\n",
      "[\"AB\", \"Fleet\"]\t0.00068170137651950345\n",
      "[\"AB\", \"Gallery\"]\t0.00085036534653965836\n",
      "46864 HW5/cosine54_out.txt\n"
     ]
    }
   ],
   "source": [
    "!rm  HW5/cosine54_out.txt\n",
    "!s3cmd get s3://agunn-w261-hw5/cosine54/part-00000 HW5/cosine54_out.txt\n",
    "\n",
    "!head HW5/cosine54_out.txt\n",
    "!wc -l HW5/cosine54_out.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "####Rough Work - Jaccard Index\n",
    "Mapper1:\n",
    "  \n",
    "     #   coming in with\n",
    "        #   wordA {w1:#, w2:#, w3:#}\n",
    "        #   coming in with\n",
    "        #   wordB {w1:#, w2:#}\n",
    "        #change to \n",
    "        #   wordA {w1:1, w2:1, w3:1}\n",
    "        #   wordB {w1:1, w2:1}\n",
    "        #yield: \n",
    "        #   wordA (w1:1)..., (w2:1), (w3:1)\n",
    "        #   wordB (w1:1), (w2:1)\n",
    "    \n",
    "Reducer1:\n",
    "     #   coming in with:\n",
    "        #    wordA [(w1:1),(w2:1)] \n",
    "        #yield\n",
    "        #    wordA  w1\n",
    "        #    wordA  w2\n",
    "        \n",
    "Mapper2:\n",
    "     #coming in with:\n",
    "        #   wordA {w1:#, w2:#, w3:#}\n",
    "        #yield:\n",
    "        #   (*,w1) 1\n",
    "        #   (w1,w2) 1\n",
    "        #   (w1,w3)  1\n",
    "        \n",
    "Reducer2:\n",
    "     #yield:\n",
    "        #   (w1,w2)  sum\n",
    "        \n",
    "Mapper3:\n",
    "     #coming in with \n",
    "        #   (w1,w2)  count\n",
    "     # if w1 == * then this is a count of the number of times this word occurrs |w2|\n",
    "     # We will have all the counts - store in dic\n",
    "     # calc = |xy| / (|x| + |y| - |xy|)\n",
    "     # yield\n",
    "       # (x,y) calc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting mrjob_Jaccard_Index.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile mrjob_Jaccard_Index.py\n",
    "#!/usr/bin/python\n",
    "## top_pages_43.py\n",
    "## Author: Angela Gunn  & Jing Xu\n",
    "## Description: Finds the top pages from the log\n",
    "\n",
    "\n",
    "from mrjob.job import MRJob\n",
    "from mrjob.step import MRStep\n",
    "import csv\n",
    "from sets import Set\n",
    "import ast, json\n",
    "import math\n",
    "\n",
    "\n",
    "class Jaccard_Index(MRJob):\n",
    "    global_doc_dict = {}\n",
    "    def steps(self):\n",
    "        return [\n",
    "            MRStep(mapper=self.mapper , reducer= self.reducer, \n",
    "                   jobconf={\n",
    "                    \"mapred.map.tasks\":16,\n",
    "                    \"mapred.reduce.tasks\":8\n",
    "                    }),\n",
    "            MRStep(mapper=self.mapper2 ,combiner=self.combiner2, reducer=self.reducer2,\n",
    "                   jobconf={\n",
    "                    \"mapred.map.tasks\":8,\n",
    "                    \"mapred.reduce.tasks\":4\n",
    "                    }\n",
    "                  ),\n",
    "             MRStep(reducer=self.jaccard_cal, \n",
    "                    jobconf={\n",
    "                    \"mapred.map.tasks\":4,\n",
    "                    \"mapred.reduce.tasks\":1\n",
    "                    })\n",
    "            \n",
    "               ]\n",
    "    \n",
    "    def mapper(self, _, line):\n",
    "        key,terms = line.strip().split('\\t')\n",
    "        key = key.replace('\"', '')\n",
    "        if key[0] != '*':  #* represents a word with count - not a word with dictionary\n",
    "            docs = eval(terms).keys()\n",
    "            for doc in docs:\n",
    "                yield doc,key\n",
    "    \n",
    "    def reducer(self,key,value):\n",
    "        doc_list ={}\n",
    "        for v in value:\n",
    "            doc_list[v]=1\n",
    "        yield key, doc_list.keys()\n",
    "        \n",
    "    def mapper2(self,key,value):\n",
    "        doc_list = list(value)\n",
    "        for key1 in doc_list:\n",
    "            starkey = '*' + key1  #addint the * back... this seems redundant, but handled a strange error.\n",
    "            yield (starkey, key1),1\n",
    "            for key2 in doc_list:\n",
    "                if(key1 != key2):\n",
    "                    yield(key1,key2),1\n",
    "    \n",
    "    def combiner2(self,key,value):\n",
    "        yield key,sum(value)\n",
    "    \n",
    "    def reducer2(self,key,value):\n",
    "        yield key,sum(value)\n",
    "    \n",
    "    def jaccard_cal(self,key,value):\n",
    "        docA,docB = key\n",
    "\n",
    "        if docA.startswith('*'): #|doc|\n",
    "            self.global_doc_dict[docB] = sum(value)\n",
    "        else:  #at this point we have all the |doc|\n",
    "            ab = sum(value)\n",
    "            calc = 1.0*ab / (self.global_doc_dict[docA] + self.global_doc_dict[docB] - ab)\n",
    "            \n",
    "            self.increment_counter(\"reducer\", \"output_pairs\", amount=1)\n",
    "            yield (docA,docB), calc\n",
    "        \n",
    "if __name__ == '__main__':\n",
    "    Jaccard_Index.run()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "####Test with SYSTEMS_TEST_DATASET"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:mrjob.runner:\n",
      "WARNING:mrjob.runner:PLEASE NOTE: Starting in mrjob v0.5.0, protocols will be strict by default. It's recommended you run your job with --strict-protocols or set up mrjob.conf as described at https://pythonhosted.org/mrjob/whats-new.html#ready-for-strict-protocols\n",
      "WARNING:mrjob.runner:\n"
     ]
    }
   ],
   "source": [
    "from mrjob_Jaccard_Index import Jaccard_Index\n",
    "import os\n",
    "\n",
    "\n",
    "mr_job = Jaccard_Index(args=['SYSTEMS_TEST_DATASET.txt'])\n",
    "\n",
    "output_file = \"SYSTEMS_TEST_JACCARD.out\"\n",
    "try:\n",
    "    os.remove(output_file)\n",
    "except OSError:\n",
    "    pass\n",
    "\n",
    "with mr_job.make_runner() as runner, open(output_file, 'a') as f: \n",
    "    runner.run()\n",
    "    # stream_output: get access of the output \n",
    "    for line in runner.stream_output():\n",
    "        #print mr_job.parse_output_line(line)\n",
    "        f.write(str(line))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[\"DocA\", \"DocB\"]\t0.6666666666666666\r\n",
      "[\"DocA\", \"DocC\"]\t0.2\r\n",
      "[\"DocB\", \"DocA\"]\t0.6666666666666666\r\n",
      "[\"DocC\", \"DocA\"]\t0.2\r\n"
     ]
    }
   ],
   "source": [
    "!head SYSTEMS_TEST_JACCARD.out\n",
    "#!tail bigram54_Jaccard.out"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "####Test with 5-grams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "delete: 's3://agunn-w261-hw5/jaccard54/_SUCCESS'\n",
      "delete: 's3://agunn-w261-hw5/jaccard54/part-00000'\n",
      "Got unexpected keyword arguments: ssh_tunnel\n",
      "using configs in /home/hduser/.mrjob.conf\n",
      "creating new scratch bucket mrjob-ba3ec136dc70d97b\n",
      "using s3://mrjob-ba3ec136dc70d97b/tmp/ as our scratch dir on S3\n",
      "creating tmp directory /tmp/mrjob_Jaccard_Index.hduser.20160217.083507.925889\n",
      "writing master bootstrap script to /tmp/mrjob_Jaccard_Index.hduser.20160217.083507.925889/b.py\n",
      "creating S3 bucket 'mrjob-ba3ec136dc70d97b' to use as scratch space\n",
      "Copying non-input files into s3://mrjob-ba3ec136dc70d97b/tmp/mrjob_Jaccard_Index.hduser.20160217.083507.925889/files/\n",
      "Waiting 5.0s for S3 eventual consistency\n",
      "Creating Elastic MapReduce job flow\n",
      "Job flow created with ID: j-39USKDJFLC9SZ\n",
      "Created new job flow j-39USKDJFLC9SZ\n",
      "Job launched 30.9s ago, status STARTING: Provisioning Amazon EC2 capacity\n",
      "Job launched 62.0s ago, status STARTING: Provisioning Amazon EC2 capacity\n",
      "Job launched 93.0s ago, status STARTING: Provisioning Amazon EC2 capacity\n",
      "Job launched 124.0s ago, status STARTING: Provisioning Amazon EC2 capacity\n",
      "Job launched 155.0s ago, status STARTING: Configuring cluster software\n",
      "Job launched 186.0s ago, status BOOTSTRAPPING: Running bootstrap actions\n",
      "Job launched 217.0s ago, status BOOTSTRAPPING: Running bootstrap actions\n",
      "Job launched 248.0s ago, status RUNNING: Running step\n",
      "Job launched 278.9s ago, status RUNNING: Running step\n",
      "Job launched 309.9s ago, status RUNNING: Running step (mrjob_Jaccard_Index.hduser.20160217.083507.925889: Step 1 of 3)\n",
      "Job launched 340.8s ago, status RUNNING: Running step (mrjob_Jaccard_Index.hduser.20160217.083507.925889: Step 1 of 3)\n",
      "Job launched 371.8s ago, status RUNNING: Running step (mrjob_Jaccard_Index.hduser.20160217.083507.925889: Step 2 of 3)\n",
      "Job launched 402.7s ago, status RUNNING: Running step (mrjob_Jaccard_Index.hduser.20160217.083507.925889: Step 3 of 3)\n",
      "Job launched 433.8s ago, status RUNNING: Running step (mrjob_Jaccard_Index.hduser.20160217.083507.925889: Step 3 of 3)\n",
      "Job launched 464.7s ago, status RUNNING: Running step (mrjob_Jaccard_Index.hduser.20160217.083507.925889: Step 3 of 3)\n",
      "Job completed.\n",
      "Running time was 196.0s (not counting time spent waiting for the EC2 instances)\n",
      "ec2_key_pair_file not specified, going to S3\n",
      "Fetching counters from S3...\n",
      "Waiting 5.0s for S3 eventual consistency\n",
      "Counters from step 1:\n",
      "  File Input Format Counters :\n",
      "    Bytes Read: 251799\n",
      "  File Output Format Counters :\n",
      "    Bytes Written: 177049\n",
      "  FileSystemCounters:\n",
      "    FILE_BYTES_READ: 153306\n",
      "    FILE_BYTES_WRITTEN: 1143045\n",
      "    HDFS_BYTES_READ: 1932\n",
      "    HDFS_BYTES_WRITTEN: 177049\n",
      "    S3_BYTES_READ: 251799\n",
      "  Job Counters :\n",
      "    Launched map tasks: 21\n",
      "    Launched reduce tasks: 8\n",
      "    Rack-local map tasks: 21\n",
      "    SLOTS_MILLIS_MAPS: 230938\n",
      "    SLOTS_MILLIS_REDUCES: 83890\n",
      "    Total time spent by all maps waiting after reserving slots (ms): 0\n",
      "    Total time spent by all reduces waiting after reserving slots (ms): 0\n",
      "  Map-Reduce Framework:\n",
      "    CPU time spent (ms): 58730\n",
      "    Combine input records: 0\n",
      "    Combine output records: 0\n",
      "    Map input bytes: 241718\n",
      "    Map input records: 1991\n",
      "    Map output bytes: 302182\n",
      "    Map output materialized bytes: 211675\n",
      "    Map output records: 14644\n",
      "    Physical memory (bytes) snapshot: 8982081536\n",
      "    Reduce input groups: 991\n",
      "    Reduce input records: 14644\n",
      "    Reduce output records: 991\n",
      "    Reduce shuffle bytes: 211675\n",
      "    SPLIT_RAW_BYTES: 1932\n",
      "    Spilled Records: 29288\n",
      "    Total committed heap usage (bytes): 11634475008\n",
      "    Virtual memory (bytes) snapshot: 56889352192\n",
      "Counters from step 2:\n",
      "  File Input Format Counters :\n",
      "    Bytes Read: 180007\n",
      "  File Output Format Counters :\n",
      "    Bytes Written: 4826416\n",
      "  FileSystemCounters:\n",
      "    FILE_BYTES_READ: 2008762\n",
      "    FILE_BYTES_WRITTEN: 4779351\n",
      "    HDFS_BYTES_READ: 181465\n",
      "    HDFS_BYTES_WRITTEN: 4826416\n",
      "  Job Counters :\n",
      "    Data-local map tasks: 7\n",
      "    Launched map tasks: 9\n",
      "    Launched reduce tasks: 4\n",
      "    Rack-local map tasks: 2\n",
      "    SLOTS_MILLIS_MAPS: 87938\n",
      "    SLOTS_MILLIS_REDUCES: 61113\n",
      "    Total time spent by all maps waiting after reserving slots (ms): 0\n",
      "    Total time spent by all reduces waiting after reserving slots (ms): 0\n",
      "  Map-Reduce Framework:\n",
      "    CPU time spent (ms): 32600\n",
      "    Combine input records: 272174\n",
      "    Combine output records: 250162\n",
      "    Map input bytes: 177049\n",
      "    Map input records: 991\n",
      "    Map output bytes: 6979638\n",
      "    Map output materialized bytes: 2418638\n",
      "    Map output records: 272174\n",
      "    Physical memory (bytes) snapshot: 3556397056\n",
      "    Reduce input groups: 188425\n",
      "    Reduce input records: 250162\n",
      "    Reduce output records: 188425\n",
      "    Reduce shuffle bytes: 2418638\n",
      "    SPLIT_RAW_BYTES: 1458\n",
      "    Spilled Records: 500324\n",
      "    Total committed heap usage (bytes): 4864868352\n",
      "    Virtual memory (bytes) snapshot: 25571880960\n",
      "Counters from step 3:\n",
      "  File Input Format Counters :\n",
      "    Bytes Read: 4826416\n",
      "  File Output Format Counters :\n",
      "    Bytes Written: 8198304\n",
      "  FileSystemCounters:\n",
      "    FILE_BYTES_READ: 1725802\n",
      "    FILE_BYTES_WRITTEN: 3660852\n",
      "    HDFS_BYTES_READ: 4827064\n",
      "    S3_BYTES_WRITTEN: 8198304\n",
      "  Job Counters :\n",
      "    Data-local map tasks: 3\n",
      "    Launched map tasks: 4\n",
      "    Launched reduce tasks: 1\n",
      "    Rack-local map tasks: 1\n",
      "    SLOTS_MILLIS_MAPS: 30557\n",
      "    SLOTS_MILLIS_REDUCES: 25712\n",
      "    Total time spent by all maps waiting after reserving slots (ms): 0\n",
      "    Total time spent by all reduces waiting after reserving slots (ms): 0\n",
      "  Map-Reduce Framework:\n",
      "    CPU time spent (ms): 20700\n",
      "    Combine input records: 0\n",
      "    Combine output records: 0\n",
      "    Map input bytes: 4826416\n",
      "    Map input records: 188425\n",
      "    Map output bytes: 4826416\n",
      "    Map output materialized bytes: 1801881\n",
      "    Map output records: 188425\n",
      "    Physical memory (bytes) snapshot: 1524248576\n",
      "    Reduce input groups: 188425\n",
      "    Reduce input records: 188425\n",
      "    Reduce output records: 187434\n",
      "    Reduce shuffle bytes: 1801881\n",
      "    SPLIT_RAW_BYTES: 648\n",
      "    Spilled Records: 376850\n",
      "    Total committed heap usage (bytes): 1979711488\n",
      "    Virtual memory (bytes) snapshot: 9884495872\n",
      "  reducer:\n",
      "    output_pairs: 187434\n",
      "removing tmp directory /tmp/mrjob_Jaccard_Index.hduser.20160217.083507.925889\n",
      "Removing all files in s3://mrjob-ba3ec136dc70d97b/tmp/mrjob_Jaccard_Index.hduser.20160217.083507.925889/\n",
      "Removing all files in s3://mrjob-ba3ec136dc70d97b/tmp/logs/j-39USKDJFLC9SZ/\n",
      "Terminating job flow: j-39USKDJFLC9SZ\n"
     ]
    }
   ],
   "source": [
    "!s3cmd rm --recursive s3://agunn-w261-hw5/jaccard54/\n",
    "! python mrjob_Jaccard_Index.py -r emr s3://agunn-w261-hw5/bigram54/part* \\\n",
    "    --output-dir=s3://agunn-w261-hw5/jaccard54 \\\n",
    "    --no-output \\\n",
    "    --no-strict-protocol"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "download: 's3://agunn-w261-hw5/jaccard54/part-00000' -> 'HW5/jaccard54_out.txt'  [1 of 1]\n",
      "download: 's3://agunn-w261-hw5/jaccard54/part-00000' -> 'HW5/jaccard54_out.txt'  [1 of 1]\n",
      " 8198304 of 8198304   100% in    1s     5.27 MB/s  done\n",
      "[\"AB\", \"Address\"]\t0.038461538461538464\n",
      "[\"AB\", \"Application\"]\t0.022727272727272728\n",
      "[\"AB\", \"Archaeology\"]\t0.029411764705882353\n",
      "[\"AB\", \"Banking\"]\t0.037037037037037035\n",
      "[\"AB\", \"Biography\"]\t0.026315789473684209\n",
      "[\"AB\", \"Biological\"]\t0.037037037037037035\n",
      "[\"AB\", \"Brown\"]\t0.023255813953488372\n",
      "[\"AB\", \"CD\"]\t0.040000000000000001\n",
      "[\"AB\", \"Capital\"]\t0.022727272727272728\n",
      "[\"AB\", \"Causes\"]\t0.02564102564102564\n",
      "187434 HW5/jaccard54_out.txt\n"
     ]
    }
   ],
   "source": [
    "!rm HW5/jaccard54_out.txt\n",
    "!s3cmd get s3://agunn-w261-hw5/jaccard54/part-00000 HW5/jaccard54_out.txt\n",
    "\n",
    "!head HW5/jaccard54_out.txt\n",
    "!wc -l HW5/jaccard54_out.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-----\n",
    "##HW 5.5 Evaluation of synonyms that your discovered\n",
    "In this part of the assignment you will evaluate the success of you synonym detector.\n",
    "Take the top 1,000 closest/most similar/correlative pairs of words as determined\n",
    "by your measure in (2), and use the synonyms function in the accompanying\n",
    "python code:\n",
    "\n",
    "nltk_synonyms.py\n",
    "\n",
    "Note: This will require installing the python nltk package:\n",
    "\n",
    "http://www.nltk.org/install.html\n",
    "\n",
    "and downloading its data with nltk.download().\n",
    "\n",
    "For each (word1,word2) pair, check to see if word1 is in the list, \n",
    "synonyms(word2), and vice-versa. If one of the two is a synonym of the other, \n",
    "then consider this pair a 'hit', and then report the precision, recall, and F1 measure  of \n",
    "your detector across your 1,000 best guesses. Report the macro averages of these measures.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "####Implementation Notes\n",
    "1. Sorted and truncated the jaccard and cosine output files to 10000 pairs\n",
    "2. For each 10000 pairs, determined if in synonyms.\n",
    "3. Assumed 1000 top pairs from 5.4 are predicted \"hits\" and the remaining are \"misses\"\n",
    "4. Calculate statistics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[\"soda\", \"sulphate\"]\t0.3888888888888889\n",
      "[\"sulphate\", \"soda\"]\t0.3888888888888889\n",
      "[\"insoluble\", \"solubility\"]\t0.375\n",
      "[\"solubility\", \"insoluble\"]\t0.375\n",
      "[\"cervix\", \"vagina\"]\t0.33333333333333331\n",
      "[\"vagina\", \"cervix\"]\t0.33333333333333331\n",
      "[\"alkaline\", \"solubility\"]\t0.31578947368421051\n",
      "[\"solubility\", \"alkaline\"]\t0.31578947368421051\n",
      "[\"embassy\", \"Embassy\"]\t0.30769230769230771\n",
      "[\"Embassy\", \"embassy\"]\t0.30769230769230771\n",
      "10000 top10k_test_jaccard_55.out\n"
     ]
    }
   ],
   "source": [
    "!cat HW5/jaccard54_out.txt | sort -k3nr  > file_test_jaccard_55.out\n",
    "!head -10000 file_test_jaccard_55.out > top10k_test_jaccard_55.out\n",
    "!head top10k_test_jaccard_55.out\n",
    "!wc -l top10k_test_jaccard_55.out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[\"Eighth\", \"Fleet\"]\t0.96790553623374387\n",
      "[\"morally\", \"sustainable\"]\t0.94175704074858269\n",
      "[\"inquired\", \"availed\"]\t0.93525333208747397\n",
      "[\"alveolar\", \"endothelial\"]\t0.89570818791440021\n",
      "[\"endothelial\", \"alveolar\"]\t0.89570818791440021\n",
      "[\"Published\", \"AT\"]\t0.8730833808181192\n",
      "[\"frank\", \"concurrence\"]\t0.82832523327411578\n",
      "[\"Mines\", \"Dominion\"]\t0.82311423512845294\n",
      "[\"incarnation\", \"presiding\"]\t0.82305489175310154\n",
      "[\"los\", \"pro\"]\t0.80366953875170655\n",
      "10000 top10k_test_cosine_55.out\n"
     ]
    }
   ],
   "source": [
    "!cat HW5/cosine54_out.txt | sort -k3nr  > file_test_cosine_55.out\n",
    "!head -10000 file_test_cosine_55.out > top10k_test_cosine_55.out\n",
    "!head top10k_test_cosine_55.out\n",
    "!wc -l top10k_test_cosine_55.out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "***** EVALUATE COSINE *****\n",
      "True Positive: 1\n",
      "True Negative: 8995\n",
      "False Positive: 999\n",
      "False Negative: 5\n",
      "\n",
      "\n",
      "Accuracy: 0.8996\n",
      "Precision: 0.001\n",
      "Recall: 0.166666666667\n",
      "F1 Score: 0.00198807157058\n",
      "\n",
      "***** EVALUATE JACCARD *****\n",
      "True Positive: 6\n",
      "True Negative: 8990\n",
      "False Positive: 994\n",
      "False Negative: 10\n",
      "\n",
      "\n",
      "Accuracy: 0.8996\n",
      "Precision: 0.006\n",
      "Recall: 0.375\n",
      "F1 Score: 0.011811023622\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk.corpus import wordnet as wn\n",
    "import sys\n",
    "import ast\n",
    "#print all the synset element of an element\n",
    "def synonyms(string):\n",
    "    syndict = {}\n",
    "    for i,j in enumerate(wn.synsets(string)):\n",
    "        syns = j.lemma_names()\n",
    "        for syn in syns:\n",
    "            syndict.setdefault(syn,1)\n",
    "    return syndict.keys()\n",
    "\n",
    "def evaluate(file_name):\n",
    "\n",
    "    line_cnt =0\n",
    "    hits = []\n",
    "    # Check if any of the top 10000 matches the synonym list\n",
    "    with open(file_name, 'r') as f:\n",
    "        for line in f:\n",
    "            line_cnt += 1\n",
    "            t = line.strip().split('\\t') \n",
    "            words = t[0]\n",
    "            words  = words.replace('\"','').replace('[','').replace(']','')\n",
    "            words =  words.split(',')\n",
    "            w1 = words[0].lower().strip()\n",
    "            w2 = words[1].lower().strip()\n",
    "            \n",
    "            if w1 in synonyms(w2) and w2 in synonyms(w1):\n",
    "                hits.append(1)\n",
    "            else:\n",
    "                hits.append(0)\n",
    "                \n",
    "                \n",
    "    #first 1000 rows we assume are \"hits\"\n",
    "    predictions = [1]*1000 + [0]*(line_cnt-1000)\n",
    "    \n",
    "    # determine measures\n",
    "    true_positive = 0\n",
    "    false_negative = 0\n",
    "    false_positive = 0\n",
    "    true_negative = 0\n",
    "\n",
    "    for i in range(len(predictions)):\n",
    "        # true positives\n",
    "        if hits[i] == 1 and predictions[i] == 1:\n",
    "            true_positive += 1\n",
    "        # true negatives\n",
    "        elif hits[i] == 0 and predictions[i] == 0:\n",
    "            true_negative += 1\n",
    "        # false negatives\n",
    "        elif hits[i] == 1 and predictions[i] == 0:\n",
    "            false_negative += 1\n",
    "        # false positives\n",
    "        else:\n",
    "            false_positive += 1    \n",
    "\n",
    "    accuracy = float(true_positive + true_negative) / len(predictions)\n",
    "    recall = float(true_positive) / float(true_positive + false_negative)\n",
    "    precision = float(true_positive) / float(true_positive + false_positive)\n",
    "    \n",
    "    print \"True Positive: {0}\".format(true_positive)\n",
    "    print \"True Negative: {0}\".format(true_negative)\n",
    "    print \"False Positive: {0}\".format(false_positive)\n",
    "    print \"False Negative: {0}\".format(false_negative)\n",
    "\n",
    "    print \"\\n\"\n",
    "    print \"Accuracy: {0}\".format(accuracy)\n",
    "    print \"Precision: {0}\".format(precision)\n",
    "    print \"Recall: {0}\".format(recall)\n",
    "    \n",
    "    try:\n",
    "        print \"F1 Score: {0}\".format(2 * (precision*recall) / (precision + recall))\n",
    "    except ZeroDivisionError:\n",
    "        print \"F1 Score: Inf\"\n",
    "    \n",
    "jaccard = 'top10k_test_jaccard_55.out'\n",
    "cosine = 'top10k_test_cosine_55.out'\n",
    "\n",
    "print \"\\n***** EVALUATE COSINE *****\"\n",
    "evaluate(cosine)\n",
    "print \"\\n***** EVALUATE JACCARD *****\"\n",
    "evaluate(jaccard)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "----\n",
    "##HW5.6 Optional\n",
    "\n",
    "Repeat HW5 using vocabulary words ranked from 8001,-10,000;  7001,-10,000; 6001,-10,000; 5001,-10,000; 3001,-10,000; and 1001,-10,000;\n",
    "Dont forget to report you Cluster configuration.\n",
    "\n",
    "Generate the following graphs:\n",
    "-- vocabulary size (X-Axis) versus CPU time for indexing\n",
    "-- vocabulary size (X-Axis) versus number of pairs processed\n",
    "-- vocabulary size (X-Axis) versus F1 measure, Precision, Recall\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "!cat HW5/frequency_out.txt | sed -n 9001,10000p > HW5/topwords_touse9000.txt\n",
    "!cat HW5/frequency_out.txt | sed -n 8001,9000p > HW5/topwords_touse8000.txt\n",
    "!cat HW5/frequency_out.txt | sed -n 7001,8000p > HW5/topwords_touse7000.txt\n",
    "!cat HW5/frequency_out.txt | sed -n 6001,7000p > HW5/topwords_touse6000.txt\n",
    "!cat HW5/frequency_out.txt | sed -n 5001,6000p > HW5/topwords_touse5000.txt\n",
    "!cat HW5/frequency_out.txt | sed -n 4001,5000p > HW5/topwords_touse4000.txt\n",
    "!cat HW5/frequency_out.txt | sed -n 3001,4000p > HW5/topwords_touse3000.txt\n",
    "!cat HW5/frequency_out.txt | sed -n 2001,3000p > HW5/topwords_touse2000.txt\n",
    "!cat HW5/frequency_out.txt | sed -n 1001,2000p > HW5/topwords_touse1000.txt\n",
    "!head -1000 HW5/frequency_out.txt > HW5/topwords_touse0000.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "upload: 'HW5/topwords_touse.txt' -> 's3://agunn-w261-hw5/topwords_touse.txt'  [1 of 11]\n",
      " 1766 of 1766   100% in    0s     7.71 kB/s  done\n",
      "upload: 'HW5/topwords_touse.txt' -> 's3://agunn-w261-hw5/topwords_touse.txt'  [1 of 11]\n",
      " 1766 of 1766   100% in    0s     7.62 kB/s  done\n",
      "upload: 'HW5/topwords_touse0000.txt' -> 's3://agunn-w261-hw5/topwords_touse0000.txt'  [2 of 11]\n",
      " 17173 of 17173   100% in    0s   125.78 kB/s  done\n",
      "upload: 'HW5/topwords_touse1000.txt' -> 's3://agunn-w261-hw5/topwords_touse1000.txt'  [3 of 11]\n",
      " 17815 of 17815   100% in    0s    87.65 kB/s  done\n",
      "upload: 'HW5/topwords_touse2000.txt' -> 's3://agunn-w261-hw5/topwords_touse2000.txt'  [4 of 11]\n",
      " 17934 of 17934   100% in    0s    84.75 kB/s  done\n",
      "upload: 'HW5/topwords_touse3000.txt' -> 's3://agunn-w261-hw5/topwords_touse3000.txt'  [5 of 11]\n",
      " 17510 of 17510   100% in    0s   159.39 kB/s  done\n",
      "upload: 'HW5/topwords_touse4000.txt' -> 's3://agunn-w261-hw5/topwords_touse4000.txt'  [6 of 11]\n",
      " 17222 of 17222   100% in    0s   148.21 kB/s  done\n",
      "upload: 'HW5/topwords_touse5000.txt' -> 's3://agunn-w261-hw5/topwords_touse5000.txt'  [7 of 11]\n",
      " 17397 of 17397   100% in    0s   167.83 kB/s  done\n",
      "upload: 'HW5/topwords_touse6000.txt' -> 's3://agunn-w261-hw5/topwords_touse6000.txt'  [8 of 11]\n",
      " 17382 of 17382   100% in    0s   181.54 kB/s  done\n",
      "upload: 'HW5/topwords_touse7000.txt' -> 's3://agunn-w261-hw5/topwords_touse7000.txt'  [9 of 11]\n",
      " 17513 of 17513   100% in    0s   141.52 kB/s  done\n",
      "upload: 'HW5/topwords_touse8000.txt' -> 's3://agunn-w261-hw5/topwords_touse8000.txt'  [10 of 11]\n",
      " 17691 of 17691   100% in    0s   148.32 kB/s  done\n",
      "upload: 'HW5/topwords_touse9000.txt' -> 's3://agunn-w261-hw5/topwords_touse9000.txt'  [11 of 11]\n",
      " 17419 of 17419   100% in    0s   131.42 kB/s  done\n"
     ]
    }
   ],
   "source": [
    "#get top 1000 words.\n",
    "#build stripes of co-occurrence on ALL 5-grams  word [co1, co2, co3]\n",
    "#this will be output from mapper for each \n",
    "#!head HW5/topwords_touse.txt\n",
    "#!wc -l HW5/topwords_touse.txt\n",
    "!s3cmd put FILE HW5/topwords* s3://agunn-w261-hw5/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###Written Response\n",
    "**Cluster:**  \n",
    "    ec2_instance_type: m3.xlarge  \n",
    "    ec2_master_instance_type: m1.medium  \n",
    "    num_ec2_instances: 4  \n",
    "    \n",
    "\n",
    "This shows clearly how Zipf's Law comes into play.  The top ranked words as far as frequency (0001-1000) took the longest to run. I also observed the words in the same top bracket occurred much more frequently with each other, which makes sense. Worst case in running this with 1000 words is all 1000 words appear with each other, meaning you have a full matrix, or O(n<sup>2</sup>)\n",
    "\n",
    "I do not have the graphs requested for this question, just the table below. \n",
    "\n",
    "You may also see information about my results here: https://docs.google.com/presentation/d/1d26yGMnln55hZRYCrNdJKNCrjUIXj2r6LSq8scmBxoQ/edit?pref=2&pli=1#slide=id.g1196a69f19_1_5\n",
    "\n",
    "\n",
    "|Bracket|Minutes|Pairs Compared in Mapper|\n",
    "|----|----:|----:|\n",
    "|9001-10000|17|1,477,162|\n",
    "|8001-9000|17|1,826,865|\n",
    "|7001-8000|17|2,839,147|\n",
    "|6001-7000|17|2,842,799|\n",
    "|5001-6000|17|3,639,018|\n",
    "|4001-5000|17|5,042,215|\n",
    "|3001-4000|18|7,386,679|\n",
    "|2001-3000|18|11,955,068|\n",
    "|1001-2000|19|23,684,259|\n",
    "|0001-1000|155|923,892,455|\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-----\n",
    "##HW 5.7 (optional)\n",
    "There is also a corpus of stopwords, that is, high-frequency words like \"the\", \"to\" and \"also\" that we sometimes want to filter out of a document before further processing. Stopwords usually have little lexical content, and their presence in a text fails to distinguish it from other texts. Python's nltk comes with a prebuilt list of stopwords (see below). Using this stopword list filter out these tokens from your analysis and rerun the experiments in 5.5 and disucuss the results of using a stopword list and without using a stopword list.\n",
    "\n",
    "> from nltk.corpus import stopwords\n",
    ">> stopwords.words('english')\n",
    "['i', 'me', 'my', 'myself', 'we', 'our', 'ours', 'ourselves', 'you', 'your', 'yours',\n",
    "'yourself', 'yourselves', 'he', 'him', 'his', 'himself', 'she', 'her', 'hers',\n",
    "'herself', 'it', 'its', 'itself', 'they', 'them', 'their', 'theirs', 'themselves',\n",
    "'what', 'which', 'who', 'whom', 'this', 'that', 'these', 'those', 'am', 'is', 'are',\n",
    "'was', 'were', 'be', 'been', 'being', 'have', 'has', 'had', 'having', 'do', 'does',\n",
    "'did', 'doing', 'a', 'an', 'the', 'and', 'but', 'if', 'or', 'because', 'as', 'until',\n",
    "'while', 'of', 'at', 'by', 'for', 'with', 'about', 'against', 'between', 'into',\n",
    "'through', 'during', 'before', 'after', 'above', 'below', 'to', 'from', 'up', 'down',\n",
    "'in', 'out', 'on', 'off', 'over', 'under', 'again', 'further', 'then', 'once', 'here',\n",
    "'there', 'when', 'where', 'why', 'how', 'all', 'any', 'both', 'each', 'few', 'more',\n",
    "'most', 'other', 'some', 'such', 'no', 'nor', 'not', 'only', 'own', 'same', 'so',\n",
    "'than', 'too', 'very', 's', 't', 'can', 'will', 'just', 'don', 'should', 'now']\n",
    "\n",
    "----\n",
    "##HW 5.8 (optional)\n",
    "There are many good ways to build our synonym detectors, so for optional homework, \n",
    "measure co-occurrence by (left/right/all) consecutive words only, \n",
    "or make stripes according to word co-occurrences with the accompanying \n",
    "2-, 3-, or 4-grams (note here that your output will no longer be interpretable as a network) inside of the 5-grams.\n",
    "\n",
    "---\n",
    "\n",
    "##Hw 5.9 (optional)\n",
    "Once again, benchmark your top 10,000 associations (as in 5.5), this time for your\n",
    "results from 5.6. Has your detector improved?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
