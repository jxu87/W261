{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# DATSCIW261 ASSIGNMENT #5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Angela Gunn, Jing Xu\n",
    "\n",
    "angela@egunn.com, jaling@gmail.com\n",
    "\n",
    "W261-3\n",
    "\n",
    "DATSCIW261 Assignment #5\n",
    "\n",
    "2/10/16"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **HW5.0**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**What is a data warehouse?**\n",
    "\n",
    "A data warehouse is a central repository of integrated data from one or many sources used for reporting and data analysis. In an enterprise setting, it serves as the primary repository of data from sales transactions to product inventories. Modern data warehouses can store:  \n",
    "- relational data  \n",
    "- semi-structured data like query logs  \n",
    "- unstructured data like tweets, titles of web pages\n",
    "\n",
    "Data warehouses form a foundation for business intelligence and data science, and are leveraged to gain a competitive advantage in the marketplace through data mining.\n",
    "\n",
    "**What is a Star schema? When is it used?**\n",
    "\n",
    "A Star Schema is a type of data mart schema that consists of one or more fact tables referencing any number of dimension tables. It gets its name from the tendency of the physical model to resemble a star with the fact table in the center and dimension tables surrounding it.\n",
    "\n",
    "<img src=\"starschema.png\">\n",
    "\n",
    "It is used to handle simpler queries as the join logic is usually simpler to handle than the join logic needed to retrieve data from a highly normalized transactional schemas. There is also more simplified business reporting logic, query performance gains, and faster aggregations compared to other schemas.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **HW5.1**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**In the database world What is 3NF?**\n",
    "\n",
    "3NF is third normal form, the third step in normalizing a database and builds on the first and second normal forms. It is a normalization process used to reduce the unnecessary duplication of data and making sure every value of of attribute column of a table exists as a value of another attribute column in the same or different table (referential integrity). This is done by ensuring that the data is in second normal form and that all the attributes in the table are determined only by the candidate keys of the table and not by any other non-prime attributes. 3NF is used to improve processing while minimizing storage costs, which is ideal for online transaction processing (OLTP) applications.\n",
    "\n",
    "**Does machine learning use data in 3NF? If so why?**\n",
    "\n",
    "3NF is often used for machine learning - 3NF's structure is ideal for machine processing. The removal of transitive functional dependency avoids inputting features for machine learning that are redundant and non-independent. Relational databases almost always contain structured, normalized data with indexes, and machine learning in this realm uses 3NF data inputs. \n",
    "\n",
    "**In what form does ML consume data?**  \n",
    "\n",
    "Although 3NF works well for ML, ML consumes data in a variety of additional forms. Not all ML applications depend on data normalization - 1NF and 2NF can be fed into ML algorithms, although the effectiveness will vary depending on the algorithm and type of data being worked with. ML algorithms are now sophisticated enough to learn from unstructured data as well. \n",
    "\n",
    "**Why would one use log files that are denormalized?**  \n",
    "\n",
    "Denormalized log files can be more efficient for queries that draw information from several tables that are stored on disk and require complex joins to complete, depending on the size and type of data. Denormalization is the process of attempting to optimize the read performance of a database by adding redundant data or by grouping data - it is a tradeoff of redundancy/extra space for scalability and read performance. Hadoop MapReduce for example primarily uses denormalized data that is fully contained in a single record to avoid costly joins.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **HW5.2**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using MRJob, implement a hashside join (memory-backed map-side) for left, right and inner joins. Run your code on the  data used in HW 4.4: (Recall HW 4.4: Find the most frequent visitor of each page using mrjob and the output of 4.2  (i.e., transfromed log file). In this output please include the webpage URL, webpageID and Visitor ID.)\n",
    ":\n",
    "  \n",
    "Justify which table you chose as the Left table in this hashside join.\n",
    "  \n",
    "Please report the number of rows resulting from:\n",
    "  \n",
    "(1) Left joining Table Left with Table Right  \n",
    "(2) Right joining Table Left with Table Right  \n",
    "(3) Inner joining Table Left with Table Right  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Approach**\n",
    "\n",
    "Recognition of 2 tables: the LogData and the URL data.  \n",
    "anonymous-msweb.data.pp: first 5 lines  \n",
    "\n",
    "V,1000,C,10001  \n",
    "V,1001,C,10001  \n",
    "V,1002,C,10001  \n",
    "V,1001,C,10002  \n",
    "V,1003,C,10002  \n",
    "\n",
    "url.txt: first 2 lines\n",
    "\n",
    "A,1287,1,\"International AutoRoute\",\"/autoroute\"  \n",
    "A,1288,1,\"library\",\"/library  \n",
    "\n",
    "The left table will be the url.txt table because it has the much smaller rowset and can be more easily stored in memory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "url.txt\r\n"
     ]
    }
   ],
   "source": [
    "# Create a file with only URL(s), i.e. records starting with 'A'\n",
    "!rm -v url.txt\n",
    "!grep ^A anonymous-msweb.data > url.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## HashSideInnerJoin"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting hashsideinnerjoin_52.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile hashsideinnerjoin_52.py\n",
    "#!/usr/bin/python\n",
    "## hashsideinnerjoin_52.py\n",
    "## Author: Angela Gunn & Jing Xu\n",
    "## Description:Inner Join\n",
    "\n",
    "from mrjob.job import MRJob\n",
    "from mrjob.step import MRJobStep\n",
    "from mrjob.compat import get_jobconf_value\n",
    "\n",
    "import csv\n",
    "\n",
    "def csv_readline(line):\n",
    "    \"\"\"Given a sting CSV line, return a list of strings.\"\"\"\n",
    "    for row in csv.reader([line]):\n",
    "        return row\n",
    "\n",
    "class innerjoin(MRJob):\n",
    "    def steps(self):\n",
    "        return [MRJobStep(mapper_init = self.mapper_init,\n",
    "                     mapper=self.mapper)]\n",
    "    \n",
    "    def mapper_init(self):\n",
    "        #store the URLs\n",
    "        self.urls = {}\n",
    "        with open('url.txt') as f:\n",
    "            for line in f:\n",
    "                cell = csv_readline(line)\n",
    "                self.urls[cell[1]] = cell[4]\n",
    "        \n",
    "    def mapper(self, _, line):\n",
    "        #this is the logs\n",
    "        cell = csv_readline(line) \n",
    "        yield cell[1], (self.urls[cell[1]], cell[3]) #yield the matching rows.\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    innerjoin.run()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:mrjob.runner:\n",
      "WARNING:mrjob.runner:PLEASE NOTE: Starting in mrjob v0.5.0, protocols will be strict by default. It's recommended you run your job with --strict-protocols or set up mrjob.conf as described at https://pythonhosted.org/mrjob/whats-new.html#ready-for-strict-protocols\n",
      "WARNING:mrjob.runner:\n",
      "WARNING:mrjob.step:MRJobStep has been renamed to MRStep. The old name will be removed in v0.5.0.\n",
      "WARNING:mrjob.step:MRJobStep has been renamed to MRStep. The old name will be removed in v0.5.0.\n",
      "WARNING:mrjob.step:MRJobStep has been renamed to MRStep. The old name will be removed in v0.5.0.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "There are 98654 records for inner join\n"
     ]
    }
   ],
   "source": [
    "%reload_ext autoreload\n",
    "%autoreload 2\n",
    "from hashsideinnerjoin_52 import innerjoin\n",
    "import os\n",
    "\n",
    "# Passing Hadoop Streaming parameters to:\n",
    "# partition by leftmost part of composite key\n",
    "# secodary sort by rightmost part of the same composite key\n",
    "\n",
    "mr_job = innerjoin(args=['anonymous-msweb.data.pp', \n",
    "                        '--file', 'url.txt'])\n",
    "\n",
    "output_file = \"output_hw52_inner.txt\"\n",
    "try:\n",
    "    os.remove(output_file)\n",
    "except OSError:\n",
    "    pass\n",
    "\n",
    "with mr_job.make_runner() as runner, open(output_file, 'a') as f: \n",
    "    runner.run()\n",
    "    count = 0\n",
    "    # stream_output: get access of the output \n",
    "    for line in runner.stream_output():\n",
    "        #print mr_job.parse_output_line(line)\n",
    "        f.write(str(line))\n",
    "        count += 1\n",
    "print \"\\n\"\n",
    "print \"There are %s records for inner join\" %count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of results:\n",
      "   98654\n",
      "-----\n",
      "first 10 rows\n",
      "page_id [url, user]\n",
      "\"1000\"\t[\"/regwiz\", \"10001\"]\n",
      "\"1001\"\t[\"/support\", \"10001\"]\n",
      "\"1002\"\t[\"/athome\", \"10001\"]\n",
      "\"1001\"\t[\"/support\", \"10002\"]\n",
      "\"1003\"\t[\"/kb\", \"10002\"]\n",
      "\"1001\"\t[\"/support\", \"10003\"]\n",
      "\"1003\"\t[\"/kb\", \"10003\"]\n",
      "\"1004\"\t[\"/search\", \"10003\"]\n",
      "\"1005\"\t[\"/norge\", \"10004\"]\n",
      "\"1006\"\t[\"/misc\", \"10005\"]\n"
     ]
    }
   ],
   "source": [
    "!echo \"Number of results:\"\n",
    "!wc -l < output_hw52_inner.txt\n",
    "!echo \"-----\"\n",
    "!echo \"first 10 rows\"\n",
    "!echo \"page_id [url, user]\"\n",
    "!head -10 output_hw52_inner.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## HashSideLeftJoin"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting hashsideleftjoin_52.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile hashsideleftjoin_52.py\n",
    "#!/usr/bin/python\n",
    "## hashsideleftjoin_52.py\n",
    "## Author: Angela Gunn & Jing Xu \n",
    "## Description:Left Join\n",
    "\n",
    "from mrjob.job import MRJob\n",
    "from mrjob.step import MRJobStep\n",
    "from mrjob.compat import get_jobconf_value\n",
    "import csv\n",
    "\n",
    "def csv_readline(line):\n",
    "    for row in csv.reader([line]):\n",
    "        return row\n",
    "\n",
    "class leftjoin(MRJob):\n",
    "    \n",
    "    def steps(self):\n",
    "        return [MRJobStep(mapper_init = self.mapper_init,\n",
    "                         mapper = self.mapper, mapper_final = self.mapper_final)]\n",
    "    \n",
    "    def mapper_init(self):\n",
    "        self.urls = {} #initialize urls library\n",
    "         \n",
    "        with open('url.txt') as f:\n",
    "            for line in f: \n",
    "                cell = csv_readline(line)\n",
    "                self.urls[cell[1]] = [cell[4],[]] #url, list of visitors\n",
    "\n",
    "    def mapper(self, _, line):\n",
    "        #these are the logs\n",
    "        cell = csv_readline(line)\n",
    "        key = cell[1]\n",
    "        self.urls[key][1].append(cell[3])\n",
    "\n",
    "    def mapper_final(self):\n",
    "        for key, values in self.urls.iteritems():\n",
    "            url = values[0]\n",
    "            if len(values[1]) > 0:\n",
    "                for u in values[1]: yield key, (url, u)\n",
    "            else:\n",
    "                yield key, (url, \"NONE\")\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    leftjoin.run()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "!chmod a+x hashsideleftjoin_52.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:mrjob.runner:\n",
      "WARNING:mrjob.runner:PLEASE NOTE: Starting in mrjob v0.5.0, protocols will be strict by default. It's recommended you run your job with --strict-protocols or set up mrjob.conf as described at https://pythonhosted.org/mrjob/whats-new.html#ready-for-strict-protocols\n",
      "WARNING:mrjob.runner:\n",
      "WARNING:mrjob.step:MRJobStep has been renamed to MRStep. The old name will be removed in v0.5.0.\n",
      "WARNING:mrjob.step:MRJobStep has been renamed to MRStep. The old name will be removed in v0.5.0.\n",
      "WARNING:mrjob.step:MRJobStep has been renamed to MRStep. The old name will be removed in v0.5.0.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "There are 98663 records for left join\n"
     ]
    }
   ],
   "source": [
    "%reload_ext autoreload\n",
    "%autoreload 2\n",
    "# Running mrjob using a Hadoop Runner in local cluster\n",
    "from hashsideleftjoin_52 import leftjoin\n",
    "import os\n",
    "\n",
    "# Passing Hadoop Streaming parameters to:\n",
    "# partition by leftmost part of composite key\n",
    "# secodary sort by rightmost part of the same composite key\n",
    "\n",
    "mr_job = leftjoin(args=['anonymous-msweb.data.pp', \n",
    "                        '--file', 'url.txt'])\n",
    "\n",
    "output_file = \"output_hw52_left.txt\"\n",
    "try:\n",
    "    os.remove(output_file)\n",
    "except OSError:\n",
    "    pass\n",
    "\n",
    "with mr_job.make_runner() as runner, open(output_file, 'a') as f: \n",
    "    runner.run()\n",
    "    count = 0\n",
    "    # stream_output: get access of the output \n",
    "    for line in runner.stream_output():\n",
    "        #print mr_job.parse_output_line(line)\n",
    "        f.write(str(line))\n",
    "        count+=1\n",
    "print \"\\n\"\n",
    "print \"There are %s records for left join\" %count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of results:\n",
      "   98663\n",
      "-----\n",
      "first 10 rows\n",
      "page_id [url, user]\n",
      "\"1142\"\t[\"/southafrica\", \"10372\"]\n",
      "\"1142\"\t[\"/southafrica\", \"13352\"]\n",
      "\"1142\"\t[\"/southafrica\", \"19019\"]\n",
      "\"1142\"\t[\"/southafrica\", \"24124\"]\n",
      "\"1142\"\t[\"/southafrica\", \"25638\"]\n",
      "\"1142\"\t[\"/southafrica\", \"25798\"]\n",
      "\"1142\"\t[\"/southafrica\", \"26342\"]\n",
      "\"1142\"\t[\"/southafrica\", \"28044\"]\n",
      "\"1142\"\t[\"/southafrica\", \"28821\"]\n",
      "\"1142\"\t[\"/southafrica\", \"29837\"]\n"
     ]
    }
   ],
   "source": [
    "!echo \"Number of results:\"\n",
    "!wc -l < output_hw52_left.txt\n",
    "!echo \"-----\"\n",
    "!echo \"first 10 rows\"\n",
    "!echo \"page_id [url, user]\"\n",
    "!head -10 output_hw52_left.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## HashSideRightJoin"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting hashsiderightjoin_52.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile hashsiderightjoin_52.py\n",
    "#!/usr/bin/python\n",
    "## hashsiderightjoin_52.py\n",
    "## Author: Angela Gunn & Jing Xu\n",
    "## Description:Right Join\n",
    "\n",
    "from mrjob.job import MRJob\n",
    "from mrjob.step import MRJobStep\n",
    "from mrjob.compat import get_jobconf_value\n",
    " \n",
    "import csv\n",
    "\n",
    "def csv_readline(line):\n",
    "    \"\"\"Given a sting CSV line, return a list of strings.\"\"\"\n",
    "    for row in csv.reader([line]):\n",
    "        return row\n",
    "\n",
    "class rightjoin(MRJob):\n",
    "    \n",
    "    def steps(self):\n",
    "        return [MRJobStep(mapper_init = self.mapper_init,\n",
    "                         mapper = self.mapper)]\n",
    "    \n",
    "    def mapper_init(self):\n",
    "        self.urls = {} #initialize urls dictionary\n",
    "        with open('url.txt') as f:\n",
    "            for line in f: \n",
    "                cell = csv_readline(line)\n",
    "                self.urls[cell[1]] = cell[4]\n",
    "\n",
    "    def mapper(self, _, line):\n",
    "        #these are the logs        \n",
    "        cell = csv_readline(line)\n",
    "        if cell[1] in self.urls.keys():\n",
    "            yield cell[1], (self.urls[cell[1]], cell[3]) #yield the matching rows.     \n",
    "        else:\n",
    "            yield None, (None , cell[3])\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    rightjoin.run()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "!chmod a+x hashsiderightjoin_52.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:mrjob.runner:\n",
      "WARNING:mrjob.runner:PLEASE NOTE: Starting in mrjob v0.5.0, protocols will be strict by default. It's recommended you run your job with --strict-protocols or set up mrjob.conf as described at https://pythonhosted.org/mrjob/whats-new.html#ready-for-strict-protocols\n",
      "WARNING:mrjob.runner:\n",
      "WARNING:mrjob.step:MRJobStep has been renamed to MRStep. The old name will be removed in v0.5.0.\n",
      "WARNING:mrjob.step:MRJobStep has been renamed to MRStep. The old name will be removed in v0.5.0.\n",
      "WARNING:mrjob.step:MRJobStep has been renamed to MRStep. The old name will be removed in v0.5.0.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "There are 98654 records for right join\n"
     ]
    }
   ],
   "source": [
    "%reload_ext autoreload\n",
    "%autoreload 2\n",
    "# Running mrjob using a Hadoop Runner in local cluster\n",
    "from hashsiderightjoin_52 import rightjoin\n",
    "import os\n",
    "\n",
    "# Passing Hadoop Streaming parameters to:\n",
    "# partition by leftmost part of composite key\n",
    "# secodary sort by rightmost part of the same composite key\n",
    "\n",
    "mr_job = rightjoin(args=['anonymous-msweb.data.pp', \n",
    "                        '--file', 'url.txt'])\n",
    "\n",
    "output_file = \"output_hw52_right.txt\"\n",
    "try:\n",
    "    os.remove(output_file)\n",
    "except OSError:\n",
    "    pass\n",
    "\n",
    "with mr_job.make_runner() as runner, open(output_file, 'a') as f: \n",
    "    runner.run()\n",
    "    count = 0\n",
    "    # stream_output: get access of the output \n",
    "    for line in runner.stream_output():\n",
    "        #print mr_job.parse_output_line(line)\n",
    "        f.write(str(line))\n",
    "        count+=1\n",
    "print \"\\n\"\n",
    "print \"There are %s records for right join\" %count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of results:\n",
      "   98654\n",
      "-----\n",
      "first 10 rows\n",
      "page_id [url, user_id]\n",
      "\"1000\"\t[\"/regwiz\", \"10001\"]\n",
      "\"1001\"\t[\"/support\", \"10001\"]\n",
      "\"1002\"\t[\"/athome\", \"10001\"]\n",
      "\"1001\"\t[\"/support\", \"10002\"]\n",
      "\"1003\"\t[\"/kb\", \"10002\"]\n",
      "\"1001\"\t[\"/support\", \"10003\"]\n",
      "\"1003\"\t[\"/kb\", \"10003\"]\n",
      "\"1004\"\t[\"/search\", \"10003\"]\n",
      "\"1005\"\t[\"/norge\", \"10004\"]\n",
      "\"1006\"\t[\"/misc\", \"10005\"]\n"
     ]
    }
   ],
   "source": [
    "!echo \"Number of results:\"\n",
    "!wc -l < output_hw52_right.txt\n",
    "!echo \"-----\"\n",
    "!echo \"first 10 rows\"\n",
    "!echo \"page_id [url, user_id]\"\n",
    "!head -10 output_hw52_right.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## HW 5.3 - 5.5\n",
    "\n",
    "For the remainder of this assignment you will work with two datasets:\n",
    "\n",
    "#### 1: unit/systems test data set: SYSTEMS TEST DATASET\n",
    "Three terms, A,B,C and their corresponding strip-docs of co-occurring terms\n",
    "\n",
    "DocA {X:20, Y:30, Z:5}\n",
    "DocB {X:100, Y:20}\n",
    "DocC {M:5, N:20, Z:5}\n",
    "\n",
    "\n",
    "#### 2: A large subset of the Google n-grams dataset\n",
    "\n",
    "https://aws.amazon.com/datasets/google-books-ngrams/\n",
    "\n",
    "which we have placed in a bucket/folder on Dropbox on s3:\n",
    "\n",
    "   https://www.dropbox.com/sh/tmqpc4o0xswhkvz/AACUifrl6wrMrlK6a3X3lZ9Ea?dl=0 \n",
    "\n",
    "   s3://filtered-5grams/\n",
    "\n",
    "For each HW 5.3 -5.5 Please unit test and system test your code with with SYSTEMS TEST DATASET and show the results. \n",
    "Please compute the expected answer by hand and show your hand calculations. Then show the results you get with you system.\n",
    "Final show your results on the Google n-grams dataset\n",
    "\n",
    "\n",
    "In particular, this bucket contains (~200) files (10Meg each) in the format:\n",
    "\n",
    "\t(ngram) \\t (count) \\t (pages_count) \\t (books_count)\n",
    "\n",
    "## HW 5.3\n",
    "\n",
    "Do some EDA on this dataset using mrjob, e.g., \n",
    "\n",
    "- Longest 5-gram (number of characters)\n",
    "- Top 10 most frequent words (count), i.e., unigrams\n",
    "- Most/Least densely appearing words (count/pages_count) sorted in decreasing order of relative frequency (Hint: save to PART-000* and take the head -n 1000)\n",
    "- Distribution of 5-gram sizes (counts) sorted in decreasing order of relative frequency. (Hint: save to PART-000* and take the head -n 1000)\n",
    "OPTIONAL Question:\n",
    "- Plot the log-log plot of the frequency distributuion of unigrams. Does it follow power law distribution?\n",
    "\n",
    "For more background see: https://en.wikipedia.org/wiki/Log%E2%80%93log_plot https://en.wikipedia.org/wiki/Power_law"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting mrjob_longest_53.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile mrjob_longest_53.py\n",
    "from mrjob.job import MRJob\n",
    "from mrjob.step import MRJobStep\n",
    "from mrjob.compat import get_jobconf_value\n",
    "\n",
    "class longest_ngram(MRJob):\n",
    "    long_ngram = None\n",
    "    long_length = 0\n",
    "    \n",
    "    def steps(self):\n",
    "        return [MRJobStep(mapper=self.mapper,\n",
    "                         reducer=self.reducer,\n",
    "                         reducer_final=self.reducer_final,\n",
    "                        jobconf={\n",
    "                            \"mapred.map.tasks\":4,\n",
    "                            \"mapred.reduce.tasks\":1,\n",
    "                            })]\n",
    "        \n",
    "    def mapper(self, _, line):\n",
    "        #break out the lengths of the cells\n",
    "        cell = line.split('\\t')\n",
    "        length = len(cell[0])\n",
    "        yield cell[0], length\n",
    "        \n",
    "    def reducer(self, key, value):\n",
    "        #Add to global values if largest \n",
    "        value = list(value)\n",
    "        if sum(value) > self.long_length:\n",
    "            self.long_ngram = key\n",
    "            self.long_length = sum(value)\n",
    "            \n",
    "    def reducer_final(self):\n",
    "        #output largest ngram and length\n",
    "        yield self.long_ngram, self.long_length\n",
    "        \n",
    "if __name__ == '__main__':\n",
    "    longest_ngram.run()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING: Module python-magic is not available. Guessing MIME types based on file extensions.\n",
      "upload: 'mrjob_longest_53.py' -> 's3://w261jing/mrjob_longest_53.py'  [1 of 1]\n",
      " 1120 of 1120   100% in    0s     4.63 kB/s  done\n"
     ]
    }
   ],
   "source": [
    "!s3cmd put FILE mrjob_longest_53.py s3://w261jing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:mrjob.runner:\n",
      "WARNING:mrjob.runner:PLEASE NOTE: Starting in mrjob v0.5.0, protocols will be strict by default. It's recommended you run your job with --strict-protocols or set up mrjob.conf as described at https://pythonhosted.org/mrjob/whats-new.html#ready-for-strict-protocols\n",
      "WARNING:mrjob.runner:\n",
      "WARNING:mrjob.step:MRJobStep has been renamed to MRStep. The old name will be removed in v0.5.0.\n",
      "WARNING:mrjob.step:MRJobStep has been renamed to MRStep. The old name will be removed in v0.5.0.\n",
      "WARNING:mrjob.step:MRJobStep has been renamed to MRStep. The old name will be removed in v0.5.0.\n",
      "WARNING:mrjob.step:MRJobStep has been renamed to MRStep. The old name will be removed in v0.5.0.\n",
      "WARNING:mrjob.step:MRJobStep has been renamed to MRStep. The old name will be removed in v0.5.0.\n",
      "WARNING:mrjob.step:MRJobStep has been renamed to MRStep. The old name will be removed in v0.5.0.\n",
      "WARNING:mrjob.step:MRJobStep has been renamed to MRStep. The old name will be removed in v0.5.0.\n",
      "WARNING:mrjob.step:MRJobStep has been renamed to MRStep. The old name will be removed in v0.5.0.\n",
      "WARNING:mrjob.step:MRJobStep has been renamed to MRStep. The old name will be removed in v0.5.0.\n",
      "WARNING:mrjob.step:MRJobStep has been renamed to MRStep. The old name will be removed in v0.5.0.\n",
      "WARNING:mrjob.step:MRJobStep has been renamed to MRStep. The old name will be removed in v0.5.0.\n",
      "WARNING:mrjob.step:MRJobStep has been renamed to MRStep. The old name will be removed in v0.5.0.\n",
      "WARNING:mrjob.step:MRJobStep has been renamed to MRStep. The old name will be removed in v0.5.0.\n",
      "WARNING:mrjob.step:MRJobStep has been renamed to MRStep. The old name will be removed in v0.5.0.\n",
      "WARNING:mrjob.step:MRJobStep has been renamed to MRStep. The old name will be removed in v0.5.0.\n",
      "WARNING:mrjob.step:MRJobStep has been renamed to MRStep. The old name will be removed in v0.5.0.\n",
      "WARNING:mrjob.step:MRJobStep has been renamed to MRStep. The old name will be removed in v0.5.0.\n"
     ]
    }
   ],
   "source": [
    "%reload_ext autoreload\n",
    "%autoreload 2\n",
    "# Running mrjob using a Hadoop Runner in local cluster for systems/unit test\n",
    "from mrjob_longest_53 import longest_ngram\n",
    "import os\n",
    "\n",
    "# Passing Hadoop Streaming parameters to:\n",
    "# partition by leftmost part of composite key\n",
    "# secodary sort by rightmost part of the same composite key\n",
    "\n",
    "mr_job = longest_ngram(args=['testngram/'])\n",
    "#mr_job = longest_ngram(args=['ngramstest.txt'])\n",
    "\n",
    "output_file = \"output_hw53_length.txt\"\n",
    "#output_file = \"output_hw53_test_length.txt\"\n",
    "try:\n",
    "    os.remove(output_file)\n",
    "except OSError:\n",
    "    pass\n",
    "\n",
    "with mr_job.make_runner() as runner, open(output_file, 'a') as f: \n",
    "    runner.run()\n",
    "    # stream_output: get access of the output \n",
    "    for line in runner.stream_output():\n",
    "        #print mr_job.parse_output_line(line)\n",
    "        f.write(str(line))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\"Ocherki istorii gosudarstvennykh uchrezhdenii dorevoliutsionnoi\"\t63\r\n"
     ]
    }
   ],
   "source": [
    "!cat output_hw53_length.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Got unexpected keyword arguments: ssh_tunnel\n",
      "inferring aws_region from scratch bucket's region (us-west-1)\n",
      "using s3://mrjob-0465390d52fc9db7/tmp/ as our scratch dir on S3\n",
      "creating tmp directory /var/folders/zs/k144hqks281fbt0x68c_zj9m0000gp/T/mrjob_longest_53.JingXu.20160216.180723.482758\n",
      "writing master bootstrap script to /var/folders/zs/k144hqks281fbt0x68c_zj9m0000gp/T/mrjob_longest_53.JingXu.20160216.180723.482758/b.py\n",
      "Copying non-input files into s3://mrjob-0465390d52fc9db7/tmp/mrjob_longest_53.JingXu.20160216.180723.482758/files/\n",
      "Waiting 5.0s for S3 eventual consistency\n",
      "Creating Elastic MapReduce job flow\n",
      "Can't access IAM API, trying default instance profile: EMR_EC2_DefaultRole\n",
      "Can't access IAM API, trying default service role: EMR_DefaultRole\n",
      "Job flow created with ID: j-3IAD38WCPFWGE\n",
      "Created new job flow j-3IAD38WCPFWGE\n",
      "Job launched 30.2s ago, status STARTING: Provisioning Amazon EC2 capacity\n",
      "Job launched 60.5s ago, status STARTING: Provisioning Amazon EC2 capacity\n",
      "Job launched 90.7s ago, status STARTING: Provisioning Amazon EC2 capacity\n",
      "Job launched 121.0s ago, status STARTING: Configuring cluster software\n",
      "Job launched 151.3s ago, status STARTING: Configuring cluster software\n",
      "Job launched 181.6s ago, status BOOTSTRAPPING: Running bootstrap actions\n",
      "Job launched 211.9s ago, status BOOTSTRAPPING: Running bootstrap actions\n",
      "Job launched 242.2s ago, status RUNNING: Running step (mrjob_longest_53.JingXu.20160216.180723.482758: Step 1 of 1)\n",
      "Opening ssh tunnel to Hadoop job tracker\n",
      "Connect to job tracker at: http://localhost:40472/jobtracker.jsp\n",
      "Job launched 273.5s ago, status RUNNING: Running step (mrjob_longest_53.JingXu.20160216.180723.482758: Step 1 of 1)\n",
      "Unable to load progress from job tracker\n",
      "Job launched 303.7s ago, status RUNNING: Running step (mrjob_longest_53.JingXu.20160216.180723.482758: Step 1 of 1)\n",
      "Job launched 334.0s ago, status RUNNING: Running step (mrjob_longest_53.JingXu.20160216.180723.482758: Step 1 of 1)\n",
      "Oops, ssh subprocess exited with return code 255, restarting...\n",
      "Opening ssh tunnel to Hadoop job tracker\n",
      "Connect to job tracker at: http://localhost:40472/jobtracker.jsp\n",
      "Job launched 365.2s ago, status RUNNING: Running step (mrjob_longest_53.JingXu.20160216.180723.482758: Step 1 of 1)\n",
      "Unable to load progress from job tracker\n",
      "Job launched 395.7s ago, status RUNNING: Running step (mrjob_longest_53.JingXu.20160216.180723.482758: Step 1 of 1)\n",
      "Job launched 425.9s ago, status RUNNING: Running step (mrjob_longest_53.JingXu.20160216.180723.482758: Step 1 of 1)\n",
      "Oops, ssh subprocess exited with return code 255, restarting...\n",
      "Opening ssh tunnel to Hadoop job tracker\n",
      "Connect to job tracker at: http://localhost:40472/jobtracker.jsp\n",
      "Job launched 457.3s ago, status RUNNING: Running step (mrjob_longest_53.JingXu.20160216.180723.482758: Step 1 of 1)\n",
      "Unable to load progress from job tracker\n",
      "Job launched 487.5s ago, status RUNNING: Running step (mrjob_longest_53.JingXu.20160216.180723.482758: Step 1 of 1)\n",
      "Job launched 518.0s ago, status RUNNING: Running step (mrjob_longest_53.JingXu.20160216.180723.482758: Step 1 of 1)\n",
      "Oops, ssh subprocess exited with return code 255, restarting...\n",
      "Opening ssh tunnel to Hadoop job tracker\n",
      "Connect to job tracker at: http://localhost:40472/jobtracker.jsp\n",
      "Job launched 549.3s ago, status RUNNING: Running step (mrjob_longest_53.JingXu.20160216.180723.482758: Step 1 of 1)\n",
      "Unable to load progress from job tracker\n",
      "Job launched 579.5s ago, status RUNNING: Running step (mrjob_longest_53.JingXu.20160216.180723.482758: Step 1 of 1)\n",
      "Job launched 609.8s ago, status RUNNING: Running step (mrjob_longest_53.JingXu.20160216.180723.482758: Step 1 of 1)\n",
      "Oops, ssh subprocess exited with return code 255, restarting...\n",
      "Opening ssh tunnel to Hadoop job tracker\n",
      "Connect to job tracker at: http://localhost:40472/jobtracker.jsp\n",
      "Job launched 641.0s ago, status RUNNING: Running step (mrjob_longest_53.JingXu.20160216.180723.482758: Step 1 of 1)\n",
      "Unable to load progress from job tracker\n",
      "Job launched 671.3s ago, status RUNNING: Running step (mrjob_longest_53.JingXu.20160216.180723.482758: Step 1 of 1)\n",
      "Job launched 701.5s ago, status RUNNING: Running step (mrjob_longest_53.JingXu.20160216.180723.482758: Step 1 of 1)\n",
      "Oops, ssh subprocess exited with return code 255, restarting...\n",
      "Opening ssh tunnel to Hadoop job tracker\n",
      "Connect to job tracker at: http://localhost:40472/jobtracker.jsp\n",
      "Job launched 732.8s ago, status RUNNING: Running step (mrjob_longest_53.JingXu.20160216.180723.482758: Step 1 of 1)\n",
      "Unable to load progress from job tracker\n",
      "Job launched 763.1s ago, status RUNNING: Running step (mrjob_longest_53.JingXu.20160216.180723.482758: Step 1 of 1)\n",
      "Job launched 793.5s ago, status RUNNING: Running step (mrjob_longest_53.JingXu.20160216.180723.482758: Step 1 of 1)\n",
      "Oops, ssh subprocess exited with return code 255, restarting...\n",
      "Opening ssh tunnel to Hadoop job tracker\n",
      "Connect to job tracker at: http://localhost:40472/jobtracker.jsp\n",
      "Job launched 824.7s ago, status RUNNING: Running step (mrjob_longest_53.JingXu.20160216.180723.482758: Step 1 of 1)\n",
      "Unable to load progress from job tracker\n",
      "Job launched 854.9s ago, status RUNNING: Running step (mrjob_longest_53.JingXu.20160216.180723.482758: Step 1 of 1)\n",
      "Job launched 885.2s ago, status RUNNING: Running step (mrjob_longest_53.JingXu.20160216.180723.482758: Step 1 of 1)\n",
      "Oops, ssh subprocess exited with return code 255, restarting...\n",
      "Opening ssh tunnel to Hadoop job tracker\n",
      "Connect to job tracker at: http://localhost:40472/jobtracker.jsp\n",
      "Job launched 916.4s ago, status RUNNING: Running step (mrjob_longest_53.JingXu.20160216.180723.482758: Step 1 of 1)\n",
      "Unable to load progress from job tracker\n",
      "Job launched 946.8s ago, status RUNNING: Running step (mrjob_longest_53.JingXu.20160216.180723.482758: Step 1 of 1)\n",
      "Job launched 977.0s ago, status RUNNING: Running step (mrjob_longest_53.JingXu.20160216.180723.482758: Step 1 of 1)\n",
      "Oops, ssh subprocess exited with return code 255, restarting...\n",
      "Opening ssh tunnel to Hadoop job tracker\n",
      "Connect to job tracker at: http://localhost:40472/jobtracker.jsp\n",
      "Job launched 1008.3s ago, status RUNNING: Running step (mrjob_longest_53.JingXu.20160216.180723.482758: Step 1 of 1)\n",
      "Unable to load progress from job tracker\n",
      "Job launched 1038.6s ago, status RUNNING: Running step (mrjob_longest_53.JingXu.20160216.180723.482758: Step 1 of 1)\n",
      "Job launched 1069.0s ago, status RUNNING: Running step (mrjob_longest_53.JingXu.20160216.180723.482758: Step 1 of 1)\n",
      "Oops, ssh subprocess exited with return code 255, restarting...\n",
      "Opening ssh tunnel to Hadoop job tracker\n",
      "Connect to job tracker at: http://localhost:40472/jobtracker.jsp\n",
      "Job launched 1100.3s ago, status RUNNING: Running step (mrjob_longest_53.JingXu.20160216.180723.482758: Step 1 of 1)\n",
      "Unable to load progress from job tracker\n",
      "Job launched 1130.5s ago, status RUNNING: Running step (mrjob_longest_53.JingXu.20160216.180723.482758: Step 1 of 1)\n",
      "Job launched 1160.8s ago, status RUNNING: Running step (mrjob_longest_53.JingXu.20160216.180723.482758: Step 1 of 1)\n",
      "Oops, ssh subprocess exited with return code 255, restarting...\n",
      "Opening ssh tunnel to Hadoop job tracker\n",
      "Connect to job tracker at: http://localhost:40472/jobtracker.jsp\n",
      "Job launched 1192.0s ago, status RUNNING: Running step (mrjob_longest_53.JingXu.20160216.180723.482758: Step 1 of 1)\n",
      "Unable to load progress from job tracker\n",
      "Job launched 1222.3s ago, status RUNNING: Running step (mrjob_longest_53.JingXu.20160216.180723.482758: Step 1 of 1)\n",
      "Job launched 1252.6s ago, status RUNNING: Running step (mrjob_longest_53.JingXu.20160216.180723.482758: Step 1 of 1)\n",
      "Oops, ssh subprocess exited with return code 255, restarting...\n",
      "Opening ssh tunnel to Hadoop job tracker\n",
      "Connect to job tracker at: http://localhost:40472/jobtracker.jsp\n",
      "Job launched 1283.9s ago, status RUNNING: Running step (mrjob_longest_53.JingXu.20160216.180723.482758: Step 1 of 1)\n",
      "Unable to load progress from job tracker\n",
      "Job launched 1314.2s ago, status RUNNING: Running step (mrjob_longest_53.JingXu.20160216.180723.482758: Step 1 of 1)\n",
      "Job launched 1344.6s ago, status RUNNING: Running step (mrjob_longest_53.JingXu.20160216.180723.482758: Step 1 of 1)\n",
      "Oops, ssh subprocess exited with return code 255, restarting...\n",
      "Opening ssh tunnel to Hadoop job tracker\n",
      "Connect to job tracker at: http://localhost:40472/jobtracker.jsp\n",
      "Job launched 1375.9s ago, status RUNNING: Running step (mrjob_longest_53.JingXu.20160216.180723.482758: Step 1 of 1)\n",
      "Unable to load progress from job tracker\n",
      "Job launched 1406.1s ago, status RUNNING: Running step (mrjob_longest_53.JingXu.20160216.180723.482758: Step 1 of 1)\n",
      "Job launched 1436.3s ago, status RUNNING: Running step (mrjob_longest_53.JingXu.20160216.180723.482758: Step 1 of 1)\n",
      "Oops, ssh subprocess exited with return code 255, restarting...\n",
      "Opening ssh tunnel to Hadoop job tracker\n",
      "Connect to job tracker at: http://localhost:40472/jobtracker.jsp\n",
      "Job launched 1467.7s ago, status RUNNING: Running step (mrjob_longest_53.JingXu.20160216.180723.482758: Step 1 of 1)\n",
      "Unable to load progress from job tracker\n",
      "Job launched 1498.0s ago, status RUNNING: Running step (mrjob_longest_53.JingXu.20160216.180723.482758: Step 1 of 1)\n",
      "Job launched 1528.2s ago, status RUNNING: Running step (mrjob_longest_53.JingXu.20160216.180723.482758: Step 1 of 1)\n",
      "Oops, ssh subprocess exited with return code 255, restarting...\n",
      "Opening ssh tunnel to Hadoop job tracker\n",
      "Connect to job tracker at: http://localhost:40472/jobtracker.jsp\n",
      "Job launched 1559.5s ago, status RUNNING: Running step (mrjob_longest_53.JingXu.20160216.180723.482758: Step 1 of 1)\n",
      "Unable to load progress from job tracker\n",
      "Job launched 1589.8s ago, status RUNNING: Running step (mrjob_longest_53.JingXu.20160216.180723.482758: Step 1 of 1)\n",
      "Job launched 1620.0s ago, status RUNNING: Running step (mrjob_longest_53.JingXu.20160216.180723.482758: Step 1 of 1)\n",
      "Oops, ssh subprocess exited with return code 255, restarting...\n",
      "Opening ssh tunnel to Hadoop job tracker\n",
      "Connect to job tracker at: http://localhost:40472/jobtracker.jsp\n",
      "Job launched 1651.3s ago, status RUNNING: Running step (mrjob_longest_53.JingXu.20160216.180723.482758: Step 1 of 1)\n",
      "Unable to load progress from job tracker\n",
      "Job launched 1681.6s ago, status RUNNING: Running step (mrjob_longest_53.JingXu.20160216.180723.482758: Step 1 of 1)\n",
      "Job launched 1712.0s ago, status RUNNING: Running step (mrjob_longest_53.JingXu.20160216.180723.482758: Step 1 of 1)\n",
      "Oops, ssh subprocess exited with return code 255, restarting...\n",
      "Opening ssh tunnel to Hadoop job tracker\n",
      "Connect to job tracker at: http://localhost:40472/jobtracker.jsp\n",
      "Job launched 1743.3s ago, status RUNNING: Running step (mrjob_longest_53.JingXu.20160216.180723.482758: Step 1 of 1)\n",
      "Unable to load progress from job tracker\n",
      "Job launched 1773.9s ago, status RUNNING: Running step (mrjob_longest_53.JingXu.20160216.180723.482758: Step 1 of 1)\n",
      "Job launched 1804.2s ago, status RUNNING: Running step (mrjob_longest_53.JingXu.20160216.180723.482758: Step 1 of 1)\n",
      "Oops, ssh subprocess exited with return code 255, restarting...\n",
      "Opening ssh tunnel to Hadoop job tracker\n",
      "Connect to job tracker at: http://localhost:40472/jobtracker.jsp\n",
      "Job launched 1835.5s ago, status RUNNING: Running step (mrjob_longest_53.JingXu.20160216.180723.482758: Step 1 of 1)\n",
      "Unable to load progress from job tracker\n",
      "Job launched 1865.8s ago, status RUNNING: Running step (mrjob_longest_53.JingXu.20160216.180723.482758: Step 1 of 1)\n",
      "Job launched 1897.0s ago, status RUNNING: Running step (mrjob_longest_53.JingXu.20160216.180723.482758: Step 1 of 1)\n",
      "Oops, ssh subprocess exited with return code 255, restarting...\n",
      "Opening ssh tunnel to Hadoop job tracker\n",
      "Connect to job tracker at: http://localhost:40472/jobtracker.jsp\n",
      "Job launched 1928.3s ago, status RUNNING: Running step (mrjob_longest_53.JingXu.20160216.180723.482758: Step 1 of 1)\n",
      "Unable to load progress from job tracker\n",
      "Job launched 1958.5s ago, status RUNNING: Running step (mrjob_longest_53.JingXu.20160216.180723.482758: Step 1 of 1)\n",
      "Job launched 1988.8s ago, status RUNNING: Running step (mrjob_longest_53.JingXu.20160216.180723.482758: Step 1 of 1)\n",
      "Oops, ssh subprocess exited with return code 255, restarting...\n",
      "Opening ssh tunnel to Hadoop job tracker\n",
      "Connect to job tracker at: http://localhost:40472/jobtracker.jsp\n",
      "Job launched 2020.0s ago, status RUNNING: Running step (mrjob_longest_53.JingXu.20160216.180723.482758: Step 1 of 1)\n",
      "Unable to load progress from job tracker\n",
      "Job launched 2050.2s ago, status RUNNING: Running step (mrjob_longest_53.JingXu.20160216.180723.482758: Step 1 of 1)\n",
      "Job launched 2080.5s ago, status RUNNING: Running step (mrjob_longest_53.JingXu.20160216.180723.482758: Step 1 of 1)\n",
      "Oops, ssh subprocess exited with return code 255, restarting...\n",
      "Opening ssh tunnel to Hadoop job tracker\n",
      "Connect to job tracker at: http://localhost:40472/jobtracker.jsp\n",
      "Job launched 2111.8s ago, status RUNNING: Running step (mrjob_longest_53.JingXu.20160216.180723.482758: Step 1 of 1)\n",
      "Unable to load progress from job tracker\n",
      "Job launched 2142.1s ago, status RUNNING: Running step (mrjob_longest_53.JingXu.20160216.180723.482758: Step 1 of 1)\n",
      "Job launched 2172.6s ago, status RUNNING: Running step (mrjob_longest_53.JingXu.20160216.180723.482758: Step 1 of 1)\n",
      "Oops, ssh subprocess exited with return code 255, restarting...\n",
      "Opening ssh tunnel to Hadoop job tracker\n",
      "Connect to job tracker at: http://localhost:40472/jobtracker.jsp\n",
      "Job launched 2203.9s ago, status RUNNING: Running step (mrjob_longest_53.JingXu.20160216.180723.482758: Step 1 of 1)\n",
      "Unable to load progress from job tracker\n",
      "Job launched 2234.2s ago, status RUNNING: Running step (mrjob_longest_53.JingXu.20160216.180723.482758: Step 1 of 1)\n",
      "Job launched 2264.7s ago, status RUNNING: Running step (mrjob_longest_53.JingXu.20160216.180723.482758: Step 1 of 1)\n",
      "Oops, ssh subprocess exited with return code 255, restarting...\n",
      "Opening ssh tunnel to Hadoop job tracker\n",
      "Connect to job tracker at: http://localhost:40472/jobtracker.jsp\n",
      "Job launched 2296.0s ago, status RUNNING: Running step (mrjob_longest_53.JingXu.20160216.180723.482758: Step 1 of 1)\n",
      "Unable to load progress from job tracker\n",
      "Job launched 2326.4s ago, status RUNNING: Running step (mrjob_longest_53.JingXu.20160216.180723.482758: Step 1 of 1)\n",
      "Job launched 2356.7s ago, status RUNNING: Running step (mrjob_longest_53.JingXu.20160216.180723.482758: Step 1 of 1)\n",
      "Oops, ssh subprocess exited with return code 255, restarting...\n",
      "Opening ssh tunnel to Hadoop job tracker\n",
      "Connect to job tracker at: http://localhost:40472/jobtracker.jsp\n",
      "Job completed.\n",
      "Running time was 2137.0s (not counting time spent waiting for the EC2 instances)\n",
      "Fetching counters from S3...\n",
      "Waiting 5.0s for S3 eventual consistency\n",
      "Counters from step 1:\n",
      "  File Input Format Counters :\n",
      "    Bytes Read: 2156069116\n",
      "  File Output Format Counters :\n",
      "    Bytes Written: 166\n",
      "  FileSystemCounters:\n",
      "    FILE_BYTES_READ: 899297035\n",
      "    FILE_BYTES_WRITTEN: 1969717846\n",
      "    HDFS_BYTES_READ: 23640\n",
      "    S3_BYTES_READ: 2156069116\n",
      "    S3_BYTES_WRITTEN: 166\n",
      "  Job Counters :\n",
      "    Launched map tasks: 192\n",
      "    Launched reduce tasks: 1\n",
      "    Rack-local map tasks: 189\n",
      "    SLOTS_MILLIS_MAPS: 3742852\n",
      "    SLOTS_MILLIS_REDUCES: 2041053\n",
      "    Total time spent by all maps waiting after reserving slots (ms): 0\n",
      "    Total time spent by all reduces waiting after reserving slots (ms): 0\n",
      "  Map-Reduce Framework:\n",
      "    CPU time spent (ms): 2836260\n",
      "    Combine input records: 0\n",
      "    Combine output records: 0\n",
      "    Map input bytes: 2156069116\n",
      "    Map input records: 58682266\n",
      "    Map output bytes: 1862272363\n",
      "    Map output materialized bytes: 1065344938\n",
      "    Map output records: 58682266\n",
      "    Physical memory (bytes) snapshot: 61036539904\n",
      "    Reduce input groups: 58682266\n",
      "    Reduce input records: 58682266\n",
      "    Reduce output records: 1\n",
      "    Reduce shuffle bytes: 1065344938\n",
      "    SPLIT_RAW_BYTES: 23640\n",
      "    Spilled Records: 117364532\n",
      "    Total committed heap usage (bytes): 53433860096\n",
      "    Virtual memory (bytes) snapshot: 180192849920\n",
      "removing tmp directory /var/folders/zs/k144hqks281fbt0x68c_zj9m0000gp/T/mrjob_longest_53.JingXu.20160216.180723.482758\n",
      "Removing all files in s3://mrjob-0465390d52fc9db7/tmp/mrjob_longest_53.JingXu.20160216.180723.482758/\n",
      "Removing all files in s3://mrjob-0465390d52fc9db7/tmp/logs/j-3IAD38WCPFWGE/\n",
      "Killing our SSH tunnel (pid 16194)\n",
      "Terminating job flow: j-3IAD38WCPFWGE\n"
     ]
    }
   ],
   "source": [
    "!s3cmd rm --recursive s3://w261jing/hw5/longest53/\n",
    "!python mrjob_longest_53.py -r emr --conf-path mrjob_261jing.conf s3://filtered-5grams/ --output-dir=s3://w261jing/hw5/longest53 --no-output --no-strict-protocol"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "download: 's3://w261jing/hw5/longest53/part-00000' -> 'longest53_out.txt'  [1 of 1]\n",
      " 166 of 166   100% in    0s     3.40 kB/s  done\n",
      "\"AIOPJUMRXUYVASLYHYPSIBEMAPODIKR UFRYDIUUOLBIGASUAURUSREXLISNAYE RNOONDQSRUNSUBUNOUGRABBERYAIRTC UTAHRAPTOREDILEIPMILBDUMMYUVERI SYEVRAHVELOCYALLOSAURUSLINROTSR\"\t159\n"
     ]
    }
   ],
   "source": [
    "!s3cmd get s3://w261jing/hw5/longest53/part* longest53_out.txt\n",
    "!head longest53_out.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting mrjob_frequency_53.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile mrjob_frequency_53.py\n",
    "#!/usr/bin/python\n",
    "## mrjob_frequency_53.py\n",
    "## Author: Angela Gunn & Jing Xu\n",
    "## Description:Find the frequency of a word in the 5-gram\n",
    "\n",
    "from mrjob.job import MRJob\n",
    "from mrjob.step import MRJobStep\n",
    "from mrjob.compat import get_jobconf_value\n",
    "\n",
    "import re\n",
    "\n",
    "WORD_RE = re.compile(r\"[A-Za-z0-9]+\")\n",
    "\n",
    "class frequency(MRJob):\n",
    "    #top10={}\n",
    "    \n",
    "    def steps(self):\n",
    "        return [\n",
    "             MRJobStep(mapper=self.mapper,\n",
    "                   combiner=self.combiner,\n",
    "                   reducer=self.reducer,\n",
    "                   jobconf={\n",
    "                            \"mapred.map.tasks\":16,\n",
    "                            \"mapred.reduce.tasks\":8,\n",
    "                            }),\n",
    "             MRJobStep(mapper=self.mapper_frequent_unigrams,\n",
    "                   reducer=self.reducer_frequent_unigrams,\n",
    "                   jobconf={\n",
    "                            'mapred.output.key.comparator.class': 'org.apache.hadoop.mapred.lib.KeyFieldBasedComparator',\n",
    "                            'mapred.text.key.comparator.options': '-k1,1rn',\n",
    "                            \"mapred.map.tasks\":4,\n",
    "                            \"mapred.reduce.tasks\":1,\n",
    "                            }\n",
    "                   )\n",
    "        ]\n",
    "    \n",
    "    def mapper(self, _, line):\n",
    "        #get the word, and count for output\n",
    "        line.strip()\n",
    "        cell = re.split(\"\\t\",line)\n",
    "        unigrams = cell[0].split()\n",
    "        count = int(cell[1])\n",
    "        for unigram in unigrams:\n",
    "            yield unigram, count\n",
    "            \n",
    "    def combiner(self, unigram, counts):\n",
    "        yield unigram, sum(counts)\n",
    "        \n",
    "    def reducer(self, unigram, counts):\n",
    "        #combines the visits, and adds top10 dictionary if qualified\n",
    "        yield unigram, sum(counts)\n",
    "        \n",
    "    def mapper_frequent_unigrams(self, unigram, count):\n",
    "        #Just passing along with count first so that they all get shuffled by the count\n",
    "        yield count, unigram\n",
    "        \n",
    "    def reducer_frequent_unigrams(self, count, unigrams):\n",
    "        #Printing.\n",
    "        for unigram in unigrams:\n",
    "            yield count, unigram\n",
    "        \n",
    "if __name__ == '__main__':\n",
    "    frequency.run()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "!chmod a+x mrjob_frequency_53.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING: Module python-magic is not available. Guessing MIME types based on file extensions.\n",
      "upload: 'mrjob_frequency_53.py' -> 's3://w261jing/mrjob_frequency_53.py'  [1 of 1]\n",
      " 2072 of 2072   100% in    0s    15.18 kB/s  done\n"
     ]
    }
   ],
   "source": [
    "!s3cmd put FILE mrjob_frequency_53.py s3://w261jing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:mrjob.runner:\n",
      "WARNING:mrjob.runner:PLEASE NOTE: Starting in mrjob v0.5.0, protocols will be strict by default. It's recommended you run your job with --strict-protocols or set up mrjob.conf as described at https://pythonhosted.org/mrjob/whats-new.html#ready-for-strict-protocols\n",
      "WARNING:mrjob.runner:\n",
      "WARNING:mrjob.step:MRJobStep has been renamed to MRStep. The old name will be removed in v0.5.0.\n",
      "WARNING:mrjob.step:MRJobStep has been renamed to MRStep. The old name will be removed in v0.5.0.\n",
      "WARNING:mrjob.step:MRJobStep has been renamed to MRStep. The old name will be removed in v0.5.0.\n",
      "WARNING:mrjob.step:MRJobStep has been renamed to MRStep. The old name will be removed in v0.5.0.\n",
      "WARNING:mrjob.step:MRJobStep has been renamed to MRStep. The old name will be removed in v0.5.0.\n",
      "WARNING:mrjob.step:MRJobStep has been renamed to MRStep. The old name will be removed in v0.5.0.\n",
      "WARNING:mrjob.step:MRJobStep has been renamed to MRStep. The old name will be removed in v0.5.0.\n",
      "WARNING:mrjob.step:MRJobStep has been renamed to MRStep. The old name will be removed in v0.5.0.\n",
      "WARNING:mrjob.step:MRJobStep has been renamed to MRStep. The old name will be removed in v0.5.0.\n",
      "WARNING:mrjob.step:MRJobStep has been renamed to MRStep. The old name will be removed in v0.5.0.\n",
      "WARNING:mrjob.step:MRJobStep has been renamed to MRStep. The old name will be removed in v0.5.0.\n",
      "WARNING:mrjob.step:MRJobStep has been renamed to MRStep. The old name will be removed in v0.5.0.\n",
      "WARNING:mrjob.step:MRJobStep has been renamed to MRStep. The old name will be removed in v0.5.0.\n",
      "WARNING:mrjob.step:MRJobStep has been renamed to MRStep. The old name will be removed in v0.5.0.\n",
      "WARNING:mrjob.step:MRJobStep has been renamed to MRStep. The old name will be removed in v0.5.0.\n",
      "WARNING:mrjob.step:MRJobStep has been renamed to MRStep. The old name will be removed in v0.5.0.\n",
      "WARNING:mrjob.step:MRJobStep has been renamed to MRStep. The old name will be removed in v0.5.0.\n",
      "WARNING:mrjob.step:MRJobStep has been renamed to MRStep. The old name will be removed in v0.5.0.\n",
      "WARNING:mrjob.step:MRJobStep has been renamed to MRStep. The old name will be removed in v0.5.0.\n",
      "WARNING:mrjob.step:MRJobStep has been renamed to MRStep. The old name will be removed in v0.5.0.\n",
      "WARNING:mrjob.step:MRJobStep has been renamed to MRStep. The old name will be removed in v0.5.0.\n",
      "WARNING:mrjob.step:MRJobStep has been renamed to MRStep. The old name will be removed in v0.5.0.\n",
      "WARNING:mrjob.step:MRJobStep has been renamed to MRStep. The old name will be removed in v0.5.0.\n",
      "WARNING:mrjob.step:MRJobStep has been renamed to MRStep. The old name will be removed in v0.5.0.\n",
      "WARNING:mrjob.step:MRJobStep has been renamed to MRStep. The old name will be removed in v0.5.0.\n",
      "WARNING:mrjob.step:MRJobStep has been renamed to MRStep. The old name will be removed in v0.5.0.\n",
      "WARNING:mrjob.step:MRJobStep has been renamed to MRStep. The old name will be removed in v0.5.0.\n",
      "WARNING:mrjob.step:MRJobStep has been renamed to MRStep. The old name will be removed in v0.5.0.\n",
      "WARNING:mrjob.step:MRJobStep has been renamed to MRStep. The old name will be removed in v0.5.0.\n",
      "WARNING:mrjob.step:MRJobStep has been renamed to MRStep. The old name will be removed in v0.5.0.\n",
      "WARNING:mrjob.step:MRJobStep has been renamed to MRStep. The old name will be removed in v0.5.0.\n",
      "WARNING:mrjob.step:MRJobStep has been renamed to MRStep. The old name will be removed in v0.5.0.\n",
      "WARNING:mrjob.step:MRJobStep has been renamed to MRStep. The old name will be removed in v0.5.0.\n",
      "WARNING:mrjob.step:MRJobStep has been renamed to MRStep. The old name will be removed in v0.5.0.\n",
      "WARNING:mrjob.step:MRJobStep has been renamed to MRStep. The old name will be removed in v0.5.0.\n",
      "WARNING:mrjob.step:MRJobStep has been renamed to MRStep. The old name will be removed in v0.5.0.\n",
      "WARNING:mrjob.step:MRJobStep has been renamed to MRStep. The old name will be removed in v0.5.0.\n",
      "WARNING:mrjob.step:MRJobStep has been renamed to MRStep. The old name will be removed in v0.5.0.\n",
      "WARNING:mrjob.step:MRJobStep has been renamed to MRStep. The old name will be removed in v0.5.0.\n",
      "WARNING:mrjob.step:MRJobStep has been renamed to MRStep. The old name will be removed in v0.5.0.\n",
      "WARNING:mrjob.step:MRJobStep has been renamed to MRStep. The old name will be removed in v0.5.0.\n",
      "WARNING:mrjob.step:MRJobStep has been renamed to MRStep. The old name will be removed in v0.5.0.\n",
      "WARNING:mrjob.step:MRJobStep has been renamed to MRStep. The old name will be removed in v0.5.0.\n",
      "WARNING:mrjob.step:MRJobStep has been renamed to MRStep. The old name will be removed in v0.5.0.\n",
      "WARNING:mrjob.step:MRJobStep has been renamed to MRStep. The old name will be removed in v0.5.0.\n",
      "WARNING:mrjob.step:MRJobStep has been renamed to MRStep. The old name will be removed in v0.5.0.\n",
      "WARNING:mrjob.step:MRJobStep has been renamed to MRStep. The old name will be removed in v0.5.0.\n",
      "WARNING:mrjob.step:MRJobStep has been renamed to MRStep. The old name will be removed in v0.5.0.\n",
      "WARNING:mrjob.step:MRJobStep has been renamed to MRStep. The old name will be removed in v0.5.0.\n",
      "WARNING:mrjob.step:MRJobStep has been renamed to MRStep. The old name will be removed in v0.5.0.\n",
      "WARNING:mrjob.step:MRJobStep has been renamed to MRStep. The old name will be removed in v0.5.0.\n",
      "WARNING:mrjob.step:MRJobStep has been renamed to MRStep. The old name will be removed in v0.5.0.\n",
      "WARNING:mrjob.step:MRJobStep has been renamed to MRStep. The old name will be removed in v0.5.0.\n",
      "WARNING:mrjob.step:MRJobStep has been renamed to MRStep. The old name will be removed in v0.5.0.\n",
      "WARNING:mrjob.step:MRJobStep has been renamed to MRStep. The old name will be removed in v0.5.0.\n",
      "WARNING:mrjob.step:MRJobStep has been renamed to MRStep. The old name will be removed in v0.5.0.\n",
      "WARNING:mrjob.step:MRJobStep has been renamed to MRStep. The old name will be removed in v0.5.0.\n",
      "WARNING:mrjob.step:MRJobStep has been renamed to MRStep. The old name will be removed in v0.5.0.\n",
      "WARNING:mrjob.step:MRJobStep has been renamed to MRStep. The old name will be removed in v0.5.0.\n",
      "WARNING:mrjob.step:MRJobStep has been renamed to MRStep. The old name will be removed in v0.5.0.\n",
      "WARNING:mrjob.step:MRJobStep has been renamed to MRStep. The old name will be removed in v0.5.0.\n",
      "WARNING:mrjob.step:MRJobStep has been renamed to MRStep. The old name will be removed in v0.5.0.\n",
      "WARNING:mrjob.step:MRJobStep has been renamed to MRStep. The old name will be removed in v0.5.0.\n",
      "WARNING:mrjob.step:MRJobStep has been renamed to MRStep. The old name will be removed in v0.5.0.\n",
      "WARNING:mrjob.step:MRJobStep has been renamed to MRStep. The old name will be removed in v0.5.0.\n",
      "WARNING:mrjob.step:MRJobStep has been renamed to MRStep. The old name will be removed in v0.5.0.\n",
      "WARNING:mrjob.step:MRJobStep has been renamed to MRStep. The old name will be removed in v0.5.0.\n",
      "WARNING:mrjob.step:MRJobStep has been renamed to MRStep. The old name will be removed in v0.5.0.\n",
      "WARNING:mrjob.step:MRJobStep has been renamed to MRStep. The old name will be removed in v0.5.0.\n",
      "WARNING:mrjob.step:MRJobStep has been renamed to MRStep. The old name will be removed in v0.5.0.\n",
      "WARNING:mrjob.step:MRJobStep has been renamed to MRStep. The old name will be removed in v0.5.0.\n",
      "WARNING:mrjob.step:MRJobStep has been renamed to MRStep. The old name will be removed in v0.5.0.\n",
      "WARNING:mrjob.step:MRJobStep has been renamed to MRStep. The old name will be removed in v0.5.0.\n",
      "WARNING:mrjob.step:MRJobStep has been renamed to MRStep. The old name will be removed in v0.5.0.\n",
      "WARNING:mrjob.step:MRJobStep has been renamed to MRStep. The old name will be removed in v0.5.0.\n",
      "WARNING:mrjob.step:MRJobStep has been renamed to MRStep. The old name will be removed in v0.5.0.\n",
      "WARNING:mrjob.step:MRJobStep has been renamed to MRStep. The old name will be removed in v0.5.0.\n",
      "WARNING:mrjob.step:MRJobStep has been renamed to MRStep. The old name will be removed in v0.5.0.\n",
      "WARNING:mrjob.step:MRJobStep has been renamed to MRStep. The old name will be removed in v0.5.0.\n",
      "WARNING:mrjob.step:MRJobStep has been renamed to MRStep. The old name will be removed in v0.5.0.\n",
      "WARNING:mrjob.step:MRJobStep has been renamed to MRStep. The old name will be removed in v0.5.0.\n",
      "WARNING:mrjob.step:MRJobStep has been renamed to MRStep. The old name will be removed in v0.5.0.\n",
      "WARNING:mrjob.step:MRJobStep has been renamed to MRStep. The old name will be removed in v0.5.0.\n",
      "WARNING:mrjob.step:MRJobStep has been renamed to MRStep. The old name will be removed in v0.5.0.\n",
      "WARNING:mrjob.step:MRJobStep has been renamed to MRStep. The old name will be removed in v0.5.0.\n",
      "WARNING:mrjob.step:MRJobStep has been renamed to MRStep. The old name will be removed in v0.5.0.\n",
      "WARNING:mrjob.step:MRJobStep has been renamed to MRStep. The old name will be removed in v0.5.0.\n",
      "WARNING:mrjob.step:MRJobStep has been renamed to MRStep. The old name will be removed in v0.5.0.\n",
      "WARNING:mrjob.step:MRJobStep has been renamed to MRStep. The old name will be removed in v0.5.0.\n",
      "WARNING:mrjob.step:MRJobStep has been renamed to MRStep. The old name will be removed in v0.5.0.\n",
      "WARNING:mrjob.step:MRJobStep has been renamed to MRStep. The old name will be removed in v0.5.0.\n",
      "WARNING:mrjob.step:MRJobStep has been renamed to MRStep. The old name will be removed in v0.5.0.\n",
      "WARNING:mrjob.step:MRJobStep has been renamed to MRStep. The old name will be removed in v0.5.0.\n",
      "WARNING:mrjob.step:MRJobStep has been renamed to MRStep. The old name will be removed in v0.5.0.\n",
      "WARNING:mrjob.step:MRJobStep has been renamed to MRStep. The old name will be removed in v0.5.0.\n",
      "WARNING:mrjob.step:MRJobStep has been renamed to MRStep. The old name will be removed in v0.5.0.\n",
      "WARNING:mrjob.step:MRJobStep has been renamed to MRStep. The old name will be removed in v0.5.0.\n",
      "WARNING:mrjob.step:MRJobStep has been renamed to MRStep. The old name will be removed in v0.5.0.\n",
      "WARNING:mrjob.step:MRJobStep has been renamed to MRStep. The old name will be removed in v0.5.0.\n",
      "WARNING:mrjob.step:MRJobStep has been renamed to MRStep. The old name will be removed in v0.5.0.\n",
      "WARNING:mrjob.step:MRJobStep has been renamed to MRStep. The old name will be removed in v0.5.0.\n",
      "WARNING:mrjob.step:MRJobStep has been renamed to MRStep. The old name will be removed in v0.5.0.\n",
      "WARNING:mrjob.step:MRJobStep has been renamed to MRStep. The old name will be removed in v0.5.0.\n",
      "WARNING:mrjob.step:MRJobStep has been renamed to MRStep. The old name will be removed in v0.5.0.\n",
      "WARNING:mrjob.step:MRJobStep has been renamed to MRStep. The old name will be removed in v0.5.0.\n",
      "WARNING:mrjob.step:MRJobStep has been renamed to MRStep. The old name will be removed in v0.5.0.\n",
      "WARNING:mrjob.step:MRJobStep has been renamed to MRStep. The old name will be removed in v0.5.0.\n",
      "WARNING:mrjob.step:MRJobStep has been renamed to MRStep. The old name will be removed in v0.5.0.\n",
      "WARNING:mrjob.step:MRJobStep has been renamed to MRStep. The old name will be removed in v0.5.0.\n",
      "WARNING:mrjob.step:MRJobStep has been renamed to MRStep. The old name will be removed in v0.5.0.\n",
      "WARNING:mrjob.step:MRJobStep has been renamed to MRStep. The old name will be removed in v0.5.0.\n",
      "WARNING:mrjob.step:MRJobStep has been renamed to MRStep. The old name will be removed in v0.5.0.\n",
      "WARNING:mrjob.step:MRJobStep has been renamed to MRStep. The old name will be removed in v0.5.0.\n",
      "WARNING:mrjob.step:MRJobStep has been renamed to MRStep. The old name will be removed in v0.5.0.\n",
      "WARNING:mrjob.step:MRJobStep has been renamed to MRStep. The old name will be removed in v0.5.0.\n",
      "WARNING:mrjob.step:MRJobStep has been renamed to MRStep. The old name will be removed in v0.5.0.\n",
      "WARNING:mrjob.step:MRJobStep has been renamed to MRStep. The old name will be removed in v0.5.0.\n",
      "WARNING:mrjob.step:MRJobStep has been renamed to MRStep. The old name will be removed in v0.5.0.\n",
      "WARNING:mrjob.step:MRJobStep has been renamed to MRStep. The old name will be removed in v0.5.0.\n",
      "WARNING:mrjob.step:MRJobStep has been renamed to MRStep. The old name will be removed in v0.5.0.\n",
      "WARNING:mrjob.step:MRJobStep has been renamed to MRStep. The old name will be removed in v0.5.0.\n",
      "WARNING:mrjob.step:MRJobStep has been renamed to MRStep. The old name will be removed in v0.5.0.\n",
      "WARNING:mrjob.step:MRJobStep has been renamed to MRStep. The old name will be removed in v0.5.0.\n",
      "WARNING:mrjob.step:MRJobStep has been renamed to MRStep. The old name will be removed in v0.5.0.\n",
      "WARNING:mrjob.step:MRJobStep has been renamed to MRStep. The old name will be removed in v0.5.0.\n",
      "WARNING:mrjob.step:MRJobStep has been renamed to MRStep. The old name will be removed in v0.5.0.\n",
      "WARNING:mrjob.step:MRJobStep has been renamed to MRStep. The old name will be removed in v0.5.0.\n",
      "WARNING:mrjob.step:MRJobStep has been renamed to MRStep. The old name will be removed in v0.5.0.\n",
      "WARNING:mrjob.step:MRJobStep has been renamed to MRStep. The old name will be removed in v0.5.0.\n",
      "WARNING:mrjob.step:MRJobStep has been renamed to MRStep. The old name will be removed in v0.5.0.\n",
      "WARNING:mrjob.step:MRJobStep has been renamed to MRStep. The old name will be removed in v0.5.0.\n",
      "WARNING:mrjob.step:MRJobStep has been renamed to MRStep. The old name will be removed in v0.5.0.\n",
      "WARNING:mrjob.step:MRJobStep has been renamed to MRStep. The old name will be removed in v0.5.0.\n",
      "WARNING:mrjob.step:MRJobStep has been renamed to MRStep. The old name will be removed in v0.5.0.\n",
      "WARNING:mrjob.step:MRJobStep has been renamed to MRStep. The old name will be removed in v0.5.0.\n",
      "WARNING:mrjob.step:MRJobStep has been renamed to MRStep. The old name will be removed in v0.5.0.\n",
      "WARNING:mrjob.step:MRJobStep has been renamed to MRStep. The old name will be removed in v0.5.0.\n",
      "WARNING:mrjob.step:MRJobStep has been renamed to MRStep. The old name will be removed in v0.5.0.\n",
      "WARNING:mrjob.step:MRJobStep has been renamed to MRStep. The old name will be removed in v0.5.0.\n",
      "WARNING:mrjob.step:MRJobStep has been renamed to MRStep. The old name will be removed in v0.5.0.\n",
      "WARNING:mrjob.step:MRJobStep has been renamed to MRStep. The old name will be removed in v0.5.0.\n",
      "WARNING:mrjob.step:MRJobStep has been renamed to MRStep. The old name will be removed in v0.5.0.\n",
      "WARNING:mrjob.step:MRJobStep has been renamed to MRStep. The old name will be removed in v0.5.0.\n",
      "WARNING:mrjob.step:MRJobStep has been renamed to MRStep. The old name will be removed in v0.5.0.\n",
      "WARNING:mrjob.step:MRJobStep has been renamed to MRStep. The old name will be removed in v0.5.0.\n",
      "WARNING:mrjob.step:MRJobStep has been renamed to MRStep. The old name will be removed in v0.5.0.\n",
      "WARNING:mrjob.step:MRJobStep has been renamed to MRStep. The old name will be removed in v0.5.0.\n",
      "WARNING:mrjob.step:MRJobStep has been renamed to MRStep. The old name will be removed in v0.5.0.\n",
      "WARNING:mrjob.step:MRJobStep has been renamed to MRStep. The old name will be removed in v0.5.0.\n",
      "WARNING:mrjob.step:MRJobStep has been renamed to MRStep. The old name will be removed in v0.5.0.\n",
      "WARNING:mrjob.step:MRJobStep has been renamed to MRStep. The old name will be removed in v0.5.0.\n",
      "WARNING:mrjob.step:MRJobStep has been renamed to MRStep. The old name will be removed in v0.5.0.\n",
      "WARNING:mrjob.step:MRJobStep has been renamed to MRStep. The old name will be removed in v0.5.0.\n",
      "WARNING:mrjob.step:MRJobStep has been renamed to MRStep. The old name will be removed in v0.5.0.\n",
      "WARNING:mrjob.step:MRJobStep has been renamed to MRStep. The old name will be removed in v0.5.0.\n",
      "WARNING:mrjob.step:MRJobStep has been renamed to MRStep. The old name will be removed in v0.5.0.\n",
      "WARNING:mrjob.step:MRJobStep has been renamed to MRStep. The old name will be removed in v0.5.0.\n",
      "WARNING:mrjob.step:MRJobStep has been renamed to MRStep. The old name will be removed in v0.5.0.\n",
      "WARNING:mrjob.step:MRJobStep has been renamed to MRStep. The old name will be removed in v0.5.0.\n",
      "WARNING:mrjob.step:MRJobStep has been renamed to MRStep. The old name will be removed in v0.5.0.\n",
      "WARNING:mrjob.step:MRJobStep has been renamed to MRStep. The old name will be removed in v0.5.0.\n",
      "WARNING:mrjob.step:MRJobStep has been renamed to MRStep. The old name will be removed in v0.5.0.\n",
      "WARNING:mrjob.step:MRJobStep has been renamed to MRStep. The old name will be removed in v0.5.0.\n",
      "WARNING:mrjob.step:MRJobStep has been renamed to MRStep. The old name will be removed in v0.5.0.\n",
      "WARNING:mrjob.step:MRJobStep has been renamed to MRStep. The old name will be removed in v0.5.0.\n",
      "WARNING:mrjob.step:MRJobStep has been renamed to MRStep. The old name will be removed in v0.5.0.\n",
      "WARNING:mrjob.step:MRJobStep has been renamed to MRStep. The old name will be removed in v0.5.0.\n",
      "WARNING:mrjob.step:MRJobStep has been renamed to MRStep. The old name will be removed in v0.5.0.\n",
      "WARNING:mrjob.step:MRJobStep has been renamed to MRStep. The old name will be removed in v0.5.0.\n",
      "WARNING:mrjob.step:MRJobStep has been renamed to MRStep. The old name will be removed in v0.5.0.\n",
      "WARNING:mrjob.step:MRJobStep has been renamed to MRStep. The old name will be removed in v0.5.0.\n",
      "WARNING:mrjob.step:MRJobStep has been renamed to MRStep. The old name will be removed in v0.5.0.\n",
      "WARNING:mrjob.step:MRJobStep has been renamed to MRStep. The old name will be removed in v0.5.0.\n",
      "WARNING:mrjob.step:MRJobStep has been renamed to MRStep. The old name will be removed in v0.5.0.\n",
      "WARNING:mrjob.step:MRJobStep has been renamed to MRStep. The old name will be removed in v0.5.0.\n",
      "WARNING:mrjob.step:MRJobStep has been renamed to MRStep. The old name will be removed in v0.5.0.\n",
      "WARNING:mrjob.step:MRJobStep has been renamed to MRStep. The old name will be removed in v0.5.0.\n",
      "WARNING:mrjob.step:MRJobStep has been renamed to MRStep. The old name will be removed in v0.5.0.\n",
      "WARNING:mrjob.step:MRJobStep has been renamed to MRStep. The old name will be removed in v0.5.0.\n",
      "WARNING:mrjob.step:MRJobStep has been renamed to MRStep. The old name will be removed in v0.5.0.\n",
      "WARNING:mrjob.step:MRJobStep has been renamed to MRStep. The old name will be removed in v0.5.0.\n",
      "WARNING:mrjob.step:MRJobStep has been renamed to MRStep. The old name will be removed in v0.5.0.\n",
      "WARNING:mrjob.step:MRJobStep has been renamed to MRStep. The old name will be removed in v0.5.0.\n",
      "WARNING:mrjob.step:MRJobStep has been renamed to MRStep. The old name will be removed in v0.5.0.\n",
      "WARNING:mrjob.step:MRJobStep has been renamed to MRStep. The old name will be removed in v0.5.0.\n",
      "WARNING:mrjob.step:MRJobStep has been renamed to MRStep. The old name will be removed in v0.5.0.\n",
      "WARNING:mrjob.step:MRJobStep has been renamed to MRStep. The old name will be removed in v0.5.0.\n",
      "WARNING:mrjob.step:MRJobStep has been renamed to MRStep. The old name will be removed in v0.5.0.\n",
      "WARNING:mrjob.step:MRJobStep has been renamed to MRStep. The old name will be removed in v0.5.0.\n",
      "WARNING:mrjob.step:MRJobStep has been renamed to MRStep. The old name will be removed in v0.5.0.\n",
      "WARNING:mrjob.step:MRJobStep has been renamed to MRStep. The old name will be removed in v0.5.0.\n",
      "WARNING:mrjob.step:MRJobStep has been renamed to MRStep. The old name will be removed in v0.5.0.\n",
      "WARNING:mrjob.step:MRJobStep has been renamed to MRStep. The old name will be removed in v0.5.0.\n",
      "WARNING:mrjob.step:MRJobStep has been renamed to MRStep. The old name will be removed in v0.5.0.\n",
      "WARNING:mrjob.step:MRJobStep has been renamed to MRStep. The old name will be removed in v0.5.0.\n",
      "WARNING:mrjob.step:MRJobStep has been renamed to MRStep. The old name will be removed in v0.5.0.\n",
      "WARNING:mrjob.step:MRJobStep has been renamed to MRStep. The old name will be removed in v0.5.0.\n",
      "WARNING:mrjob.step:MRJobStep has been renamed to MRStep. The old name will be removed in v0.5.0.\n",
      "WARNING:mrjob.step:MRJobStep has been renamed to MRStep. The old name will be removed in v0.5.0.\n",
      "WARNING:mrjob.step:MRJobStep has been renamed to MRStep. The old name will be removed in v0.5.0.\n",
      "WARNING:mrjob.step:MRJobStep has been renamed to MRStep. The old name will be removed in v0.5.0.\n",
      "WARNING:mrjob.step:MRJobStep has been renamed to MRStep. The old name will be removed in v0.5.0.\n",
      "WARNING:mrjob.step:MRJobStep has been renamed to MRStep. The old name will be removed in v0.5.0.\n",
      "WARNING:mrjob.step:MRJobStep has been renamed to MRStep. The old name will be removed in v0.5.0.\n",
      "WARNING:mrjob.step:MRJobStep has been renamed to MRStep. The old name will be removed in v0.5.0.\n",
      "WARNING:mrjob.step:MRJobStep has been renamed to MRStep. The old name will be removed in v0.5.0.\n",
      "WARNING:mrjob.step:MRJobStep has been renamed to MRStep. The old name will be removed in v0.5.0.\n",
      "WARNING:mrjob.step:MRJobStep has been renamed to MRStep. The old name will be removed in v0.5.0.\n",
      "WARNING:mrjob.step:MRJobStep has been renamed to MRStep. The old name will be removed in v0.5.0.\n",
      "WARNING:mrjob.step:MRJobStep has been renamed to MRStep. The old name will be removed in v0.5.0.\n",
      "WARNING:mrjob.step:MRJobStep has been renamed to MRStep. The old name will be removed in v0.5.0.\n",
      "WARNING:mrjob.step:MRJobStep has been renamed to MRStep. The old name will be removed in v0.5.0.\n",
      "WARNING:mrjob.step:MRJobStep has been renamed to MRStep. The old name will be removed in v0.5.0.\n",
      "WARNING:mrjob.step:MRJobStep has been renamed to MRStep. The old name will be removed in v0.5.0.\n",
      "WARNING:mrjob.step:MRJobStep has been renamed to MRStep. The old name will be removed in v0.5.0.\n",
      "WARNING:mrjob.step:MRJobStep has been renamed to MRStep. The old name will be removed in v0.5.0.\n",
      "WARNING:mrjob.step:MRJobStep has been renamed to MRStep. The old name will be removed in v0.5.0.\n",
      "WARNING:mrjob.step:MRJobStep has been renamed to MRStep. The old name will be removed in v0.5.0.\n",
      "WARNING:mrjob.step:MRJobStep has been renamed to MRStep. The old name will be removed in v0.5.0.\n",
      "WARNING:mrjob.step:MRJobStep has been renamed to MRStep. The old name will be removed in v0.5.0.\n",
      "WARNING:mrjob.step:MRJobStep has been renamed to MRStep. The old name will be removed in v0.5.0.\n",
      "WARNING:mrjob.step:MRJobStep has been renamed to MRStep. The old name will be removed in v0.5.0.\n"
     ]
    }
   ],
   "source": [
    "# Running mrjob using a Hadoop Runner in local cluster for systems/unit test\n",
    "from mrjob_frequency_53 import frequency\n",
    "import os\n",
    "\n",
    "# Passing Hadoop Streaming parameters to:\n",
    "# partition by leftmost part of composite key\n",
    "# secodary sort by rightmost part of the same composite key\n",
    "\n",
    "mr_job = frequency(args=['testngram/'])\n",
    "#mr_job = longest_ngram(args=['small_grams.txt'])\n",
    "\n",
    "output_file = \"output_hw53_freq.txt\"\n",
    "try:\n",
    "    os.remove(output_file)\n",
    "except OSError:\n",
    "    pass\n",
    "\n",
    "with mr_job.make_runner() as runner, open(output_file, 'a') as f: \n",
    "    runner.run()\n",
    "    # stream_output: get access of the output \n",
    "    for line in runner.stream_output():\n",
    "        #print mr_job.parse_output_line(line)\n",
    "        f.write(str(line))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100\t\"Acquaintance\"\r\n",
      "100\t\"Aigina\"\r\n",
      "100\t\"Allowances\"\r\n",
      "100\t\"Antipater\"\r\n",
      "100\t\"Atalantis\"\r\n",
      "100\t\"BCD\"\r\n",
      "100\t\"Bembo\"\r\n",
      "100\t\"Bibelstudien\"\r\n",
      "100\t\"Bodleiana\"\r\n",
      "100\t\"Cloete\"\r\n"
     ]
    }
   ],
   "source": [
    "!head -10 output_hw53_freq.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING: Remote list is empty.\n",
      "Got unexpected keyword arguments: ssh_tunnel\n",
      "no configs found; falling back on auto-configuration\n",
      "no configs found; falling back on auto-configuration\n",
      "inferring aws_region from scratch bucket's region (us-west-1)\n",
      "using s3://mrjob-0465390d52fc9db7/tmp/ as our scratch dir on S3\n",
      "creating tmp directory /var/folders/zs/k144hqks281fbt0x68c_zj9m0000gp/T/mrjob_frequency_53.JingXu.20160216.234515.019133\n",
      "writing master bootstrap script to /var/folders/zs/k144hqks281fbt0x68c_zj9m0000gp/T/mrjob_frequency_53.JingXu.20160216.234515.019133/b.py\n"
     ]
    }
   ],
   "source": [
    "!s3cmd rm --recursive s3://w261jing/hw5/frequency53/\n",
    "!python mrjob_frequency_53.py -r emr s3://filtered-5grams --output-dir=s3://w261jing/hw5/frequency53 --no-output --no-strict-protocol"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "!s3cmd get s3://w261jing/frequency53/part* frequency_out.txt\n",
    "!head frequency_out.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "no configs found; falling back on auto-configuration\n",
      "no configs found; falling back on auto-configuration\n",
      "inferring aws_region from scratch bucket's region (us-west-1)\n",
      "using s3://mrjob-0465390d52fc9db7/tmp/ as our scratch dir on S3\n",
      "Terminated job flow j-3BFS6FTHTMWLP\n"
     ]
    }
   ],
   "source": [
    "!python -m mrjob.tools.emr.terminate_job_flow j-3BFS6FTHTMWLP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "## Most/Least Dense\n",
    "#Most/Least densely appearing words (count/pages_count) sorted in decreasing order of relative frequency \n",
    "#(Hint: save to PART-000* and take the head -n 1000)\n",
    "\n",
    "#(ngram) \\t (count) \\t (pages_count) \\t (books_count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing mrjob_density_53.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile mrjob_density_53.py\n",
    "#!/usr/bin/python\n",
    "## mrjob_density_53.py\n",
    "## Author: Angela Gunn & Jing Xu\n",
    "## Description:Find the density of the words in 5gram\n",
    "\n",
    "from mrjob.job import MRJob\n",
    "from mrjob.step import MRStep\n",
    "from mrjob.compat import get_jobconf_value\n",
    "\n",
    "class density_53(MRJob):\n",
    "\n",
    "    \n",
    "    def steps(self):\n",
    "        return [MRStep(mapper=self.mapper,\n",
    "                         reducer=self.reducer, \n",
    "                   jobconf={\n",
    "                    \"mapred.map.tasks\":16,\n",
    "                    \"mapred.reduce.tasks\":8\n",
    "                    }\n",
    "                         ),\n",
    "               MRStep(mapper=self.mapper_max_min,\n",
    "                        reducer=self.reducer_max_min,\n",
    "                   jobconf={\n",
    "                            'mapred.output.key.comparator.class': 'org.apache.hadoop.mapred.lib.KeyFieldBasedComparator',\n",
    "                            'mapred.text.key.comparator.options': '-k1,1rn',\n",
    "                            \"mapred.map.tasks\":4,\n",
    "                            \"mapred.reduce.tasks\":1\n",
    "                            }\n",
    "                     )]\n",
    "        \n",
    "    def mapper(self, _, line):\n",
    "        #output the density (count / pages)\n",
    "        cell = line.split('\\t')\n",
    "        words = cell[0].split()\n",
    "        density = round((int(cell[1]) * 1.0 / int(cell[2])), 3)\n",
    "        for w in words:\n",
    "            yield w.lower(), density\n",
    "            \n",
    "    def combiner(self, unigram, densities):\n",
    "        #combine\n",
    "        densities = [d for d in densities]\n",
    "        yield unigram, min(densities) \n",
    "        yield unigram, max(densities)\n",
    "        \n",
    "    def reducer(self, unigram, densities):\n",
    "        #combine\n",
    "        densities = [d for d in densities]\n",
    "        yield unigram, min(densities)\n",
    "        yield unigram, max(densities)\n",
    "        \n",
    "    def mapper_max_min(self, unigram, density):\n",
    "        #output with density first so grouping can happen\n",
    "        yield density, unigram\n",
    "        \n",
    "    def reducer_max_min(self, density, unigrams):\n",
    "        #final output\n",
    "        for unigram in unigrams:\n",
    "            yield density, unigram\n",
    "            \n",
    "        \n",
    "if __name__ == '__main__':\n",
    "    density_53.run()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING: Module python-magic is not available. Guessing MIME types based on file extensions.\n",
      "upload: 'mrjob_density_53.py' -> 's3://w261jing/mrjob_density_53.py'  [1 of 1]\n",
      " 1169 of 1169   100% in    0s     2.50 kB/s  done\n",
      "upload: 'mrjob_density_53.py' -> 's3://w261jing/mrjob_density_53.py'  [1 of 1]\n",
      " 1169 of 1169   100% in    0s    11.77 kB/s  done\n",
      "upload: 'mrjob_density_53.py' -> 's3://w261jing/mrjob_density_53.py'  [1 of 1]\n",
      " 1169 of 1169   100% in    0s     8.80 kB/s  done\n"
     ]
    }
   ],
   "source": [
    "!s3cmd put FILE mrjob_density_53.py s3://w261jing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Running mrjob using a Hadoop Runner in local cluster for systems/unit test\n",
    "from mrjob_density_53 import density_53\n",
    "import os\n",
    "\n",
    "# Passing Hadoop Streaming parameters to:\n",
    "# partition by leftmost part of composite key\n",
    "# secodary sort by rightmost part of the same composite key\n",
    "\n",
    "JOB_CONFIG = {'mapred.output.key.comparator.class':\n",
    "      'org.apache.hadoop.mapred.lib.KeyFieldBasedComparator',\n",
    "  'mapred.text.key.comparator.options': '-k1 1nr'}\n",
    "\n",
    "\n",
    "mr_job = density_53(args=['testngram/'])\n",
    "#mr_job = density_53(args=['small_grams.txt'])\n",
    "#mr_job.jobconf(JOB_CONFIG)\n",
    "\n",
    "output_file = \"output_hw53_density.txt\"\n",
    "try:\n",
    "    os.remove(output_file)\n",
    "except OSError:\n",
    "    pass\n",
    "\n",
    "with mr_job.make_runner() as runner, open(output_file, 'a') as f: \n",
    "    runner.run()\n",
    "    # stream_output: get access of the output \n",
    "    for line in runner.stream_output():\n",
    "        #print mr_job.parse_output_line(line)\n",
    "        f.write(str(line))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "!s3cmd rm --recursive s3://w261jing/hw5/density53/\n",
    "!python mrjob_density_53.py -r emr s3://filtered-5grams/ --output-dir=s3://w261jing/hw5/density53 --no-output --no-strict-protocol    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "!s3cmd get s3://w261jing/hw5/density53/part* density_out.txt\n",
    "!head density_out.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Distribution of 5-gram sizes (counts) sorted in decreasing order of relative frequency. (Hint: save to PART-000* and take the head -n 1000) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing mrjob_distribution_53.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile mrjob_distribution_53.py\n",
    "#!/usr/bin/python\n",
    "## mrjob_distribution_53.py\n",
    "## Author: Angela Gunn & Jing Xu\n",
    "## Description: Distribution of 5gram lengths\n",
    "\n",
    "from mrjob.job import MRJob\n",
    "from mrjob.step import MRStep\n",
    "from mrjob.compat import get_jobconf_value\n",
    "\n",
    "class distribution_53(MRJob):\n",
    "    \n",
    "    def steps(self):\n",
    "        return [MRStep(mapper=self.mapper,\n",
    "                       combiner = self.combiner,\n",
    "                         reducer=self.reducer, \n",
    "                   jobconf={\n",
    "                            'mapred.output.key.comparator.class': 'org.apache.hadoop.mapred.lib.KeyFieldBasedComparator',\n",
    "                            'mapred.text.key.comparator.options': '-k1,1rn',\n",
    "                            \"mapred.map.tasks\":8,\n",
    "                            \"mapred.reduce.tasks\":1\n",
    "                            }\n",
    "                     )]\n",
    "        \n",
    "    def mapper(self, _, line):\n",
    "        #get length, and count = 1\n",
    "        cell = line.split('\\t')\n",
    "        ngram_length = len(cell[0])\n",
    "        count = int(cell[1])\n",
    "        yield ngram_length, count\n",
    "            \n",
    "    def combiner(self, length, count):\n",
    "        #combine\n",
    "        count = sum(count)\n",
    "        yield length, count\n",
    "        \n",
    "    def reducer(self, length, count):\n",
    "        #combine\n",
    "        count = sum(count)\n",
    "        yield length, count\n",
    "        \n",
    "if __name__ == '__main__':\n",
    "    distribution_53.run()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING: Module python-magic is not available. Guessing MIME types based on file extensions.\n",
      "upload: 'mrjob_distribution_53.py' -> 's3://w261jing/mrjob_distribution_53.py'  [1 of 1]\n",
      " 994 of 994   100% in    0s  1879.06 B/s  done\n",
      "upload: 'mrjob_distribution_53.py' -> 's3://w261jing/mrjob_distribution_53.py'  [1 of 1]\n",
      " 994 of 994   100% in    0s     9.65 kB/s  done\n",
      "upload: 'mrjob_distribution_53.py' -> 's3://w261jing/mrjob_distribution_53.py'  [1 of 1]\n",
      " 994 of 994   100% in    0s     7.32 kB/s  done\n"
     ]
    }
   ],
   "source": [
    "!s3cmd put FILE mrjob_distribution_53.py s3://w261jing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Running mrjob using a Hadoop Runner in local cluster for systems/unit test\n",
    "from mrjob_distribution_53 import distribution_53\n",
    "import os\n",
    "\n",
    "\n",
    "#mr_job = distribution_53(args=['HW5/'])\n",
    "mr_job = distribution_53(args=['testngram/'])\n",
    "\n",
    "output_file = \"output_hw53_distribution.txt\"\n",
    "try:\n",
    "    os.remove(output_file)\n",
    "except OSError:\n",
    "    pass\n",
    "\n",
    "with mr_job.make_runner() as runner, open(output_file, 'a') as f: \n",
    "    runner.run()\n",
    "    # stream_output: get access of the output \n",
    "    for line in runner.stream_output():\n",
    "        #print mr_job.parse_output_line(line)\n",
    "        f.write(str(line))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "!s3cmd rm --recursive s3://w261jing/hw5/distribution53/\n",
    "!python mrjob_distribution_53.py -r emr s3://filtered-5grams/ --output-dir=s3://w261jing/hw5/distribution53 --no-output --no-strict-protocol    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "!s3cmd get s3://w261jing/hw5/distribution53/part* distribution_out.txt\n",
    "!echo \"------------------------------\"\n",
    "!echo \"ngram length   count of length\"\n",
    "!echo \"------------------------------\"\n",
    "!head distribution_out.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.rcParams['figure.figsize'] = 16, 8  # plotsize \n",
    "\n",
    "df = pd.read_csv('HW5/distribution_out.txt',sep='\\t',header=None)\n",
    "df.columns = ['length','frequency']\n",
    "df = df.sort('length')\n",
    "df = df.set_index('length')\n",
    "my_plot = df.plot(kind='bar',legend=None,title=\"5-gram character length distribution\")\n",
    "my_plot.set_xlabel(\"5-gram length\")\n",
    "my_plot.set_ylabel(\"frequency\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## HW 5.4\n",
    "\n",
    "In this part of the assignment we will focus on developing methods\n",
    "for detecting synonyms, using the Google 5-grams dataset. To accomplish\n",
    "this you must script two main tasks using MRJob:\n",
    "\n",
    "**(1) Build stripes of word co-ocurrence for the top 10,000 using the words ranked from 9001,-10,000 as a basis\n",
    "most frequently appearing words across the entire set of 5-grams,\n",
    "and output to a file in your bucket on s3 (bigram analysis, though the words are non-contiguous).**\n",
    "\n",
    "**==Design notes for (1)==**\n",
    "For this task you will be able to modify the pattern we used in HW 3.2\n",
    "(feel free to use the solution as reference). To total the word counts \n",
    "across the 5-grams, output the support from the mappers using the total \n",
    "order inversion pattern:\n",
    "\n",
    "<*word,count>\n",
    "\n",
    "to ensure that the support arrives before the cooccurrences.\n",
    "\n",
    "In addition to ensuring the determination of the total word counts,\n",
    "the mapper must also output co-occurrence counts for the pairs of\n",
    "words inside of each 5-gram. Treat these words as a basket,\n",
    "as we have in HW 3, but count all stripes or pairs in both orders,\n",
    "i.e., count both orderings: (word1,word2), and (word2,word1), to preserve\n",
    "symmetry in our output for (2).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%%writefile createtest.py\n",
    "\n",
    "with open('SYSTEMS_TEST_DATASET.txt','w') as f:\n",
    "    f.write('DocA\\t{\"X\":20,\"Y\":30,\"Z\":5}\\n')\n",
    "    f.write('DocB\\t{\"X\":100,\"Y\":20}\\n')\n",
    "    f.write('DocC\\t{\"M\":5,\"N\":20,\"Z\":5}\\n')\n",
    "\n",
    "with open('SYSTEMS_TEST_DATASET_freq.txt','w') as f:\n",
    "    f.write('120\\t\"X\"\\n')\n",
    "    f.write('150\\t\"Y\"\\n')\n",
    "    f.write('10\\t\"Z\"\\n')\n",
    "    f.write('5\\t\"M\"\\n')\n",
    "    f.write('20\\t\"N\"\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "!python createtest.py\n",
    "!cat SYSTEMS_TEST_DATASET.txt\n",
    "!cat SYSTEMS_TEST_DATASET_freq.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "!cat HW5/frequency_out.txt | sed -n 9001,10000p > HW5/topwords_touse.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#get top 1000 words.\n",
    "#build stripes of co-occurrence on ALL 5-grams  word [co1, co2, co3]\n",
    "#this will be output from mapper for each \n",
    "!head HW5/topwords_touse.txt\n",
    "!wc -l HW5/topwords_touse.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting mrjob_bigram_occurrence.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile mrjob_bigram_occurrence.py\n",
    "#!/usr/bin/python\n",
    "## inverse_index.py\n",
    "## Author: Angela Gunn & Jing Xu\n",
    "## Description: Inverses an Index.\n",
    "\n",
    "\n",
    "from mrjob.job import MRJob\n",
    "from mrjob.step import MRStep\n",
    "import csv\n",
    "from sets import Set\n",
    "import ast\n",
    "import re\n",
    "\n",
    "WORD_RE = re.compile(r\"[A-Za-z0-9]+\")\n",
    "\n",
    "class bigram_occurrence(MRJob):\n",
    "    \n",
    "    doc_dict={} #global list\n",
    "    \n",
    "    def steps(self):\n",
    "        return [MRStep(mapper_init = self.mapper_init,\n",
    "                       mapper=self.mapper_main,\n",
    "                     combiner=self.combiner,\n",
    "                      reducer=self.reducer, \n",
    "                   jobconf={\n",
    "                            'mapred.output.key.comparator.class': 'org.apache.hadoop.mapred.lib.KeyFieldBasedComparator',\n",
    "                            'mapred.text.key.comparator.options': '-k1,1',\n",
    "                            \"mapred.map.tasks\":32,\n",
    "                            \"mapred.reduce.tasks\":16\n",
    "                            })]\n",
    "    \n",
    "    def mapper_init(self):\n",
    "        #load unigrams\n",
    "        self.unigrams = {}\n",
    "        with open('topwords','r') as f:\n",
    "            for line in f:\n",
    "                cells = line.strip().split('\\t')\n",
    "                word = cells[1].replace('\"','').strip()\n",
    "                self.unigrams[word] = int(cells[0])\n",
    "                yield \"*\"+word, int(cells[0])\n",
    "                                \n",
    "    def mapper_main(self, _, line):        \n",
    "        cell = line.strip().split('\\t')\n",
    "        words = WORD_RE.findall(cell[0])\n",
    "        # Filter 5-grams to only those in list\n",
    "        words = [w for w in words if w in self.unigrams.keys()]        \n",
    "        w_len = len(words)\n",
    "        for i in range(0, w_len): #for each word\n",
    "            key = words[i]\n",
    "            H = {}\n",
    "            for j in xrange(0, w_len): #for each word after this\n",
    "                w = words[j]\n",
    "                if key != w: H[w] = H.get(w,0) + 1\n",
    "            #emit \n",
    "            if len(H) > 0: yield key, H\n",
    "                      \n",
    "    def combiner(self, key, stripes):\n",
    "        dic = {}\n",
    "        key = key.replace('\"','')\n",
    "        if key[0] == '*':\n",
    "            total = sum(stripes)\n",
    "            yield key, total\n",
    "        else:\n",
    "            for s in stripes:\n",
    "                for k, v in s.iteritems():\n",
    "                    k = k.replace('\"','')\n",
    "                    dic[k] = dic.get(k,0) + int(v)\n",
    "            yield key, dic\n",
    "\n",
    "    def reducer(self, key, stripes):\n",
    "        dic = {}\n",
    "        key = key.replace('\"','')\n",
    "        if key[0] == '*':\n",
    "            total = sum(stripes)\n",
    "            yield key, total\n",
    "        else:\n",
    "            for s in stripes:\n",
    "                for k, v in s.iteritems():\n",
    "                    k = k.replace('\"','')\n",
    "                    dic[k] = dic.get(k,0) + int(v)\n",
    "            yield key, dic\n",
    "        \n",
    "\n",
    "        \n",
    "if __name__ == '__main__':\n",
    "    bigram_occurrence.run()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING: Module python-magic is not available. Guessing MIME types based on file extensions.\n",
      "upload: 'mrjob_bigram_occurrence.py' -> 's3://w261jing/mrjob_bigram_occurrence.py'  [1 of 1]\n",
      " 2615 of 2615   100% in    0s    13.85 kB/s  done\n"
     ]
    }
   ],
   "source": [
    "!s3cmd put FILE mrjob_bigram_occurrence.py s3://w261jing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:mrjob.runner:\n",
      "WARNING:mrjob.runner:PLEASE NOTE: Starting in mrjob v0.5.0, protocols will be strict by default. It's recommended you run your job with --strict-protocols or set up mrjob.conf as described at https://pythonhosted.org/mrjob/whats-new.html#ready-for-strict-protocols\n",
      "WARNING:mrjob.runner:\n"
     ]
    }
   ],
   "source": [
    "# Running mrjob using a Hadoop Runner in local cluster\n",
    "from mrjob_bigram_occurrence import bigram_occurrence\n",
    "import os\n",
    "\n",
    "mr_job = bigram_occurrence(args=['ngramstest.txt', '--file', '1000grams.txt']) #use ngramstest file to test python script\n",
    "\n",
    "output_file = \"bigram_test_54.out\"\n",
    "try:\n",
    "    os.remove(output_file)\n",
    "except OSError:\n",
    "    pass\n",
    "\n",
    "with mr_job.make_runner() as runner, open(output_file, 'a') as f: \n",
    "    runner.run()\n",
    "    # stream_output: get access of the output \n",
    "    for line in runner.stream_output():\n",
    "        #print mr_job.parse_output_line(line)\n",
    "        f.write(str(line))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\"*aa\"\t63\n",
      "\"*aachen\"\t7\n",
      "\"*aan\"\t9\n",
      "\"*aba\"\t6\n",
      "\"*abate\"\t87\n",
      "\"*abated\"\t92\n",
      "\"*abbas\"\t8\n",
      "\"*abbreviated\"\t72\n",
      "\"*abbreviation\"\t55\n",
      "\"*abbreviations\"\t97\n",
      "\"wrought\"\t{\"me\": 1, \"be\": 2, \"great\": 1, \"lived\": 1, \"that\": 1, \"well\": 1, \"could\": 1, \"upon\": 1, \"most\": 1, \"between\": 1, \"is\": 1, \"must\": 2}\n"
     ]
    }
   ],
   "source": [
    "!head -10 bigram_test_54.out\n",
    "!tail -1 bigram_test_54.out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Got unexpected keyword arguments: ssh_tunnel\n",
      "inferring aws_region from scratch bucket's region (us-west-1)\n",
      "using s3://mrjob-0465390d52fc9db7/tmp/ as our scratch dir on S3\n",
      "creating tmp directory /var/folders/zs/k144hqks281fbt0x68c_zj9m0000gp/T/mrjob_bigram_occurrence.JingXu.20160216.171800.722951\n",
      "writing master bootstrap script to /var/folders/zs/k144hqks281fbt0x68c_zj9m0000gp/T/mrjob_bigram_occurrence.JingXu.20160216.171800.722951/b.py\n",
      "\n",
      "PLEASE NOTE: Starting in mrjob v0.5.0, protocols will be strict by default. It's recommended you run your job with --strict-protocols or set up mrjob.conf as described at https://pythonhosted.org/mrjob/whats-new.html#ready-for-strict-protocols\n",
      "\n",
      "Copying non-input files into s3://mrjob-0465390d52fc9db7/tmp/mrjob_bigram_occurrence.JingXu.20160216.171800.722951/files/\n",
      "Waiting 5.0s for S3 eventual consistency\n",
      "Creating Elastic MapReduce job flow\n",
      "Can't access IAM API, trying default instance profile: EMR_EC2_DefaultRole\n",
      "Can't access IAM API, trying default service role: EMR_DefaultRole\n",
      "Job flow created with ID: j-8NTJXI2WHLT8\n",
      "Created new job flow j-8NTJXI2WHLT8\n",
      "Job launched 30.3s ago, status STARTING: Provisioning Amazon EC2 capacity\n",
      "Job launched 60.5s ago, status STARTING: Provisioning Amazon EC2 capacity\n",
      "Job launched 90.8s ago, status STARTING: Provisioning Amazon EC2 capacity\n",
      "Job launched 121.2s ago, status STARTING: Provisioning Amazon EC2 capacity\n",
      "Job launched 151.5s ago, status STARTING: Configuring cluster software\n",
      "Job launched 181.8s ago, status BOOTSTRAPPING: Running bootstrap actions\n",
      "Job launched 212.1s ago, status BOOTSTRAPPING: Running bootstrap actions\n",
      "Job launched 242.4s ago, status RUNNING: Running step (mrjob_bigram_occurrence.JingXu.20160216.171800.722951: Step 1 of 1)\n",
      "Opening ssh tunnel to Hadoop job tracker\n",
      "Connect to job tracker at: http://localhost:40222/jobtracker.jsp\n",
      "Job launched 273.7s ago, status RUNNING: Running step (mrjob_bigram_occurrence.JingXu.20160216.171800.722951: Step 1 of 1)\n",
      "Unable to load progress from job tracker\n",
      "Job launched 304.0s ago, status RUNNING: Running step (mrjob_bigram_occurrence.JingXu.20160216.171800.722951: Step 1 of 1)\n"
     ]
    }
   ],
   "source": [
    "!s3cmd rm --recursive s3://w261jing/hw5/bigram54/\n",
    "!python mrjob_bigram_occurrence.py -r emr s3://filtered-5grams/ --output-dir=s3://w261jing/hw5/bigram54 --no-output --no-strict-protocol    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**(2) Using two (symmetric) comparison methods of your choice \n",
    "(e.g., correlations, distances, similarities), pairwise compare \n",
    "all stripes (vectors), and output to a file in your bucket on s3.**\n",
    "\n",
    "**==Design notes for (2)==**  \n",
    "For this task you will have to determine a method of comparison.\n",
    "Here are a few that you might consider:\n",
    "\n",
    "- Jaccard\n",
    "- Cosine similarity\n",
    "- Spearman correlation\n",
    "- Euclidean distance\n",
    "- Taxicab (Manhattan) distance\n",
    "- Shortest path graph distance (a graph, because our data is symmetric!)\n",
    "- Pearson correlation\n",
    "- Kendall correlation\n",
    "...\n",
    "\n",
    "However, be cautioned that some comparison methods are more difficult to\n",
    "parallelize than others, and do not perform more associations than is necessary, \n",
    "since your choice of association will be symmetric.\n",
    "\n",
    "Please use the inverted index (discussed in live session #5) based pattern to compute the pairwise (term-by-term) similarity matrix."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting mrjob_Cosine_Inverted_Index.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile mrjob_Cosine_Inverted_Index.py\n",
    "#!/usr/bin/python\n",
    "## mrjob_Cosine_Inverted_Index.py\n",
    "## Author: Angela Gunn & Jing Xu\n",
    "## Description: Finds cosine scores for inverted index\n",
    "\n",
    "\n",
    "from mrjob.job import MRJob\n",
    "from mrjob.step import MRStep\n",
    "import csv\n",
    "from sets import Set\n",
    "import ast, json\n",
    "import math\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#then invert it: \n",
    "#   w1 {wordA:1/LA, wordB:1/LB}\n",
    "#   w2 {wordA:1/LA, wordB:1/LB}\n",
    "#   w3 {wordA:1/LA}\n",
    "#cosine(wordA, wordB) = 1/LA*1/LB + 1/LA*1/LB = 1/LA*0\n",
    "\n",
    "\n",
    "class Cosine_Inverted_Index(MRJob):\n",
    "\n",
    "    global_doc_dict = {}\n",
    "    def steps(self):\n",
    "        return [\n",
    "            MRStep(mapper=self.mapper, reducer= self.reducer,jobconf={\n",
    "                    \"mapred.map.tasks\":16,\n",
    "                    \"mapred.reduce.tasks\":8\n",
    "                    }\n",
    "                  ),\n",
    "            MRStep(mapper=self.mapper2 ,combiner=self.combiner2, reducer=self.reducer2,\n",
    "                   jobconf={\n",
    "                    \"mapred.map.tasks\":8,\n",
    "                    \"mapred.reduce.tasks\":4\n",
    "                    }\n",
    "                  )\n",
    "               ]\n",
    "    \n",
    "    def mapper(self, _, line):\n",
    "        total_sqrt = 0\n",
    "        total_sq_cnt= 0 \n",
    "        key,terms = line.strip().split('\\t')\n",
    "        key = key.replace('\"', '')\n",
    "        if key[0] != '*':\n",
    "            docs = eval(terms)\n",
    "            #normalise the counts for cosine similarity\n",
    "            for word, count in docs.iteritems():\n",
    "                total_sq_cnt += count**2\n",
    "            total_sqrt = math.sqrt(total_sq_cnt)\n",
    "            for doc,count in docs.iteritems():\n",
    "                yield doc,(key, 1.0*count/total_sqrt)\n",
    "    \n",
    "    def reducer(self,key,value):\n",
    "        doc_list ={}\n",
    "        for doc,dist in value:\n",
    "            doc_list[doc]=dist\n",
    "        yield key, doc_list\n",
    "        \n",
    "    def mapper2(self,key,value):\n",
    "        keys = value.keys()\n",
    "        for key1 in keys:\n",
    "            for key2 in keys:\n",
    "                if(key1 == key2):\n",
    "                    continue\n",
    "                multiplied_keys = value[key1]*value[key2]\n",
    "                yield(key1,key2),multiplied_keys\n",
    "    \n",
    "    def combiner2(self,key,value):\n",
    "        yield key,sum(value)\n",
    "    \n",
    "    def reducer2(self,key,value):\n",
    "        yield key,sum(value)\n",
    "\n",
    "        \n",
    "if __name__ == '__main__':\n",
    "    Cosine_Inverted_Index.run()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "!s3cmd put FILE mrjob_Cosine_Inverted_Index.py s3://w261jing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Test with SYSTEMS_TEST_DATASET**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:mrjob.runner:\n",
      "WARNING:mrjob.runner:PLEASE NOTE: Starting in mrjob v0.5.0, protocols will be strict by default. It's recommended you run your job with --strict-protocols or set up mrjob.conf as described at https://pythonhosted.org/mrjob/whats-new.html#ready-for-strict-protocols\n",
      "WARNING:mrjob.runner:\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "13323\n"
     ]
    }
   ],
   "source": [
    "from mrjob_Cosine_Inverted_Index import Cosine_Inverted_Index\n",
    "import os\n",
    "\n",
    "#mr_job = Cosine_Inverted_Index(args=['HW5/54_out.txt'])\n",
    "mr_job = Cosine_Inverted_Index(args=['SYSTEMS_TEST_DATASET.txt'])\n",
    "\n",
    "output_file = \"SYSTEMS_TEST_COSINE.out\"\n",
    "try:\n",
    "    os.remove(output_file)\n",
    "except OSError:\n",
    "    pass\n",
    "\n",
    "with mr_job.make_runner() as runner, open(output_file, 'a') as f: \n",
    "    runner.run()\n",
    "    # stream_output: get access of the output \n",
    "    for line in runner.stream_output():\n",
    "        #print mr_job.parse_output_line(line)\n",
    "        f.write(str(line))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "!cat SYSTEMS_TEST_COSINE.out"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Test with 5-grams**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#code to make things public for future functions\n",
    "!s3cmd setacl s3://w261jing/hw5/bigram54/* --acl-public --recursive"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "!s3cmd rm --recursive s3://w261jing/hw5/cosine54/\n",
    "!python mrjob_Cosine_Inverted_Index.py -r emr s3://w261jing/hw5/bigram54/part* --output-dir=s3://w261jing/hw5/cosine54 --no-output --no-strict-protocol    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "!s3cmd get s3://w261jing/hw5/cosine54/part-00000 cosine54_out.txt\n",
    "\n",
    "!head cosine54_out.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting mrjob_Jaccard_Index.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile mrjob_Jaccard_Index.py\n",
    "#!/usr/bin/python\n",
    "## mrjob_Jaccard_Index.py\n",
    "## Author: Angela Gunn & Jing Xu\n",
    "## Description: Finds Jaccard scores for inverted index\n",
    "\n",
    "\n",
    "from mrjob.job import MRJob\n",
    "from mrjob.step import MRStep\n",
    "import csv\n",
    "from sets import Set\n",
    "import ast, json\n",
    "import math\n",
    "\n",
    "\n",
    "class Jaccard_Index(MRJob):\n",
    "    global_doc_dict = {}\n",
    "    def steps(self):\n",
    "        return [\n",
    "            MRStep(mapper=self.mapper , reducer= self.reducer, \n",
    "                   jobconf={\n",
    "                    \"mapred.map.tasks\":16,\n",
    "                    \"mapred.reduce.tasks\":8\n",
    "                    }),\n",
    "            MRStep(mapper=self.mapper2 ,combiner=self.combiner2, reducer=self.reducer2,\n",
    "                   jobconf={\n",
    "                    \"mapred.map.tasks\":8,\n",
    "                    \"mapred.reduce.tasks\":4\n",
    "                    }\n",
    "                  ),\n",
    "             MRStep(reducer=self.jaccard_cal, \n",
    "                    jobconf={\n",
    "                    \"mapred.map.tasks\":4,\n",
    "                    \"mapred.reduce.tasks\":1\n",
    "                    })\n",
    "            \n",
    "               ]\n",
    "    \n",
    "    def mapper(self, _, line):\n",
    "        key,terms = line.strip().split('\\t')\n",
    "        key = key.replace('\"', '')\n",
    "        if key[0] != '*':  #* represents a word with count - not a word with dictionary\n",
    "            docs = eval(terms).keys()\n",
    "            for doc in docs:\n",
    "                yield doc,key\n",
    "    \n",
    "    def reducer(self,key,value):\n",
    "        doc_list ={}\n",
    "        for v in value:\n",
    "            doc_list[v]=1\n",
    "        yield key, doc_list.keys()\n",
    "        \n",
    "    def mapper2(self,key,value):\n",
    "        doc_list = list(value)\n",
    "        for key1 in doc_list:\n",
    "            starkey = '*' + key1  #addint the * back... this seems redundant, but handled a strange error.\n",
    "            yield (starkey, key1),1\n",
    "            for key2 in doc_list:\n",
    "                if(key1 != key2):\n",
    "                    yield(key1,key2),1\n",
    "    \n",
    "    def combiner2(self,key,value):\n",
    "        yield key,sum(value)\n",
    "    \n",
    "    def reducer2(self,key,value):\n",
    "        yield key,sum(value)\n",
    "    \n",
    "    def jaccard_cal(self,key,value):\n",
    "        docA,docB = key\n",
    "\n",
    "        if docA.startswith('*'): #|doc|\n",
    "            self.global_doc_dict[docB] = sum(value)\n",
    "        else:  #at this point we have all the |doc|\n",
    "            ab = sum(value)\n",
    "            calc = 1.0*ab / (self.global_doc_dict[docA] + self.global_doc_dict[docB] - ab)\n",
    "            yield (docA,docB), calc\n",
    "        \n",
    "if __name__ == '__main__':\n",
    "    Jaccard_Index.run()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING: Module python-magic is not available. Guessing MIME types based on file extensions.\n",
      "upload: 'mrjob_Jaccard_Index.py' -> 's3://w261jing/mrjob_Jaccard_Index.py'  [1 of 1]\n",
      " 2637 of 2637   100% in    0s    16.38 kB/s  done\n"
     ]
    }
   ],
   "source": [
    "!s3cmd put FILE mrjob_Jaccard_Index.py s3://w261jing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Test with SYSTEMS_TEST_DATASET**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from mrjob_Jaccard_Index import Jaccard_Index\n",
    "import os\n",
    "\n",
    "mr_job = Jaccard_Index(args=['SYSTEMS_TEST_DATASET.txt'])\n",
    "\n",
    "output_file = \"SYSTEMS_TEST_JACCARD.out\"\n",
    "try:\n",
    "    os.remove(output_file)\n",
    "except OSError:\n",
    "    pass\n",
    "\n",
    "with mr_job.make_runner() as runner, open(output_file, 'a') as f: \n",
    "    runner.run()\n",
    "    # stream_output: get access of the output \n",
    "    for line in runner.stream_output():\n",
    "        #print mr_job.parse_output_line(line)\n",
    "        f.write(str(line))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "!head SYSTEMS_TEST_JACCARD.out"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Test with 5-grams**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:mrjob.runner:\n",
      "WARNING:mrjob.runner:PLEASE NOTE: Starting in mrjob v0.5.0, protocols will be strict by default. It's recommended you run your job with --strict-protocols or set up mrjob.conf as described at https://pythonhosted.org/mrjob/whats-new.html#ready-for-strict-protocols\n",
      "WARNING:mrjob.runner:\n"
     ]
    }
   ],
   "source": [
    "from mrjob_Jaccard_Index import Jaccard_Index\n",
    "import os\n",
    "\n",
    "mr_job = Jaccard_Index(args=['bigram_test_54.out'])\n",
    "\n",
    "output_file = \"bigram_test_Jaccard_54.out\"\n",
    "try:\n",
    "    os.remove(output_file)\n",
    "except OSError:\n",
    "    pass\n",
    "\n",
    "with mr_job.make_runner() as runner, open(output_file, 'a') as f: \n",
    "    runner.run()\n",
    "    # stream_output: get access of the output \n",
    "    for line in runner.stream_output():\n",
    "        #print mr_job.parse_output_line(line)\n",
    "        f.write(str(line))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[\"abate\", \"between\"]\t0.003289473684210526\n",
      "[\"abate\", \"black\"]\t0.021739130434782608\n",
      "[\"abate\", \"brick\"]\t0.125\n",
      "[\"abate\", \"building\"]\t0.016129032258064516\n",
      "[\"abate\", \"cell\"]\t0.02127659574468085\n",
      "[\"abate\", \"cells\"]\t0.024390243902439025\n",
      "[\"abate\", \"first\"]\t0.003937007874015748\n",
      "[\"abate\", \"flying\"]\t0.09090909090909091\n",
      "[\"abate\", \"heat\"]\t0.023809523809523808\n",
      "[\"abate\", \"is\"]\t0.0014005602240896359\n",
      "[\"worked\", \"works\"]\t0.23376623376623376\n",
      "[\"worked\", \"write\"]\t0.17857142857142858\n",
      "[\"worked\", \"writing\"]\t0.18666666666666668\n",
      "[\"worked\", \"wrought\"]\t0.1951219512195122\n",
      "[\"works\", \"write\"]\t0.1650485436893204\n",
      "[\"works\", \"writing\"]\t0.2222222222222222\n",
      "[\"works\", \"wrought\"]\t0.09375\n",
      "[\"write\", \"writing\"]\t0.17525773195876287\n",
      "[\"write\", \"wrought\"]\t0.12121212121212122\n",
      "[\"writing\", \"wrought\"]\t0.12280701754385964\n"
     ]
    }
   ],
   "source": [
    "!s3cmd rm --recursive s3://w261jing/hw5/jaccard54/\n",
    "!python mrjob_Jaccard_Index.py -r emr s3://w261jing/hw5/bigram54/part* --output-dir=s3://w261jing/hw5/jaccard54 --no-output --no-strict-protocol    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "!s3cmd get s3://w261jing/hw5/jaccard54/part-00000 jaccard54_out.txt\n",
    "\n",
    "!head jaccard54_out.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## HW 5.5\n",
    "In this part of the assignment you will evaluate the success of you synonym detector.\n",
    "Take the top 1,000 closest/most similar/correlative pairs of words as determined\n",
    "by your measure in (2), and use the synonyms function in the accompanying\n",
    "python code:\n",
    "\n",
    "nltk_synonyms.py\n",
    "\n",
    "Note: This will require installing the python nltk package:\n",
    "\n",
    "http://www.nltk.org/install.html\n",
    "\n",
    "and downloading its data with nltk.download().\n",
    "\n",
    "For each (word1,word2) pair, check to see if word1 is in the list, \n",
    "synonyms(word2), and vice-versa. If one of the two is a synonym of the other, \n",
    "then consider this pair a 'hit', and then report the precision, recall, and F1 measure  of \n",
    "your detector across your 1,000 best guesses. Report the macro averages of these measures."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[\"abdication\", \"accentuated\"]\t1.0\n",
      "[\"aching\", \"admittance\"]\t1.0\n",
      "[\"aching\", \"admonished\"]\t1.0\n",
      "[\"admittance\", \"admonished\"]\t1.0\n",
      "[\"be\", \"that\"]\t0.7340686274509803\n",
      "[\"is\", \"that\"]\t0.7305389221556886\n",
      "[\"be\", \"is\"]\t0.725925925925926\n",
      "[\"asserted\", \"checked\"]\t0.6666666666666666\n",
      "[\"checked\", \"hate\"]\t0.6666666666666666\n",
      "[\"beaten\", \"bestowed\"]\t0.6\n",
      "[\"abdication\", \"accentuated\"]\t1.0\n",
      "[\"aching\", \"admittance\"]\t1.0\n",
      "[\"aching\", \"admonished\"]\t1.0\n",
      "[\"admittance\", \"admonished\"]\t1.0\n",
      "[\"be\", \"that\"]\t0.7340686274509803\n",
      "[\"is\", \"that\"]\t0.7305389221556886\n",
      "[\"be\", \"is\"]\t0.725925925925926\n",
      "[\"asserted\", \"checked\"]\t0.6666666666666666\n",
      "[\"checked\", \"hate\"]\t0.6666666666666666\n",
      "[\"beaten\", \"bestowed\"]\t0.6\n",
      "    1000 top1k_test_55.out\n"
     ]
    }
   ],
   "source": [
    "!cat bigram_test_Jaccard_54.out | sort -k3nr  > file_test_55.out\n",
    "!head file_test_55.out\n",
    "!head -1000 file_test_55.out > top1k_test_55.out\n",
    "!head top1k_test_55.out\n",
    "!wc -l top1k_test_55.out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100\n",
      "200\n",
      "300\n",
      "400\n",
      "500\n",
      "600\n",
      "700\n",
      "800\n",
      "900\n",
      "1000\n",
      "Length of Synonym Dict: 1457\n",
      "MATCH: ['be', 'is']\n",
      "MATCH: ['made', 'make']\n",
      "MATCH: ['about', 'most']\n",
      "MATCH: ['do', 'make']\n",
      "MATCH: ['tried', 'try']\n",
      "MATCH: ['do', 'made']\n",
      "MATCH: ['job', 'task']\n",
      "MATCH: ['make', 'work']\n",
      "\n",
      "Total Count: 1000, TP: 8, FP: 8, FN: 0\n",
      "\n",
      "### PRECISION: 0.008\n",
      "\n",
      "### RECALL: 1.0\n",
      "\n",
      "### F1 Score: 0.016\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk.corpus import wordnet as wn\n",
    "import sys\n",
    "import ast\n",
    "#print all the synset element of an element\n",
    "def synonyms(string):\n",
    "    syndict = {}\n",
    "    for i,j in enumerate(wn.synsets(string)):\n",
    "        syns = j.lemma_names()\n",
    "        for syn in syns:\n",
    "            syndict.setdefault(syn,1)\n",
    "    return syndict.keys()\n",
    "\n",
    "total = 0\n",
    "true_positives = 0\n",
    "false_negatives = 0\n",
    "\n",
    "#Load synomyn file\n",
    "dict_syn = {}\n",
    "line_cnt = 0\n",
    "with open('top1k_test_55.out', 'r') as f:\n",
    "    for line in f:\n",
    "        line_cnt += 1\n",
    "        if line_cnt%100 == 0: print line_cnt\n",
    "        t = line.strip().split('\\t')\n",
    "        w1 = t[0].lower()\n",
    "        w2 = t[1].lower()\n",
    "        if w1 in dict_syn.keys():\n",
    "            dict_syn[w1].append(w2)\n",
    "        else:\n",
    "            dict_syn[w1] = [w2]\n",
    "        if w2 in dict_syn.keys():\n",
    "            dict_syn[w2].append(w1)\n",
    "        else:\n",
    "            dict_syn[w2] = [w1]\n",
    "\n",
    "print \"Length of Synonym Dict: {0}\".format(len(dict_syn))\n",
    "\n",
    "# Check if any of the top 1000 matches the synonym list\n",
    "with open('top1k_test_55.out', 'r') as f:\n",
    "    for line in f:\n",
    "        total += 1\n",
    "        t = line.strip().split('\\t')\n",
    "        pair = ast.literal_eval(t[0])\n",
    "        syn0 = synonyms(pair[0].lower().strip(' '))\n",
    "        syn1 = synonyms(pair[1].lower().strip(' '))\n",
    "        # Precision\n",
    "        if pair[1].lower() in syn0 or pair[0].lower() in syn1:\n",
    "            print \"MATCH: {0}\".format(pair)\n",
    "            true_positives += 1\n",
    "        \n",
    "        # Recall\n",
    "        if pair[0].lower() in dict_syn.keys() or pair[1].lower() in dict_syn.keys():\n",
    "            false_negatives += 1\n",
    "        \n",
    "            \n",
    "print \"\\nTotal Count: {0}, TP: {1}, FP: {2}, FN: {3}\".format(total, true_positives, true_positives - false_negatives, false_negatives)\n",
    "\n",
    "p = round(float(true_positives) / total, 3)\n",
    "r = round(float(true_positives) / (true_positives + false_negatives), 3)\n",
    "f1 = round(2 * p * r / (p + r), 3)\n",
    "\n",
    "print \"\\n### PRECISION: {0}\".format(p)\n",
    "print \"\\n### RECALL: {0}\".format(r)\n",
    "print \"\\n### F1 Score: {0}\".format(f1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting inverse_index_54.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile inverse_index_54.py\n",
    "#!/usr/bin/python\n",
    "## inverse_index_54.py\n",
    "## Author: Jing Xu \n",
    "## Description: Inverses an Index.\n",
    "\n",
    "\n",
    "from mrjob.job import MRJob\n",
    "from mrjob.step import MRStep\n",
    "from sets import Set\n",
    "import csv\n",
    "import ast\n",
    "\n",
    "class inverse_index(MRJob):\n",
    "    \n",
    "    stripes={} #initialize dictionary of stripes\n",
    "    \n",
    "    def steps(self):\n",
    "        return [MRStep(mapper=self.mapper,\n",
    "                     combiner=self.reducer)]\n",
    "    \n",
    "    def mapper(self, _, line):        \n",
    "        tokens = line.split()\n",
    "        key = tokens[0].strip()\n",
    "        stripe = tokens[1].strip()\n",
    "        stripes = ast.literal_eval(str(stripe))\n",
    "        word_len = len(stripes)\n",
    "        words = stripes.keys()\n",
    "        i = 0\n",
    "        for word in words:\n",
    "            yield word, (key,1)  #X, DocA\n",
    "    \n",
    "    def reducer(self, key, value):\n",
    "        dic = {}\n",
    "        for w,c in value:dic[w] = c\n",
    "            \n",
    "        yield  key, dic\n",
    "        \n",
    "if __name__ == '__main__':\n",
    "    inverse_index.run()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:mrjob.runner:\n",
      "WARNING:mrjob.runner:PLEASE NOTE: Starting in mrjob v0.5.0, protocols will be strict by default. It's recommended you run your job with --strict-protocols or set up mrjob.conf as described at https://pythonhosted.org/mrjob/whats-new.html#ready-for-strict-protocols\n",
      "WARNING:mrjob.runner:\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "M {'DocC': 1}\n",
      "N {'DocC': 1}\n",
      "X {'DocB': 1, 'DocA': 1}\n",
      "Y {'DocB': 1, 'DocA': 1}\n",
      "Z {'DocC': 1, 'DocA': 1}\n",
      "\n",
      "\n",
      "There are 5 records\n"
     ]
    }
   ],
   "source": [
    "from inverse_index_54 import inverse_index\n",
    "mr_job = inverse_index(args=['SYSTEMS_TEST_DATASET.txt'])\n",
    "with mr_job.make_runner() as runner: \n",
    "    runner.run()\n",
    "    count = 0\n",
    "    # stream_output: get access of the output \n",
    "    for line in runner.stream_output():\n",
    "        key,value =  mr_job.parse_output_line(line)\n",
    "        print key, value\n",
    "        count = count + 1\n",
    "print \"\\n\"\n",
    "print \"There are %s records\" %count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "!tail -1000 output_hw53_freq.txt > 1000grams.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\"asset\"\t547\r\n",
      "\"european\"\t5470\r\n",
      "\"reported\"\t5474\r\n",
      "\"bundle\"\t548\r\n",
      "\"college\"\t5481\r\n",
      "\"command\"\t5484\r\n",
      "\"notice\"\t5488\r\n",
      "\"hire\"\t549\r\n",
      "\"objects\"\t5494\r\n",
      "\"fell\"\t5495\r\n"
     ]
    }
   ],
   "source": [
    "!head 1000grams.txt"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
