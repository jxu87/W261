{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#DATSCIW261 ASSIGNMENT #1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Jing Xu\n",
    "\n",
    "jaling@gmail.com\n",
    "\n",
    "W261-3\n",
    "\n",
    "DATSCIW261 Assignment #1\n",
    "\n",
    "1/15/16"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**HW1.0.0. Define big data. Provide an example of a big data problem in your domain of expertise.**\n",
    "\n",
    "Big data is an umbrella description for existing datasets or sources of new data that haven't been tapped previously that contain a volume and/or complexity of data that can't be feasibly processed using traditional data-processing applications. Previously unquantifiable and uncaptured data, for example in the form of bioinformatic data or user behavior data, are now being generated and logged at a volume and complexity that exceed the hardware capabilities of single machines. These datasets require some form of parallel processing in order to achieve a throughput acceptable for working timelines in industry. Big data  For example, in the legal industry, there are projects using natural language processing to automate the classification of all legal and court documents produced daily by every local, state, and federal court in the U.S. These documents amount to gigabytes of data every week and petabytes every year. The current existing corpus of documents are in the petabytes. This has become a big data problem because companies in the legal tech space are looking to build legal research and document generation products that actively query these document databases and run NLP algorithms too complex for a single machine to process. A parallel processing infrastructure is needed to reasonably access and analyze this volume of legal document data. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**HW1.0.1. In 500 words (English or pseudo code or a combination) describe how to estimate the bias, the variance, the irreduciable error for a test dataset T when using polynomial regression models of degree 1, 2,3, 4,5 are considered. How would you select a model?**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"images/expected-error.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"images/equations.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Expected prediction error of a regression model can be broken down as Variance + Bias^2 + Irreducible Error (Noise^2) \n",
    "\n",
    "Bias - The bias of the regression model is calculated using the formula (h - y), where h is the average prediction model fit over all the datasets that can be sampled from the complete population that dataset T is sampled from, and y is the true function that describes the population. In this case, we will need to sample multiple datasets from the complete population to determine h. As the degree of the polynomial increases, the model becomes better fit to the data, which decreases the value of h and decreases the bias of the model. \n",
    "\n",
    "Variance - The variance of the regression model is calculated using the formula above, where k is each datapoint in T, yk is the model fit over T in question, and K is the total number of parameters in the model. Variance is a measure of the difference between the model dependent on T with the model estimated over all datasets drawn from the complete population. As the degree of the polynomial increases, the model yk becomes better fit to the data but the estimates become farther from the average model h, leading variance to increase.\n",
    "\n",
    "Irreducible Error - The irreducible error theoretically can't be calculated because we almost never know the true underlying function from which the dataset is generated. It is a noise term that measures the natural difference between the mean estimator fit on all datasets and the true function of the complete population."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"images/bias-variance.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I would select a model by incorporating either an AIC or BIC metric. AIC and BIC is a method of adding a penalty term for the number of parameters in a model. I would calculate the AIC or BIC for each polynomial regression model and choose the model where the AIC/BIC gain is balanced out by the increasing complexity of the model.  BIC penalizes the model for more parameters more so than AIC, so depending on whether I want the benefits of a more complex model I may use AIC or BIC. Both AIC and BIC are derived from the formula [-2logL + kp], where L is the likelihood function, p is the number of parameters in the model, and k is 2 for AIC and log(n) where n = sample size for BIC. To determine the final best model, I would look at a combination of the AIC/BIC scores along with the bias^2 and variance. I would plot these values for all regression models and observe if there is an optimal lowpoint - if there is a clear model with the lowest scores in all areas, that will most likely be the best model. If the scores are not as clear, I would choose a model based on the type of data, whether I am looking to minimize the bias to get a model closest to the true underlying function or if I'm trying to minimize variance to get a model that would be closest to the estimated best fit model over all possible sample datasets from the population."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "done\n"
     ]
    }
   ],
   "source": [
    "# HW1.1. Read through the provided control script (pNaiveBayes.sh) and all of its comments. \n",
    "# When you are comfortable with their purpose and function, respond to the remaining homework questions below. \n",
    "# A simple cell in the notebook with a print statmement with  a \"done\" string will suffice here. \n",
    "print \"done\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# HW1.2. Provide a mapper/reducer pair that, when executed by pNaiveBayes.sh\n",
    "# will determine the number of occurrences of a single, user-specified word. \n",
    "# Examine the word “assistance” and report your results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting mapper.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile mapper.py\n",
    "#!/usr/bin/python\n",
    "## mapper.py\n",
    "## Author: Jing Xu\n",
    "## Description: mapper code for HW1.2\n",
    "import sys\n",
    "import re\n",
    "import string\n",
    "\n",
    "filename = sys.argv[1] #read in first argument as the emails to be parsed\n",
    "findwords = sys.argv[2] #read in second argument as the word to be counted\n",
    "print \"FINDWORD\", findwords\n",
    "\n",
    "with open (filename, \"r\") as emails:\n",
    "    for email in emails.readlines(): #read each line in enronemail file, each corresponding to a single email sent\n",
    "        words = email.translate(string.maketrans(\"\",\"\"), string.punctuation) #strip all punctuation\n",
    "        words = words.split() #convert line into list of words\n",
    "        for word in words:\n",
    "            print word, 1\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting reducer.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile reducer.py\n",
    "#!/usr/bin/python\n",
    "## reducer.py\n",
    "## Author: Jing Xu\n",
    "## Description: reducer code for HW1.2\n",
    "\n",
    "import sys\n",
    "\n",
    "findword = None\n",
    "words = {} #creating unique word list\n",
    "\n",
    "for filenames in sys.argv[1:]: #open each filename in the countfile list\n",
    "    myfile = open('%s'%filenames, \"r\")\n",
    "    for line in myfile.readlines(): #read each line in mapper output\n",
    "        line = line.split() #split each line into list\n",
    "        if line[0] == \"FINDWORD\": findword = line[1]\n",
    "        else:\n",
    "            for word in line:\n",
    "                if word not in words: words[word] = 1\n",
    "                else: words[word]+=1\n",
    "\n",
    "print words[findword]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "!chmod a+x reducer.py\n",
    "!chmod a+x mapper.py\n",
    "!chmod a+x pNaiveBayes.sh"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "!./pNaiveBayes.sh 5 \"assistance\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are 10 occurrences of the word \"assistance\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# HW1.3. Provide a mapper/reducer pair that, when executed by pNaiveBayes.sh will classify \n",
    "# the email messages by a single, user-specified word using the multinomial Naive Bayes Formulation. \n",
    "# Examine the word “assistance” and report your results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 204,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting mapper.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile mapper.py\n",
    "#!/usr/bin/python\n",
    "## mapper.py\n",
    "## Author: Jing Xu\n",
    "## Description: mapper code for HW1.3\n",
    "import sys\n",
    "import re\n",
    "import string\n",
    "\n",
    "filename = sys.argv[1]\n",
    "findwords = sys.argv[2]\n",
    "emails = open(filename, \"r\")\n",
    "print \"FINDWORD\", findwords\n",
    "\n",
    "for line in emails.readlines():\n",
    "    email_id = line.split('\\t')[0]\n",
    "    line = line.translate(string.maketrans(\"\",\"\"), string.punctuation) #strip punctuation\n",
    "    email = re.split(r'\\t+', line) #strip words that include any numbers\n",
    "    if len(email) != 4: #skip over email data formatting errors          \n",
    "        continue\n",
    "    content = email[2] + email[3] #concatenate subject and body sections into one string\n",
    "    content = re.sub(r'\\w*\\d\\w*', '', content).strip() #strip all words that include a number as these words are unlikely to be predictive\n",
    "    content = re.sub(\"\\s\\s+\" , \" \", content) #strip all extra whitespaces\n",
    "    list_content = content.split(' ') #list of each word in line\n",
    "    if int(email[1]) == 1: #check if the email is spam or not, count instances of word appearing in spam/not-spam emails and total emails\n",
    "        print \"SPAM_COUNT\", len(list_content) #emit key of SPAM_COUNT along with a word count value for later calculation of total SPAM words\n",
    "        for word in list_content:\n",
    "            print email_id, word.lower(), \"SPAM\", list_content.count(word) #emit email_id key, word, class, and count\n",
    "    else: \n",
    "        print \"HAM_COUNT\", len(list_content) #emit key of HAM_COUNT along with a word count value for later calculation of total HAM words\n",
    "        for word in list_content:\n",
    "            print email_id, word.lower(), \"HAM\", list_content.count(word) #emit email_id key, word, class, and count\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 205,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting reducer.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile reducer.py\n",
    "#!/usr/bin/python\n",
    "## reducer.py\n",
    "## Author: Jing Xu\n",
    "## Description: reducer code for HW1.3\n",
    "\n",
    "import sys\n",
    "import re\n",
    "import string\n",
    "import ast\n",
    "\n",
    "spam_emails = 0\n",
    "ham_emails = 0\n",
    "total_spam_words = 0\n",
    "total_ham_words = 0\n",
    "findword = ''\n",
    "filename = ''\n",
    "words = {} #creating unique word list\n",
    "\n",
    "for filenames in sys.argv[1:]: #open each filename in the countfile list\n",
    "    myfile = open('%s'%filenames, \"r\")\n",
    "    for line in myfile.readlines(): #read each line in mapper output\n",
    "        line = line.split() #split each line into list\n",
    "        if line[0] == 'SPAM_COUNT': \n",
    "            spam_emails+=1 #add 1 to spam_emails\n",
    "            total_spam_words+=int(line[1]) #add spam words in email to total_spam_words\n",
    "        elif line[0] == 'HAM_COUNT': \n",
    "            ham_emails+=1 #add 1 to ham_emails\n",
    "            total_ham_words+=int(line[1]) #add ham words in email to total_spam_words\n",
    "        elif line[0] == 'FINDWORD': findword = line[1] #store findword in memory\n",
    "        else: \n",
    "            word = str(line[1])\n",
    "            if line[2] == \"SPAM\": #sort word into SPAM dictionary\n",
    "                if word not in words: #create new dictionary index for word if not already existing\n",
    "                    words[word] = {}\n",
    "                    words[word]['SPAM'] = int(line[3]) #set count of the number of word in spam emails to 1\n",
    "                else:\n",
    "                    if 'SPAM' in words[word]: words[word]['SPAM']+=int(line[3]) #add 1 to number of word in spam emails\n",
    "                    else: words[word]['SPAM'] = int(line[3]) #if word exists in dictionary but not the spam count, create spam count for the word\n",
    "            else: #sort word into HAM dictionary\n",
    "                if word not in words: #create new dictionary index for word if not already existing\n",
    "                    words[word] = {}\n",
    "                    words[word]['HAM'] = int(line[3]) #set count of the number of word in ham emails to 1            \n",
    "                else: \n",
    "                    if 'HAM' in words[word]: words[word]['HAM']+=int(line[3]) #add 1 to number of word in ham emails\n",
    "                    else: words[word]['HAM'] = int(line[3]) #if word exists in dictionary but not the ham count, create spam count for the word\n",
    "                \n",
    "\n",
    "prior_spam = float(spam_emails)/(float(spam_emails)+float(ham_emails)) #prior spam = spam emails / total emails\n",
    "prior_ham = float(ham_emails)/(float(spam_emails)+float(ham_emails)) #prior ham = ham emails / total emails\n",
    "spam_probability = float(words[findword]['SPAM'])/float(total_spam_words) #spam probability is the number of occurrences of word in spam emails / total words in spam emails\n",
    "ham_probability = float(words[findword]['HAM'])/float(total_ham_words) #ham probability is the number of occurrences of word in ham emails / total words in non-spam emails\n",
    "\n",
    "print \"SPAM,\", findword, spam_probability\n",
    "print \"HAM,\", findword, ham_probability\n",
    "print \"SPAM Prior =\", prior_spam\n",
    "print \"HAM Prior =\", prior_ham\n",
    "\n",
    "# for email in all_emails:\n",
    "#     count_of_word = 0\n",
    "#     for each in email: \n",
    "#         if findword == each: #create count of word\n",
    "#             count_of_word+=1\n",
    "#             print 'yes'\n",
    "#     mnb_spam_probability = prior_spam*spam_probability**count_of_word #mnb formula for calculating probability of spam given a word\n",
    "#     mnb_ham_probability = prior_ham*ham_probability**count_of_word #mnb formula for calculating probability of ham given a word    \n",
    "#     if mnb_spam_probability > mnb_ham_probability: predictions.append(1) #if probability of spam > ham, prediction of 1 indicates spam\n",
    "#     else: predictions.append(0)\n",
    "\n",
    "# print predictions\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 206,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "!./pNaiveBayes.sh 5 \"assistance\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "SPAM, assistance 0.000755770013371\n",
    "\n",
    "HAM, assistance 0.000164758217316\n",
    "\n",
    "SPAM Prior = 0.438775510204\n",
    "\n",
    "HAM Prior = 0.561224489796"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# HW1.4. Provide a mapper/reducer pair that, when executed by pNaiveBayes.sh\n",
    "# will classify the email messages by a list of one or more user-specified words. \n",
    "# Examine the words “assistance”, “valium”, and “enlargementWithATypo” and report your results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 226,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting mapper.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile mapper.py\n",
    "#!/usr/bin/python\n",
    "## mapper.py\n",
    "## Author: Jing Xu\n",
    "## Description: mapper code for HW1.4\n",
    "import sys\n",
    "import re\n",
    "import string\n",
    "\n",
    "filename = sys.argv[1]\n",
    "findwords = sys.argv[2]\n",
    "emails = open(filename, \"r\")\n",
    "print \"FINDWORDS\", findwords\n",
    "\n",
    "for line in emails.readlines():\n",
    "    email_id = line.split('\\t')[0]\n",
    "    line = line.translate(string.maketrans(\"\",\"\"), string.punctuation) #strip punctuation\n",
    "    email = re.split(r'\\t+', line) #strip words that include any numbers\n",
    "    if len(email) != 4: #skip over email data formatting errors          \n",
    "        continue\n",
    "    content = email[2] + email[3] #concatenate subject and body sections into one string\n",
    "    content = re.sub(r'\\w*\\d\\w*', '', content).strip() #strip all words that include a number as these words are unlikely to be predictive\n",
    "    content = re.sub(\"\\s\\s+\" , \" \", content) #strip all extra whitespaces\n",
    "    list_content = content.split(' ') #list of each word in line\n",
    "    if int(email[1]) == 1: #check if the email is spam or not, count instances of word appearing in spam/not-spam emails and total emails\n",
    "        print \"SPAM_COUNT\", len(list_content) #emit key of SPAM_COUNT along with a word count value for later calculation of total SPAM words\n",
    "        for word in list_content:\n",
    "            print email_id, word.lower(), \"SPAM\", list_content.count(word) #emit email_id key, word, class, and count\n",
    "    else: \n",
    "        print \"HAM_COUNT\", len(list_content) #emit key of HAM_COUNT along with a word count value for later calculation of total HAM words\n",
    "        for word in list_content:\n",
    "            print email_id, word.lower(), \"HAM\", list_content.count(word) #emit email_id key, word, class, and count\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 247,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting reducer.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile reducer.py\n",
    "#!/usr/bin/python\n",
    "## reducer.py\n",
    "## Author: Jing Xu\n",
    "## Description: reducer code for HW1.4\n",
    "\n",
    "import sys\n",
    "import re\n",
    "import string\n",
    "import ast\n",
    "\n",
    "spam_emails = 0\n",
    "ham_emails = 0\n",
    "total_spam_words = 0\n",
    "total_ham_words = 0\n",
    "findwords = ''\n",
    "filename = ''\n",
    "words = {} #creating unique word list\n",
    "\n",
    "for filenames in sys.argv[1:]: #open each filename in the countfile list\n",
    "    myfile = open('%s'%filenames, \"r\")\n",
    "    for line in myfile.readlines(): #read each line in mapper output\n",
    "        line = line.split() #split each line into list\n",
    "        if line[0] == 'SPAM_COUNT': \n",
    "            spam_emails+=1 #add 1 to spam_emails\n",
    "            total_spam_words+=int(line[1]) #add spam words in email to total_spam_words\n",
    "        elif line[0] == 'HAM_COUNT': \n",
    "            ham_emails+=1 #add 1 to ham_emails\n",
    "            total_ham_words+=int(line[1]) #add ham words in email to total_spam_words\n",
    "        elif line[0] == 'FINDWORDS': findwords = line[1:] #store findword in memory\n",
    "        else: \n",
    "            word = str(line[1])\n",
    "            if line[2] == \"SPAM\": #sort word into SPAM dictionary\n",
    "                if word not in words: #create new dictionary index for word if not already existing\n",
    "                    words[word] = {}\n",
    "                    words[word]['SPAM'] = int(line[3]) #set count of the number of word in spam emails to 1\n",
    "                else:\n",
    "                    if 'SPAM' in words[word]: words[word]['SPAM']+=int(line[3]) #add 1 to number of word in spam emails\n",
    "                    else: words[word]['SPAM'] = int(line[3]) #if word exists in dictionary but not the spam count, create spam count for the word\n",
    "            else: #sort word into HAM dictionary\n",
    "                if word not in words: #create new dictionary index for word if not already existing\n",
    "                    words[word] = {}\n",
    "                    words[word]['HAM'] = int(line[3]) #set count of the number of word in ham emails to 1            \n",
    "                else: \n",
    "                    if 'HAM' in words[word]: words[word]['HAM']+=int(line[3]) #add 1 to number of word in ham emails\n",
    "                    else: words[word]['HAM'] = int(line[3]) #if word exists in dictionary but not the ham count, create spam count for the word\n",
    "\n",
    "for findword in findwords:\n",
    "    prior_spam = float(spam_emails)/(float(spam_emails)+float(ham_emails)) #prior spam = spam emails / total emails\n",
    "    prior_ham = float(ham_emails)/(float(spam_emails)+float(ham_emails)) #prior ham = ham emails / total emails\n",
    "    try: spam_probability = float(words[findword]['SPAM'])/float(total_spam_words) #spam probability is the number of occurrences of word in spam emails / total words in spam emails\n",
    "    except: spam_probability = 'N/A'\n",
    "    try: ham_probability = float(words[findword]['HAM'])/float(total_ham_words) #ham probability is the number of occurrences of word in ham emails / total words in non-spam emails\n",
    "    except: ham_probability = 'N/A'\n",
    "    print \"Class conditional: SPAM,\", findword, spam_probability\n",
    "    print \"Class conditional: HAM,\", findword, ham_probability\n",
    "    print \"SPAM Prior =\", prior_spam\n",
    "    print \"HAM Prior =\", prior_ham"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 248,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "!./pNaiveBayes.sh 5 \"assistance valium enlargementWithATypo\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Class conditional: SPAM, assistance 0.000755770013371\n",
    "\n",
    "Class conditional: HAM, assistance 0.000164758217316\n",
    "\n",
    "SPAM Prior = 0.438775510204\n",
    "\n",
    "HAM Prior = 0.561224489796\n",
    "\n",
    "Class conditional: SPAM, valium 0.000174408464624\n",
    "\n",
    "Class conditional: HAM, valium N/A\n",
    "\n",
    "SPAM Prior = 0.438775510204\n",
    "\n",
    "HAM Prior = 0.561224489796\n",
    "\n",
    "Class conditional: SPAM, enlargementWithATypo N/A\n",
    "\n",
    "Class conditional: HAM, enlargementWithATypo N/A\n",
    "\n",
    "SPAM Prior = 0.438775510204\n",
    "\n",
    "HAM Prior = 0.561224489796"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
