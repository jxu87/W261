{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#DATSCIW261 ASSIGNMENT #1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Jing Xu\n",
    "\n",
    "jaling@gmail.com\n",
    "\n",
    "W261-3\n",
    "\n",
    "DATSCIW261 Assignment #1\n",
    "\n",
    "1/15/16"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "HW1.0.0. Big data is an umbrella description for datasets that contain a volume and/or complexity of data that can't be feasibly processed using traditional data-processing applications. These datasets require some form of parallel processing in order to achieve a throughput acceptable for working timelines in industry. For example, in the legal industry, there are projects using natural language processing to automate the classification of all legal and court documents produced daily by every local, state, and federal court in the U.S. These documents amount "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "HW1.0.1. \n",
    "\n",
    "The total expected error of a regression model can be broken down as Irreducible Error + Bias^2 + Variance\n",
    "\n",
    "Bias - The bias of the regression model is calculated using the formula E[g(x)] - f(x), where E[g(x)] is the expected (mean) estimator fit over all the datasets that can be sampled from the complete population that dataset T is sampled from, and f(x) is the true function that describes T. In this case, we will need to sample multiple datasets from the complete population to determine E[g(x)]. As the degree of the polynomial increases, the model becomes better fit to the data, which decreases the value of E[g(x)] and decreases the bias of the model. \n",
    "\n",
    "Variance - The variance of the regression model is calculated using the formula: variance=E[(g(x)-E[g(x)])^2], where g(x) is the model fit over T. This is to measure the difference between the model dependent on T with the model estimated over all datasets drawn from the complete population. As the degree of the polynomial increases, the model g(x) becomes better fit to the data but the estimates become farther from the average model E[g(x)], leading variance to increase.\n",
    "\n",
    "Irreducible Error - The irreducible error theoretically can't be calculated because we almost never know the true underlying function from which the dataset is generated. It is a noise term that measures the natural difference between the mean estimator fit on all datasets and the true function of the complete population.\n",
    "\n",
    "I would select a model by incorporating either an AIC or BIC metric. AIC and BIC is a method of adding a penalty term for the number of parameters in a model. I would calculate the AIC or BIC for each polynomial regression model and choose the model where the AIC/BIC gain is balanced out by the increasing complexity of the model.  BIC penalizes the model for more parameters more so than AIC, so depending on whether I want the benefits of a more complex model I may use AIC or BIC. Both AIC and BIC are derived from the formula [-2logL + kp], where L is the likelihood function, p is the number of parameters in the model, and k is 2 for AIC and log(n) where n = sample size for BIC. To determine the final best model, I would look at a combination of the AIC/BIC scores along with the bias^2 and variance. If there is a clear model with the lowest scores in all areas, that will most likely be the best model. If the scores are not as clear, I would choose a model based on the type of data, whether I am looking to minimize the bias to get a model closest to the true underlying function or if I'm trying to minimize variance to get a model that would be closest to the estimated best fit model over all possible sample datasets from the true population."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "done\n"
     ]
    }
   ],
   "source": [
    "# HW1.1. Read through the provided control script (pNaiveBayes.sh) and all of its comments. \n",
    "# When you are comfortable with their purpose and function, respond to the remaining homework questions below. \n",
    "# A simple cell in the notebook with a print statmement with  a \"done\" string will suffice here. \n",
    "print \"done\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# HW1.2. Provide a mapper/reducer pair that, when executed by pNaiveBayes.sh\n",
    "# will determine the number of occurrences of a single, user-specified word. \n",
    "# Examine the word “assistance” and report your results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting mapper.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile mapper.py\n",
    "#!/usr/bin/python\n",
    "## mapper.py\n",
    "## Author: Jing Xu\n",
    "## Description: mapper code for HW1.2\n",
    "import sys\n",
    "import re\n",
    "import string\n",
    "\n",
    "count = 0\n",
    "filename = sys.argv[1]\n",
    "findwords = sys.argv[2]\n",
    "with open (filename, \"r\") as myfile:\n",
    "    for line in myfile.readlines():\n",
    "        words = line.translate(string.maketrans(\"\",\"\"), string.punctuation) #strip all punctuation\n",
    "        words = re.split(\" \", words.lower()) #convert line into list of words\n",
    "        for word in words:\n",
    "            if word.lower() == findwords.lower(): #count each word match\n",
    "                count+=1\n",
    "print int(count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting reducer.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile reducer.py\n",
    "#!/usr/bin/python\n",
    "## reducer.py\n",
    "## Author: Jing Xu\n",
    "## Description: reducer code for HW1.2\n",
    "\n",
    "import sys\n",
    "total = 0\n",
    "for filenames in sys.argv[1:]: #open each filename in the countfile list\n",
    "    myfile = open('%s'%filenames, \"r\")\n",
    "    for line in myfile.readlines(): \n",
    "        total+=int(line) #add each chunk count\n",
    "print total\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "!chmod a+x reducer.py\n",
    "!chmod a+x mapper.py\n",
    "!chmod a+x pNaiveBayes.sh"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "!./pNaiveBayes.sh 5 \"assistance\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are 10 occurrences of the word \"assistance\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# HW1.3. Provide a mapper/reducer pair that, when executed by pNaiveBayes.sh will classify \n",
    "# the email messages by a single, user-specified word using the multinomial Naive Bayes Formulation. \n",
    "# Examine the word “assistance” and report your results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting mapper.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile mapper.py\n",
    "#!/usr/bin/python\n",
    "## mapper.py\n",
    "## Author: Jing Xu\n",
    "## Description: mapper code for HW1.3\n",
    "import sys\n",
    "import re\n",
    "import string\n",
    "\n",
    "spam_emails = 0\n",
    "total_emails = 0\n",
    "total_spam_words = 0\n",
    "total_ham_words = 0\n",
    "spam_count = 0\n",
    "ham_count = 0\n",
    "filename = sys.argv[1]\n",
    "findwords = sys.argv[2]\n",
    "emails = open(filename, \"r\")\n",
    "for line in emails.readlines():\n",
    "    line = line.translate(string.maketrans(\"\",\"\"), string.punctuation) #strip punctuation\n",
    "    email = re.split(r'\\t+', line) \n",
    "    if len(email) != 4: #skip over email data formatting errors          \n",
    "        continue\n",
    "    total_emails+=1\n",
    "    content = email[0] + email[2] + email[3] #concatenate subject and body sections into one string\n",
    "    content = re.sub(r'\\w*\\d\\w*', '', content).strip() #strip all words that include a number as these words are unlikely to be predictive\n",
    "    content = re.sub(\"\\s\\s+\" , \" \", content) #strip all extra whitespaces\n",
    "    list_content = content.split(' ') #list of each word in line\n",
    "    if int(email[1]) == 1: #check if the email is spam or not, count instances of word appearing in spam/not-spam emails and total emails\n",
    "        spam_emails+=1\n",
    "        for word in list_content:\n",
    "            if word.lower() == findwords.lower():\n",
    "                spam_count+=1        \n",
    "            total_spam_words+=1\n",
    "    else: \n",
    "        for word in list_content:\n",
    "            if word.lower() == findwords.lower():\n",
    "                ham_count+=1\n",
    "            total_ham_words+=1\n",
    "    print content\n",
    "    \n",
    "print \"spam_emails\", spam_emails\n",
    "print \"total_emails\", total_emails\n",
    "print \"total_spam_words\", total_spam_words\n",
    "print \"total_ham_words\", total_ham_words\n",
    "print \"spam_count\", spam_count\n",
    "print \"ham_count\", ham_count\n",
    "print \"word\", findwords\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting reducer.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile reducer.py\n",
    "#!/usr/bin/python\n",
    "## reducer.py\n",
    "## Author: Jing Xu\n",
    "## Description: reducer code for HW1.3\n",
    "\n",
    "import sys\n",
    "\n",
    "spam_emails = 0\n",
    "total_emails = 0\n",
    "total_spam_words = 0\n",
    "total_ham_words = 0\n",
    "spam_count = 0\n",
    "ham_count = 0\n",
    "word = ''\n",
    "all_emails = []\n",
    "for filenames in sys.argv[1:]: #open each filename in the countfile list\n",
    "    myfile = open('%s'%filenames, \"r\")\n",
    "    for line in myfile.readlines():\n",
    "        line = line.split()\n",
    "        #aggregate counts for all variables of interest\n",
    "        if line[0] == \"spam_emails\": \n",
    "            spam_emails+=int(line[1])\n",
    "        elif line[0] == \"total_emails\":\n",
    "            total_emails+=int(line[1])    \n",
    "        elif line[0] == 'total_spam_words':\n",
    "            total_spam_words+=int(line[1])\n",
    "        elif line[0] == 'total_ham_words':\n",
    "            total_ham_words+=int(line[1])\n",
    "        elif line[0] == 'spam_count':\n",
    "            spam_count+=int(line[1])\n",
    "        elif line[0] == 'ham_count':\n",
    "            ham_count+=int(line[1])\n",
    "        elif line[0] == 'word': #create variable for search word\n",
    "            word = line[1]\n",
    "        else: all_emails.append(line)\n",
    "\n",
    "prior_spam = float(spam_emails)/float(total_emails) #prior spam = spam emails / total emails\n",
    "prior_ham = float(total_emails-spam_emails)/float(total_emails) #prior ham = ham emails / total emails\n",
    "spam_probability = float(spam_count)/float(total_spam_words) #spam probability is the number of occurrences of word in spam emails / total words in spam emails\n",
    "ham_probability = float(ham_count)/float(total_ham_words) #ham probability is the number of occurrences of word in ham emails / total words in non-spam emails\n",
    "\n",
    "predictions = []\n",
    "\n",
    "for email in all_emails:\n",
    "    count_of_word = 0\n",
    "    for each in email: \n",
    "        if word == each: #create count of word\n",
    "            count_of_word+=1\n",
    "    mnb_spam_probability = prior_spam*spam_probability**count_of_word #formula for calculating probability of spam given a word\n",
    "    mnb_ham_probability = prior_ham*ham_probability**count_of_word #formula for calculating probability of ham given a word    \n",
    "    if mnb_spam_probability > mnb_ham_probability: predictions.append(1) #if probability of spam > ham, prediction of 1 indicates spam\n",
    "    else: predictions.append(0)\n",
    "\n",
    "print predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "!./pNaiveBayes.sh 5 \"assistance\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using \"assistance\" with my single word MNB model results in the classification of 7 emails in the dataset as spam."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# HW1.4. Provide a mapper/reducer pair that, when executed by pNaiveBayes.sh\n",
    "# will classify the email messages by a list of one or more user-specified words. \n",
    "# Examine the words “assistance”, “valium”, and “enlargementWithATypo” and report your results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting mapper.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile mapper.py\n",
    "#!/usr/bin/python\n",
    "## mapper.py\n",
    "## Author: Jing Xu\n",
    "## Description: mapper code for HW1.4\n",
    "import sys\n",
    "import re\n",
    "import string\n",
    "\n",
    "\n",
    "spam_emails = 0\n",
    "total_emails = 0\n",
    "total_spam_words = 0\n",
    "total_ham_words = 0\n",
    "filename = sys.argv[1]\n",
    "findwords = sys.argv[2].split(' ')\n",
    "count_dictionary = {}\n",
    "count_dictionary['spam'] = {}\n",
    "count_dictionary['ham'] = {}  \n",
    "emails = open(filename, \"r\")\n",
    "for line in emails.readlines():\n",
    "    line = line.translate(string.maketrans(\"\",\"\"), string.punctuation) #strip punctuation\n",
    "    email = re.split(r'\\t+', line) \n",
    "    if len(email) != 4: #skip over email data formatting errors          \n",
    "        continue\n",
    "    total_emails+=1\n",
    "    content = email[0] + email[2] + email[3] #concatenate subject and body sections into one string\n",
    "    content = re.sub(r'\\w*\\d\\w*', '', content).strip() #strip all words that include a number as these words are unlikely to be predictive\n",
    "    content = re.sub(\"\\s\\s+\" , \" \", content) #strip all extra whitespaces\n",
    "    list_content = content.split(' ') #list of each word in line\n",
    "    if int(email[1]) == 1: #check if the email is spam or not, count instances of word appearing in spam/not-spam emails and total emails\n",
    "        spam_emails+=1\n",
    "        for word in findwords:\n",
    "            word = word.lower()\n",
    "            for each in list_content:\n",
    "                if each.lower() == word:\n",
    "                    if word not in count_dictionary['spam']: count_dictionary['spam'][word] = 1   \n",
    "                    else: count_dictionary['spam'][word]+=1\n",
    "                total_spam_words+=1\n",
    "    else: \n",
    "        for word in findwords:\n",
    "            word = word.lower()\n",
    "            for each in list_content:\n",
    "                if each.lower() == word:\n",
    "                    if word not in count_dictionary['ham']: count_dictionary['ham'][word] = 1   \n",
    "                    else: count_dictionary['ham'][word]+=1\n",
    "                total_ham_words+=1    \n",
    "    print content\n",
    "    \n",
    "print \"spam_emails\", spam_emails\n",
    "print \"total_emails\", total_emails\n",
    "print \"total_spam_words\", total_spam_words\n",
    "print \"total_ham_words\", total_ham_words\n",
    "print \"word\", sys.argv[2]\n",
    "print count_dictionary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting reducer.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile reducer.py\n",
    "#!/usr/bin/python\n",
    "## reducer.py\n",
    "## Author: Jing Xu\n",
    "## Description: reducer code for HW1.4\n",
    "\n",
    "import sys\n",
    "import ast\n",
    "import math\n",
    "\n",
    "spam_emails = 0\n",
    "total_emails = 0\n",
    "total_spam_words = 0\n",
    "total_ham_words = 0\n",
    "words = ''\n",
    "unique_words = []\n",
    "all_emails = []\n",
    "final_count_dictionary = {}\n",
    "final_count_dictionary['spam'] = {}\n",
    "final_count_dictionary['ham'] = {}  \n",
    "for filenames in sys.argv[1:]: #open each filename in the countfile list\n",
    "    myfile = open('%s'%filenames, \"r\")\n",
    "    for line in myfile.readlines():\n",
    "        if line[0] == \"{\":\n",
    "            count_dictionary = line\n",
    "            count_dictionary = ast.literal_eval(count_dictionary) #convert dictionary string to dictionary class\n",
    "            for key in count_dictionary:\n",
    "                for word in count_dictionary[key]:\n",
    "                    if word not in final_count_dictionary[key]: final_count_dictionary[key][word] = count_dictionary[key][word]\n",
    "                    else: final_count_dictionary[key][word] += count_dictionary[key][word]\n",
    "        else: line = line.split()\n",
    "        #aggregate counts for all variables of interest\n",
    "        if line[0] == \"spam_emails\": \n",
    "            spam_emails+=int(line[1])\n",
    "        elif line[0] == \"total_emails\":\n",
    "            total_emails+=int(line[1])    \n",
    "        elif line[0] == 'total_spam_words':\n",
    "            total_spam_words+=int(line[1])\n",
    "        elif line[0] == 'total_ham_words':\n",
    "            total_ham_words+=int(line[1])\n",
    "        elif line[0] == 'word': #create variable for search word\n",
    "            words = line[1:]\n",
    "        else: #create list of unique words for later use\n",
    "            for word in line:\n",
    "                if word not in unique_words: unique_words.append(word)\n",
    "            all_emails.append(line)\n",
    "            \n",
    "prior_spam = float(spam_emails)/float(total_emails) #prior spam = spam emails / total emails\n",
    "prior_ham = float(total_emails-spam_emails)/float(total_emails) #prior ham = ham emails / total emails\n",
    "\n",
    "predictions = []\n",
    "\n",
    "#creation of conditional probability dictionary for all search words in all spam and ham emails\n",
    "conditional_prob = {}\n",
    "conditional_prob['spam'] = {}\n",
    "conditional_prob['ham'] = {}\n",
    "for word in words:\n",
    "    if word in final_count_dictionary['spam']: \n",
    "        conditional_prob['spam'][word] = (float(final_count_dictionary['spam'][word]) + float(1))/(float(total_spam_words) + float(len(unique_words)))\n",
    "    else: conditional_prob['spam'][word] = (float(1))/(float(total_spam_words)+float(len(unique_words)))\n",
    "    if word in final_count_dictionary['ham']:\n",
    "        conditional_prob['ham'][word] = (float(final_count_dictionary['ham'][word]) + float(1))/(float(total_ham_words) + float(len(unique_words)))\n",
    "    else: conditional_prob['ham'][word] = (float(1))/(float(total_ham_words)+float(len(unique_words)))\n",
    "        \n",
    "for email in all_emails:\n",
    "    mnb_spam_probability = prior_spam #start of MNB formula to calculate ham probability given search words\n",
    "    mnb_ham_probability = prior_ham #start of MNB formula to calculate ham probability given search words\n",
    "    for word in words:\n",
    "        count_of_word = 0\n",
    "        for each in email: \n",
    "            if word == each: #create count of word\n",
    "                count_of_word+=1\n",
    "            mnb_spam_probability *= float(conditional_prob['spam'][word]**count_of_word) #completion of formula for calculating probability of spam given a word\n",
    "            mnb_ham_probability *= float(conditional_prob['ham'][word]**count_of_word) #completion of formula for calculating probability of ham given a word        \n",
    "    if mnb_spam_probability > mnb_ham_probability: predictions.append(1) #if probability of spam > ham, prediction of 1 indicates spam\n",
    "    else: predictions.append(0)\n",
    "\n",
    "print predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "!./pNaiveBayes.sh 5 \"assistance valium enlargementWithATypo\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using \"assistance\", \"valium\", and \"enlargementWithATypo\" with my MNB model results in the classification of 3 emails in the dataset as spam."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
