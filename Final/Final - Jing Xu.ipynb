{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "%matplotlib inline\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Welcome to\n",
      "      ____              __\n",
      "     / __/__  ___ _____/ /__\n",
      "    _\\ \\/ _ \\/ _ `/ __/  '_/\n",
      "   /__ / .__/\\_,_/_/ /_/\\_\\   version 1.4.1\n",
      "      /_/\n",
      "\n",
      "Using Python version 2.7.10 (default, May 28 2015 17:04:42)\n",
      "SparkContext available as sc, HiveContext available as sqlContext.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import sys\n",
    "spark_home = os.environ['SPARK_HOME'] = '/Users/JingXu/Downloads/spark-1.4.1-bin-hadoop2.6/'\n",
    "\n",
    "\n",
    "if not spark_home:\n",
    "    raise ValueError('SPARK_HOME enviroment variable is not set')\n",
    "sys.path.insert(0,os.path.join(spark_home,'python'))\n",
    "sys.path.insert(0,os.path.join(spark_home,'python/lib/py4j-0.8.2.1-src.zip'))\n",
    "execfile(os.path.join(spark_home,'python/pyspark/shell.py'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ET13"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(3, (4, 9)), (3, (4, 6)), (3, (6, 9)), (3, (6, 6))]\n"
     ]
    }
   ],
   "source": [
    "rdd1 = sc.parallelize([(1, 2), (3, 4), (3, 6)])\n",
    "rdd2 = sc.parallelize([(3, 9), (3, 6)])\n",
    "joined = rdd1.join(rdd2)\n",
    "print joined.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ET15"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting callsign_tbl_sorted.txt\n"
     ]
    }
   ],
   "source": [
    "%%writefile callsign_tbl_sorted.txt\n",
    "3AZ, Monaco (Principality of)\n",
    "3BZ, Mauritius (Republic of)\n",
    "3CZ, Equatorial Guinea (Republic of)\n",
    "3DM, Swaziland (Kingdom of)\n",
    "3DZ, Fiji (Republic of)\n",
    "3FZ, Panama (Republic of)\n",
    "3GZ, Chile\n",
    "3UZ, China (People's Republic of)\n",
    "3VZ, Tunisia\n",
    "3WZ, Viet Nam (Socialist Republic of)\n",
    "3XZ, Guinea (Republic of)\n",
    "3YZ, Norway\n",
    "3ZZ, Poland (Republic of)\n",
    "4CZ, Mexico\n",
    "4IZ, Philippines (Republic of the)\n",
    "4KZ, Azerbaijani Republic\n",
    "4LZ, Georgia (Republic of)\n",
    "4MZ, Venezuela (Republic of)\n",
    "4OZ, Montenegro (Republic of)                    \t(WRC-07)\n",
    "4SZ, Sri Lanka (Democratic Socialist Republic of)\n",
    "4TZ, Peru\n",
    "4UZ, United Nations\n",
    "4VZ, Haiti (Republic of)\n",
    "4WZ, Democratic Republic of Timor-Leste   (WRC-03)\n",
    "4XZ, Israel (State of)\n",
    "4YZ, International Civil Aviation Organization\n",
    "4ZZ, Israel (State of)\n",
    "5AZ, Libya (Socialist People's Libyan Arab Jamahiriya)\n",
    "5BZ, Cyprus (Republic of)\n",
    "5GZ, Morocco (Kingdom of)\n",
    "5IZ, Tanzania (United Republic of)\n",
    "5KZ, Colombia (Republic of)\n",
    "5MZ, Liberia (Republic of)\n",
    "5OZ, Nigeria (Federal Republic of)\n",
    "5QZ, Denmark\n",
    "5SZ, Madagascar (Republic of)\n",
    "5TZ, Mauritania (Islamic Republic of)\n",
    "5UZ, Niger (Republic of the)\n",
    "5VZ, Togolese Republic\n",
    "5WZ, Samoa (Independent State of)\n",
    "5XZ, Uganda (Republic of)\n",
    "5ZZ, Kenya (Republic of)\n",
    "6BZ, Egypt (Arab Republic of)\n",
    "6CZ, Syrian Arab Republic\n",
    "6JZ, Mexico\n",
    "6NZ, Korea (Republic of)\n",
    "6OZ, Somali Democratic Republic\n",
    "6SZ, Pakistan (Islamic Republic of)\n",
    "6UZ, Sudan (Republic of the)\n",
    "6WZ, Senegal (Republic of)\n",
    "6XZ, Madagascar (Republic of)\n",
    "6YZ, Jamaica\n",
    "6ZZ, Liberia (Republic of)\n",
    "7IZ, Indonesia (Republic of)\n",
    "7NZ, Japan\n",
    "7OZ, Yemen (Republic of)\n",
    "7PZ, Lesotho (Kingdom of)\n",
    "7QZ, Malawi\n",
    "7RZ, Algeria (People's Democratic Republic of)\n",
    "7SZ, Sweden\n",
    "7YZ, Algeria (People's Democratic Republic of)\n",
    "7ZZ, Saudi Arabia (Kingdom of)\n",
    "8IZ, Indonesia (Republic of)\n",
    "8NZ, Japan\n",
    "8OZ, Botswana (Republic of)\n",
    "8PZ, Barbados\n",
    "8QZ, Maldives (Republic of)\n",
    "8RZ, Guyana\n",
    "8SZ, Sweden\n",
    "8YZ, India (Republic of)\n",
    "8ZZ, Saudi Arabia (Kingdom of)\n",
    "9AZ, Croatia (Republic of)\n",
    "9DZ, Iran (Islamic Republic of)\n",
    "9FZ, Ethiopia (Federal Democratic Republic of)\n",
    "9GZ, Ghana\n",
    "9HZ, Malta\n",
    "9JZ, Zambia (Republic of)\n",
    "9KZ, Kuwait (State of)\n",
    "9LZ, Sierra Leone\n",
    "9MZ, Malaysia\n",
    "9NZ, Nepal\n",
    "9TZ, Democratic Republic of the Congo\n",
    "9UZ, Burundi (Republic of)\n",
    "9VZ, Singapore (Republic of)\n",
    "9WZ, Malaysia\n",
    "9XZ, Rwandese Republic\n",
    "9ZZ, Trinidad and Tobago\n",
    "A2Z, Botswana (Republic of)\n",
    "A3Z, Tonga (Kingdom of)\n",
    "A4Z, Oman (Sultanate of)\n",
    "A5Z, Bhutan (Kingdom of)\n",
    "A6Z, United Arab Emirates\n",
    "A7Z, Qatar (State of)\n",
    "A8Z, Liberia (Republic of)\n",
    "A9Z, Bahrain (State of)\n",
    "ALZ, United States of America\n",
    "AOZ, Spain\n",
    "ASZ, Pakistan (Islamic Republic of)\n",
    "AWZ, India (Republic of)\n",
    "AXZ, Australia\n",
    "AZZ, Argentine Republic\n",
    "BZZ, China (People's Republic of)\n",
    "C2Z, Nauru (Republic of)\n",
    "C3Z, Andorra (Principality of)\n",
    "C4Z, Cyprus (Republic of)\n",
    "C5Z, Gambia (Republic of the)\n",
    "C6Z, Bahamas (Commonwealth of the)\n",
    "C7Z, World Meteorological Organization\n",
    "C9Z, Mozambique (Republic of)\n",
    "CEZ, Chile\n",
    "CKZ, Canada\n",
    "CMZ, Cuba\n",
    "CNZ, Morocco (Kingdom of)\n",
    "COZ, Cuba\n",
    "CPZ, Bolivia (Republic of)\n",
    "CUZ, Portugal\n",
    "CXZ, Uruguay (Eastern Republic of)\n",
    "CZZ, Canada\n",
    "D3Z, Angola (Republic of)\n",
    "D4Z, Cape Verde (Republic of)\n",
    "D5Z, Liberia (Republic of)\n",
    "D6Z, Comoros (Islamic Federal Republic of the)\n",
    "D9Z, Korea (Republic of)\n",
    "DRZ, Germany (Federal Republic of)\n",
    "DTZ, Korea (Republic of)\n",
    "DZZ, Philippines (Republic of the)\n",
    "E2Z, Thailand\n",
    "E3Z, Eritrea\n",
    "E4Z, Palestinian Authority\n",
    "E5Z, New Zealand - Cook Islands                      \t(WRC-07)\n",
    "E6Z, New Zealand - Niue\n",
    "E7Z, Bosnia and Herzegovina (Republic of)         \t(WRC-07)\n",
    "EHZ, Spain\n",
    "EJZ, Ireland\n",
    "EKZ, Armenia (Republic of)\n",
    "ELZ, Liberia (Republic of)\n",
    "EOZ, Ukraine\n",
    "EQZ, Iran (Islamic Republic of)\n",
    "ERZ, Moldova (Republic of)\n",
    "ESZ, Estonia (Republic of)\n",
    "ETZ, Ethiopia (Federal Democratic Republic of)\n",
    "EWZ, Belarus (Republic of)\n",
    "EXZ, Kyrgyz Republic\n",
    "EYZ, Tajikistan (Republic of)\n",
    "EZZ, Turkmenistan\n",
    "FZZ, France\n",
    "GZZ, United Kingdom of Great Britain and Northern Ireland\n",
    "H2Z, Cyprus (Republic of)\n",
    "H3Z, Panama (Republic of)\n",
    "H4Z, Solomon Islands\n",
    "H7Z, Nicaragua\n",
    "H9Z, Panama (Republic of)\n",
    "HAZ, Hungary (Republic of)\n",
    "HBZ, Switzerland (Confederation of)\n",
    "HDZ, Ecuador\n",
    "HEZ, Switzerland (Confederation of)\n",
    "HFZ, Poland (Republic of)\n",
    "HGZ, Hungary (Republic of)\n",
    "HHZ, Haiti (Republic of)\n",
    "HIZ, Dominican Republic\n",
    "HKZ, Colombia (Republic of)\n",
    "HLZ, Korea (Republic of)\n",
    "HMZ, Democratic People's Republic of Korea\n",
    "HNZ, Iraq (Republic of)\n",
    "HPZ, Panama (Republic of)\n",
    "HRZ, Honduras (Republic of)\n",
    "HSZ, Thailand\n",
    "HTZ, Nicaragua\n",
    "HUZ, El Salvador (Republic of)\n",
    "HVZ, Vatican City State\n",
    "HYZ, France\n",
    "HZZ, Saudi Arabia (Kingdom of)\n",
    "IZZ, Italy\n",
    "J2Z, Djibouti (Republic of)\n",
    "J3Z, Grenada\n",
    "J4Z, Greece\n",
    "J5Z, Guinea-Bissau (Republic of)\n",
    "J6Z, Saint Lucia\n",
    "J7Z, Dominica (Commonwealth of)\n",
    "J8Z, Saint Vincent and the Grenadines\n",
    "JSZ, Japan\n",
    "JVZ, Mongolia\n",
    "JXZ, Norway\n",
    "JYZ, Jordan (Hashemite Kingdom of)\n",
    "JZZ, Indonesia (Republic of)\n",
    "KZZ, United States of America\n",
    "L9Z, Argentine Republic\n",
    "LNZ, Norway\n",
    "LWZ, Argentine Republic\n",
    "LXZ, Luxembourg\n",
    "LYZ, Lithuania (Republic of)\n",
    "LZZ, Bulgaria (Republic of)\n",
    "MZZ, United Kingdom of Great Britain and Northern Ireland\n",
    "NZZ, United States of America\n",
    "OCZ, Peru\n",
    "ODZ, Lebanon\n",
    "OEZ, Austria\n",
    "OJZ, Finland\n",
    "OLZ, Czech Republic\n",
    "OMZ, Slovak Republic\n",
    "OTZ, Belgium\n",
    "OZZ, Denmark\n",
    "P2Z, Papua New Guinea\n",
    "P3Z, Cyprus (Republic of)\n",
    "P4Z, Netherlands (Kingdom of the) - Aruba\n",
    "P9Z, Democratic People's Republic of Korea\n",
    "PIZ, Netherlands (Kingdom of the)\n",
    "PJZ, Netherlands (Kingdom of the) - Netherlands Caribbean\n",
    "POZ, Indonesia (Republic of)\n",
    "PYZ, Brazil (Federative Republic of)\n",
    "PZZ, Suriname (Republic of)\n",
    "RZZ, Russian Federation\n",
    "S3Z, Bangladesh (People's Republic of)\n",
    "S5Z, Slovenia (Republic of)\n",
    "S6Z, Singapore (Republic of)\n",
    "S7Z, Seychelles (Republic of)\n",
    "S8Z, South Africa (Republic of)\n",
    "S9Z, Sao Tome and Principe (Democratic Republic of)\n",
    "SMZ, Sweden\n",
    "SRZ, Poland (Republic of)\n",
    "SSM, Egypt (Arab Republic of)\n",
    "STZ, Sudan (Republic of the)\n",
    "SUZ, Egypt (Arab Republic of)\n",
    "SZZ, Greece\n",
    "T2Z, Tuvalu\n",
    "T3Z, Kiribati (Republic of)\n",
    "T4Z, Cuba\n",
    "T5Z, Somali Democratic Republic\n",
    "T6Z, Afghanistan (Islamic State of)\n",
    "T7Z, San Marino (Republic of)\n",
    "T8Z, Palau (Republic of)\n",
    "TCZ, Turkey\n",
    "TDZ, Guatemala (Republic of)\n",
    "TEZ, Costa Rica\n",
    "TFZ, Iceland\n",
    "TGZ, Guatemala (Republic of)\n",
    "THZ, France\n",
    "TIZ, Costa Rica\n",
    "TJZ, Cameroon (Republic of)\n",
    "TKZ, France\n",
    "TLZ, Central African Republic\n",
    "TMZ, France\n",
    "TNZ, Congo (Republic of the)\n",
    "TQZ, France\n",
    "TRZ, Gabonese Republic\n",
    "TSZ, Tunisia\n",
    "TTZ, Chad (Republic of)\n",
    "TUZ, Côte d'Ivoire (Republic of)\n",
    "TXZ, France\n",
    "TYZ, Benin (Republic of)\n",
    "TZZ, Mali (Republic of)\n",
    "UIZ, Russian Federation\n",
    "UMZ, Uzbekistan (Republic of)\n",
    "UQZ, Kazakhstan (Republic of)\n",
    "UZZ, Ukraine\n",
    "V2Z, Antigua and Barbuda\n",
    "V3Z, Belize\n",
    "V4Z, Saint Kitts and Nevis\n",
    "V5Z, Namibia (Republic of)\n",
    "V6Z, Micronesia (Federated States of)\n",
    "V7Z, Marshall Islands (Republic of the)\n",
    "V8Z, Brunei Darussalam\n",
    "VGZ, Canada\n",
    "VNZ, Australia\n",
    "VOZ, Canada\n",
    "VQZ, United Kingdom of Great Britain and Northern Ireland\n",
    "VRZ, China (People's Republic of) - Hong Kong\n",
    "VSZ, United Kingdom of Great Britain and Northern Ireland\n",
    "VWZ, India (Republic of)\n",
    "VYZ, Canada\n",
    "VZZ, Australia\n",
    "WZZ, United States of America\n",
    "XIZ, Mexico\n",
    "XOZ, Canada\n",
    "XPZ, Denmark\n",
    "XRZ, Chile\n",
    "XSZ, China (People's Republic of)\n",
    "XTZ, Burkina Faso\n",
    "XUZ, Cambodia (Kingdom of)\n",
    "XVZ, Viet Nam (Socialist Republic of)\n",
    "XWZ, Lao People's Democratic Republic\n",
    "XXZ, China (People's Republic of) - Macao         \t(WRC-07)\n",
    "XZZ, Myanmar (Union of)\n",
    "Y9Z, Germany (Federal Republic of)\n",
    "YAZ, Afghanistan (Islamic State of)\n",
    "YHZ, Indonesia (Republic of)\n",
    "YIZ, Iraq (Republic of)\n",
    "YJZ, Vanuatu (Republic of)\n",
    "YKZ, Syrian Arab Republic\n",
    "YLZ, Latvia (Republic of)\n",
    "YMZ, Turkey\n",
    "YNZ, Nicaragua\n",
    "YRZ, Romania\n",
    "YSZ, El Salvador (Republic of)\n",
    "YUZ, Serbia (Republic of)                                   \t(WRC-07)\n",
    "YYZ, Venezuela (Republic of)\n",
    "Z2Z, Zimbabwe (Republic of)\n",
    "Z3Z, The Former Yugoslav Republic of Macedonia\n",
    "Z8Z, South Sudan (Republic of)\n",
    "ZAZ, Albania (Republic of)\n",
    "ZJZ, United Kingdom of Great Britain and Northern Ireland\n",
    "ZMZ, New Zealand\n",
    "ZOZ, United Kingdom of Great Britain and Northern Ireland\n",
    "ZPZ, Paraguay (Republic of)\n",
    "ZQZ, United Kingdom of Great Britain and Northern Ireland\n",
    "ZUZ, South Africa (Republic of)\n",
    "ZZZ, Brazil (Federative Republic of)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#..... Other code...\n",
    "#Country lookup code\n",
    "\n",
    "# Helper functions for looking up the call signs\n",
    "\n",
    "def lookupCountry(sign, prefixes):\n",
    "    pos = bisect.bisect_left(prefixes, sign)\n",
    "    return prefixes[pos].split(\",\")[1]\n",
    "\n",
    "\n",
    "def loadCallSignTable():\n",
    "    f = open(\"callsign_tbl_sorted.txt\", \"r\")\n",
    "    return f.readlines()\n",
    "\n",
    "# Lookup the locations of the call signs on the\n",
    "# RDD contactCounts. We load a list of call sign\n",
    "# prefixes to country code to support this lookup.\n",
    "signPrefixes = loadCallSignTable()\n",
    "\n",
    "\n",
    "def processSignCount(sign_count, signPrefixes):\n",
    "    country = lookupCountry(sign_count[0], signPrefixes)\n",
    "    count = sign_count[1]\n",
    "    return (country, count)\n",
    "\n",
    "contactCounts = sc.parallelize([[\"ZMZ\", 1], [\"ZMZ\", 3]])\n",
    "\n",
    "countryContactCounts = (contactCounts\n",
    "                        .map(lambda signCount: processSignCount(signCount, signPrefixes))\n",
    "                        .reduceByKey((lambda x, y: x + y)))\n",
    "\n",
    "#countryContactCounts.saveAsTextFile(\"tmp/countries.txt\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting beerSales.txt\n"
     ]
    }
   ],
   "source": [
    "%%writefile beerSales.txt\n",
    "Week\tPRICE12PK\tPRICE18PK\tPRICE30PK\tCASES12PK\tCASES18PK\tCASES30PK\n",
    "1\t19.98\t14.10\t15.19\t223.5\t439\t55.00\n",
    "2\t19.98\t18.65\t15.19\t215.0\t98\t66.75\n",
    "3\t19.98\t18.65\t13.87\t227.5\t70\t242.00\n",
    "4\t19.98\t18.65\t12.83\t244.5\t52\t488.50\n",
    "5\t19.98\t18.65\t13.16\t313.5\t64\t308.75\n",
    "6\t19.98\t18.65\t15.19\t279.0\t72\t111.75\n",
    "7\t19.98\t18.65\t13.92\t238.0\t47\t252.50\n",
    "8\t20.10\t18.73\t14.42\t315.5\t85\t221.25\n",
    "9\t20.12\t18.75\t13.83\t217.0\t59\t245.25\n",
    "10\t20.13\t18.75\t14.50\t209.5\t63\t148.50\n",
    "11\t20.14\t18.75\t13.87\t227.0\t57\t229.75\n",
    "12\t20.12\t18.75\t13.64\t216.5\t54\t312.00\n",
    "13\t20.12\t13.87\t14.31\t169.0\t404\t96.75\n",
    "14\t20.13\t14.27\t13.85\t178.0\t380\t123.25\n",
    "15\t20.14\t18.76\t14.20\t301.5\t65\t200.50\n",
    "16\t20.14\t18.77\t13.64\t266.5\t40\t359.75\n",
    "17\t20.13\t13.87\t14.33\t182.5\t456\t113.50\n",
    "18\t20.13\t14.14\t13.14\t159.0\t176\t136.50\n",
    "19\t20.13\t18.76\t13.81\t285.5\t61\t225.50\n",
    "20\t20.13\t18.72\t15.19\t360.0\t91\t122.25\n",
    "21\t20.13\t18.76\t13.13\t263.0\t59\t443.75\n",
    "22\t19.18\t18.76\t13.63\t443.5\t83\t322.75\n",
    "23\t14.78\t18.74\t15.19\t1101.5\t41\t53.00\n",
    "24\t16.04\t18.75\t13.89\t814.0\t47\t140.75\n",
    "25\t20.12\t18.75\t14.28\t365.0\t84\t210.75\n",
    "26\t19.75\t18.75\t15.19\t510.0\t85\t110.50\n",
    "27\t19.65\t18.75\t13.12\t580.5\t116\t568.25\n",
    "28\t19.69\t13.79\t13.78\t251.0\t544\t115.50\n",
    "29\t20.12\t13.49\t15.19\t237.0\t890\t58.75\n",
    "30\t20.12\t14.89\t15.19\t302.5\t371\t77.25\n",
    "31\t20.13\t13.94\t15.19\t229.5\t557\t66.25\n",
    "32\t20.14\t13.67\t15.19\t188.5\t775\t50.00\n",
    "33\t15.14\t14.43\t15.19\t795.5\t236\t46.50\n",
    "34\t14.33\t18.75\t15.19\t1556.5\t43\t65.75\n",
    "35\t16.24\t18.22\t13.14\t807.5\t63\t252.75\n",
    "36\t19.93\t14.06\t13.45\t243.0\t469\t179.00\n",
    "37\t21.06\t14.43\t13.00\t201.5\t335\t226.25\n",
    "38\t21.19\t19.48\t13.60\t294.0\t75\t288.50\n",
    "39\t21.23\t15.15\t14.46\t220.5\t461\t114.25\n",
    "40\t20.12\t13.79\t14.94\t255.5\t817\t70.00\n",
    "41\t14.73\t14.31\t15.19\t920.5\t200\t47.75\n",
    "42\t14.57\t19.50\t15.19\t730.0\t32\t98.75\n",
    "43\t15.94\t13.85\t15.19\t262.5\t460\t77.00\n",
    "44\t20.70\t14.23\t13.43\t209.5\t751\t160.50\n",
    "45\t19.57\t19.31\t14.37\t283.0\t70\t143.50\n",
    "46\t19.60\t19.29\t15.19\t262.5\t80\t133.00\n",
    "47\t19.94\t13.76\t15.19\t310.0\t523\t68.75\n",
    "48\t21.28\t13.45\t15.19\t278.5\t741\t81.75\n",
    "49\t14.56\t15.13\t15.19\t741.5\t130\t56.25\n",
    "50\t14.39\t19.43\t15.19\t1316.0\t69\t68.75\n",
    "51\t16.81\t13.26\t15.19\t449.0\t493\t49.25\n",
    "52\t19.86\t13.92\t15.19\t505.0\t814\t76.50"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df = sc.textFile(\"beerSales.txt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "Py4JJavaError",
     "evalue": "An error occurred while calling z:org.apache.spark.api.python.PythonRDD.collectAndServe.\n: org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 12.0 failed 1 times, most recent failure: Lost task 0.0 in stage 12.0 (TID 42, localhost): org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n  File \"/Users/JingXu/Downloads/spark-1.4.1-bin-hadoop2.6/python/lib/pyspark.zip/pyspark/worker.py\", line 111, in main\n    process()\n  File \"/Users/JingXu/Downloads/spark-1.4.1-bin-hadoop2.6/python/lib/pyspark.zip/pyspark/worker.py\", line 106, in process\n    serializer.dump_stream(func(split_index, iterator), outfile)\n  File \"/Users/JingXu/Downloads/spark-1.4.1-bin-hadoop2.6/python/pyspark/rdd.py\", line 2330, in pipeline_func\n    return func(split, prev_func(split, iterator))\n  File \"/Users/JingXu/Downloads/spark-1.4.1-bin-hadoop2.6/python/pyspark/rdd.py\", line 2330, in pipeline_func\n    return func(split, prev_func(split, iterator))\n  File \"/Users/JingXu/Downloads/spark-1.4.1-bin-hadoop2.6/python/pyspark/rdd.py\", line 316, in func\n    return f(iterator)\n  File \"/Users/JingXu/Downloads/spark-1.4.1-bin-hadoop2.6/python/pyspark/rdd.py\", line 1846, in combine\n    merger.mergeValues(iterator)\n  File \"/Users/JingXu/Downloads/spark-1.4.1-bin-hadoop2.6/python/lib/pyspark.zip/pyspark/shuffle.py\", line 266, in mergeValues\n    for k, v in iterator:\n  File \"/Users/JingXu/Downloads/spark-1.4.1-bin-hadoop2.6/python/pyspark/rdd.py\", line 699, in <lambda>\n    return self.map(lambda x: (f(x), x)).groupByKey(numPartitions)\nTypeError: 'str' object is not callable\n\n\tat org.apache.spark.api.python.PythonRDD$$anon$1.read(PythonRDD.scala:138)\n\tat org.apache.spark.api.python.PythonRDD$$anon$1.<init>(PythonRDD.scala:179)\n\tat org.apache.spark.api.python.PythonRDD.compute(PythonRDD.scala:97)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:277)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:244)\n\tat org.apache.spark.api.python.PairwiseRDD.compute(PythonRDD.scala:315)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:277)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:244)\n\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:70)\n\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:41)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:70)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:213)\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)\n\tat java.lang.Thread.run(Thread.java:745)\n\nDriver stacktrace:\n\tat org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1273)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1264)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1263)\n\tat scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59)\n\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:47)\n\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1263)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:730)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:730)\n\tat scala.Option.foreach(Option.scala:236)\n\tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:730)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1457)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1418)\n\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:48)\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mPy4JJavaError\u001b[0m                             Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-15-62fce454739b>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0moperator\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m \u001b[0mdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgroupBy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'CASES18PK'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmean\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/Users/JingXu/Downloads/spark-1.4.1-bin-hadoop2.6/python/pyspark/rdd.pyc\u001b[0m in \u001b[0;36mmean\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1129\u001b[0m         \u001b[0;36m2.0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1130\u001b[0m         \"\"\"\n\u001b[0;32m-> 1131\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstats\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmean\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1132\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1133\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mvariance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/JingXu/Downloads/spark-1.4.1-bin-hadoop2.6/python/pyspark/rdd.pyc\u001b[0m in \u001b[0;36mstats\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    992\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mleft_counter\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmergeStats\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mright_counter\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    993\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 994\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmapPartitions\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mlambda\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mStatCounter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreduce\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mredFunc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    995\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    996\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mhistogram\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbuckets\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/JingXu/Downloads/spark-1.4.1-bin-hadoop2.6/python/pyspark/rdd.pyc\u001b[0m in \u001b[0;36mreduce\u001b[0;34m(self, f)\u001b[0m\n\u001b[1;32m    781\u001b[0m             \u001b[0;32myield\u001b[0m \u001b[0mreduce\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0miterator\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minitial\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    782\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 783\u001b[0;31m         \u001b[0mvals\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmapPartitions\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfunc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcollect\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    784\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mvals\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    785\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mreduce\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvals\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/JingXu/Downloads/spark-1.4.1-bin-hadoop2.6/python/pyspark/rdd.pyc\u001b[0m in \u001b[0;36mcollect\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    755\u001b[0m         \"\"\"\n\u001b[1;32m    756\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mSCCallSiteSync\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcontext\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mcss\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 757\u001b[0;31m             \u001b[0mport\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mctx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jvm\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mPythonRDD\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcollectAndServe\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jrdd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrdd\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    758\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_load_from_socket\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mport\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jrdd_deserializer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    759\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/JingXu/Downloads/spark-1.4.1-bin-hadoop2.6/python/lib/py4j-0.8.2.1-src.zip/py4j/java_gateway.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m    536\u001b[0m         \u001b[0manswer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgateway_client\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend_command\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcommand\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    537\u001b[0m         return_value = get_return_value(answer, self.gateway_client,\n\u001b[0;32m--> 538\u001b[0;31m                 self.target_id, self.name)\n\u001b[0m\u001b[1;32m    539\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    540\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mtemp_arg\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtemp_args\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/JingXu/Downloads/spark-1.4.1-bin-hadoop2.6/python/lib/py4j-0.8.2.1-src.zip/py4j/protocol.py\u001b[0m in \u001b[0;36mget_return_value\u001b[0;34m(answer, gateway_client, target_id, name)\u001b[0m\n\u001b[1;32m    298\u001b[0m                 raise Py4JJavaError(\n\u001b[1;32m    299\u001b[0m                     \u001b[0;34m'An error occurred while calling {0}{1}{2}.\\n'\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 300\u001b[0;31m                     format(target_id, '.', name), value)\n\u001b[0m\u001b[1;32m    301\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    302\u001b[0m                 raise Py4JError(\n",
      "\u001b[0;31mPy4JJavaError\u001b[0m: An error occurred while calling z:org.apache.spark.api.python.PythonRDD.collectAndServe.\n: org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 12.0 failed 1 times, most recent failure: Lost task 0.0 in stage 12.0 (TID 42, localhost): org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n  File \"/Users/JingXu/Downloads/spark-1.4.1-bin-hadoop2.6/python/lib/pyspark.zip/pyspark/worker.py\", line 111, in main\n    process()\n  File \"/Users/JingXu/Downloads/spark-1.4.1-bin-hadoop2.6/python/lib/pyspark.zip/pyspark/worker.py\", line 106, in process\n    serializer.dump_stream(func(split_index, iterator), outfile)\n  File \"/Users/JingXu/Downloads/spark-1.4.1-bin-hadoop2.6/python/pyspark/rdd.py\", line 2330, in pipeline_func\n    return func(split, prev_func(split, iterator))\n  File \"/Users/JingXu/Downloads/spark-1.4.1-bin-hadoop2.6/python/pyspark/rdd.py\", line 2330, in pipeline_func\n    return func(split, prev_func(split, iterator))\n  File \"/Users/JingXu/Downloads/spark-1.4.1-bin-hadoop2.6/python/pyspark/rdd.py\", line 316, in func\n    return f(iterator)\n  File \"/Users/JingXu/Downloads/spark-1.4.1-bin-hadoop2.6/python/pyspark/rdd.py\", line 1846, in combine\n    merger.mergeValues(iterator)\n  File \"/Users/JingXu/Downloads/spark-1.4.1-bin-hadoop2.6/python/lib/pyspark.zip/pyspark/shuffle.py\", line 266, in mergeValues\n    for k, v in iterator:\n  File \"/Users/JingXu/Downloads/spark-1.4.1-bin-hadoop2.6/python/pyspark/rdd.py\", line 699, in <lambda>\n    return self.map(lambda x: (f(x), x)).groupByKey(numPartitions)\nTypeError: 'str' object is not callable\n\n\tat org.apache.spark.api.python.PythonRDD$$anon$1.read(PythonRDD.scala:138)\n\tat org.apache.spark.api.python.PythonRDD$$anon$1.<init>(PythonRDD.scala:179)\n\tat org.apache.spark.api.python.PythonRDD.compute(PythonRDD.scala:97)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:277)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:244)\n\tat org.apache.spark.api.python.PairwiseRDD.compute(PythonRDD.scala:315)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:277)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:244)\n\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:70)\n\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:41)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:70)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:213)\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)\n\tat java.lang.Thread.run(Thread.java:745)\n\nDriver stacktrace:\n\tat org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1273)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1264)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1263)\n\tat scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59)\n\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:47)\n\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1263)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:730)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:730)\n\tat scala.Option.foreach(Option.scala:236)\n\tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:730)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1457)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1418)\n\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:48)\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "from operator import add\n",
    "from pyspark import SparkContext\n",
    "import operator\n",
    "\n",
    "df.groupBy('CASES18PK').mean().show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "My Spark install kept throwing me errors when I attempted to calculate the mean of the 'CASES18PK' column. Altered the bearSales.txt file and continued by hand."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df = sc.textFile(\"beerSales.txt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "256\n"
     ]
    }
   ],
   "source": [
    "df = df.map(lambda x: x)\n",
    "values = df.collect()\n",
    "mean = sum(values)/len(values)\n",
    "print mean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.57692307692\n"
     ]
    }
   ],
   "source": [
    "#mean = 256\n",
    "total = 0\n",
    "differences = 0\n",
    "\n",
    "with open('beerSales.txt', 'r') as beer:\n",
    "    for line in beer:\n",
    "        differences += float(abs(int(line)-256)/(int(line)))\n",
    "\n",
    "avg_differences = differences/52        \n",
    "\n",
    "print avg_differences"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ET16"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Faced similar difficulty with my Spark install, had no time to attempt to reinstall a different version, calculated the answer by hand"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df = sc.textFile(\"beerSales.txt\")\n",
    "df = df.map(lambda x: np.log(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "Py4JJavaError",
     "evalue": "An error occurred while calling z:org.apache.spark.api.python.PythonRDD.collectAndServe.\n: org.apache.spark.SparkException: Job aborted due to stage failure: Task 1 in stage 28.0 failed 1 times, most recent failure: Lost task 1.0 in stage 28.0 (TID 73, localhost): org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n  File \"/Users/JingXu/Downloads/spark-1.4.1-bin-hadoop2.6/python/lib/pyspark.zip/pyspark/worker.py\", line 111, in main\n    process()\n  File \"/Users/JingXu/Downloads/spark-1.4.1-bin-hadoop2.6/python/lib/pyspark.zip/pyspark/worker.py\", line 106, in process\n    serializer.dump_stream(func(split_index, iterator), outfile)\n  File \"/Users/JingXu/Downloads/spark-1.4.1-bin-hadoop2.6/python/lib/pyspark.zip/pyspark/serializers.py\", line 263, in dump_stream\n    vs = list(itertools.islice(iterator, batch))\n  File \"<ipython-input-34-2264b9fb2d12>\", line 2, in <lambda>\nTypeError: ufunc 'log' not supported for the input types, and the inputs could not be safely coerced to any supported types according to the casting rule ''safe''\n\n\tat org.apache.spark.api.python.PythonRDD$$anon$1.read(PythonRDD.scala:138)\n\tat org.apache.spark.api.python.PythonRDD$$anon$1.<init>(PythonRDD.scala:179)\n\tat org.apache.spark.api.python.PythonRDD.compute(PythonRDD.scala:97)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:277)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:244)\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:63)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:70)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:213)\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)\n\tat java.lang.Thread.run(Thread.java:745)\n\nDriver stacktrace:\n\tat org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1273)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1264)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1263)\n\tat scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59)\n\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:47)\n\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1263)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:730)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:730)\n\tat scala.Option.foreach(Option.scala:236)\n\tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:730)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1457)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1418)\n\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:48)\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mPy4JJavaError\u001b[0m                             Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-37-954a44a490a5>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mlogvalues\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcollect\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/Users/JingXu/Downloads/spark-1.4.1-bin-hadoop2.6/python/pyspark/rdd.pyc\u001b[0m in \u001b[0;36mcollect\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    755\u001b[0m         \"\"\"\n\u001b[1;32m    756\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mSCCallSiteSync\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcontext\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mcss\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 757\u001b[0;31m             \u001b[0mport\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mctx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jvm\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mPythonRDD\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcollectAndServe\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jrdd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrdd\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    758\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_load_from_socket\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mport\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jrdd_deserializer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    759\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/JingXu/Downloads/spark-1.4.1-bin-hadoop2.6/python/lib/py4j-0.8.2.1-src.zip/py4j/java_gateway.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m    536\u001b[0m         \u001b[0manswer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgateway_client\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend_command\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcommand\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    537\u001b[0m         return_value = get_return_value(answer, self.gateway_client,\n\u001b[0;32m--> 538\u001b[0;31m                 self.target_id, self.name)\n\u001b[0m\u001b[1;32m    539\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    540\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mtemp_arg\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtemp_args\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/JingXu/Downloads/spark-1.4.1-bin-hadoop2.6/python/lib/py4j-0.8.2.1-src.zip/py4j/protocol.py\u001b[0m in \u001b[0;36mget_return_value\u001b[0;34m(answer, gateway_client, target_id, name)\u001b[0m\n\u001b[1;32m    298\u001b[0m                 raise Py4JJavaError(\n\u001b[1;32m    299\u001b[0m                     \u001b[0;34m'An error occurred while calling {0}{1}{2}.\\n'\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 300\u001b[0;31m                     format(target_id, '.', name), value)\n\u001b[0m\u001b[1;32m    301\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    302\u001b[0m                 raise Py4JError(\n",
      "\u001b[0;31mPy4JJavaError\u001b[0m: An error occurred while calling z:org.apache.spark.api.python.PythonRDD.collectAndServe.\n: org.apache.spark.SparkException: Job aborted due to stage failure: Task 1 in stage 28.0 failed 1 times, most recent failure: Lost task 1.0 in stage 28.0 (TID 73, localhost): org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n  File \"/Users/JingXu/Downloads/spark-1.4.1-bin-hadoop2.6/python/lib/pyspark.zip/pyspark/worker.py\", line 111, in main\n    process()\n  File \"/Users/JingXu/Downloads/spark-1.4.1-bin-hadoop2.6/python/lib/pyspark.zip/pyspark/worker.py\", line 106, in process\n    serializer.dump_stream(func(split_index, iterator), outfile)\n  File \"/Users/JingXu/Downloads/spark-1.4.1-bin-hadoop2.6/python/lib/pyspark.zip/pyspark/serializers.py\", line 263, in dump_stream\n    vs = list(itertools.islice(iterator, batch))\n  File \"<ipython-input-34-2264b9fb2d12>\", line 2, in <lambda>\nTypeError: ufunc 'log' not supported for the input types, and the inputs could not be safely coerced to any supported types according to the casting rule ''safe''\n\n\tat org.apache.spark.api.python.PythonRDD$$anon$1.read(PythonRDD.scala:138)\n\tat org.apache.spark.api.python.PythonRDD$$anon$1.<init>(PythonRDD.scala:179)\n\tat org.apache.spark.api.python.PythonRDD.compute(PythonRDD.scala:97)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:277)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:244)\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:63)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:70)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:213)\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)\n\tat java.lang.Thread.run(Thread.java:745)\n\nDriver stacktrace:\n\tat org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1273)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1264)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1263)\n\tat scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59)\n\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:47)\n\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1263)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:730)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:730)\n\tat scala.Option.foreach(Option.scala:236)\n\tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:730)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1457)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1418)\n\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:48)\n"
     ]
    }
   ],
   "source": [
    "logvalues = df.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "58.3653846154\n"
     ]
    }
   ],
   "source": [
    "#mean = 256\n",
    "total = 0\n",
    "differences = 0\n",
    "\n",
    "with open('beerSales.txt', 'r') as beer:\n",
    "    for line in beer:\n",
    "        line = np.log(int(line))\n",
    "        differences += float(abs(int(line)-256)/(int(line)))\n",
    "\n",
    "avg_differences = differences/52        \n",
    "\n",
    "print avg_differences"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ET17"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Faced similar difficulty with my Spark install, had no time to attempt to reinstall a different version, calculated the answer by hand"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.57692307692\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
