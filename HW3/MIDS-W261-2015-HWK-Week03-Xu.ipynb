{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# DATSCIW261 ASSIGNMENT #3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Jing Xu\n",
    "\n",
    "jaling@gmail.com\n",
    "\n",
    "W261-3\n",
    "\n",
    "DATSCIW261 Assignment #2\n",
    "\n",
    "1/23/16"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**HW3.0. What is a merge sort? Where is it used in Hadoop? How is a combiner function in the context of Hadoop?  Give an example where it can be used and justify why it should be used in the context of this problem. What is the Hadoop shuffle?**\n",
    "\n",
    "Merge sort is an algorithm used to sort n amount of sorted lists. It works by looking at the front of each list and always picking the smallest element to move to a temporary array, afterwards moving the pointer on the list from which the most recent smallest element over one position. If m = sum of the lengths of all the lists, then the algorithm generally runs in O(m log n) time, and can use a heap-based priority queue. Hadoop uses merge sort in the shuffle steps: partition, sort, and combine. On the map side, mapper outputs are spilled onto disk and a merge sort is run on the keys to create a single, partitioned file. Combiners are then run on the sorted partition file. Combiners in the context of Hadoop are functions that operate between the map and reduce stages that take the intermediate outputs from the mappers and aggregates the results based on keys. The output from the combiner is then copied over to the reducer machine where they are merge sorted again with the output from other combiners assigned to that reducer machine, and then that result is streamed to the reduce function.\n",
    "\n",
    "One example of an effective use of combiners is when computing the average (mean) transaction amount per customer from log files that record only individual transactions. It would be efficient to have the averages for each customer emitted by a mapper already calculated for that mapper output before shuffling and then sending to the reducers. In this case, we can't use the reducer as a combiner because mean isn't an associative operation and because the output of the mapper needs to match the input for the reducers (Output of the mapper currently would be in form (key, value) while the reducer would need (key, [value, count]) to compute the average.  First, we need the mapper to emit a pair of values (sum, count) for each string key, and then a combiner can be used to compute an intermediary sum all the sums for a key and sum all the counts for the key. The combiner output is then sent to the reducer in an appropriate format to be used to calculate average while also providing a more efficient shuffling step to lower overall runtime.\n",
    "\n",
    "The Hadoop shuffle is comprised of the partition, sort, and combine steps described in detail above. It is the part of the Hadoop MapReduce execution framework synchronization step that gathers mapper outputs, sorts according to combiner settings, and shuffles through partitioning settings intermediate data to the final reducer steps. These steps involve both in-memory buffering (map-side partitioning, sorting, and combining) and on-disk (when combiner outputs are shuffled to the reducers.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**HW3.1 Use Counters to do EDA (exploratory data analysis and to monitor progress)\n",
    "Counters are lightweight objects in Hadoop that allow you to keep track of system progress in both the map and reduce stages of processing. By default, Hadoop defines a number of standard counters in \"groups\"; these show up in the jobtracker webapp, giving you information such as \"Map input records\", \"Map output records\", etc.** \n",
    "\n",
    "**While processing information/data using MapReduce job, it is a challenge to monitor the progress of parallel threads running across nodes of distributed clusters. Moreover, it is also complicated to distinguish between the data that has been processed and the data which is yet to be processed. The MapReduce Framework offers a provision of user-defined Counters, which can be effectively utilized to monitor the progress of data across nodes of distributed clusters.**\n",
    "\n",
    "**The consumer complaints dataset consists of diverse consumer complaints, which have been reported across the United States regarding various types of loans. The dataset consists of records of the form:**\n",
    "\n",
    "**Complaint ID,Product,Sub-product,Issue,Sub-issue,State,ZIP code,Submitted via,Date received,Date sent to company,Company,Company response,Timely response?,Consumer disputed?**\n",
    "\n",
    "**User-defined Counters**\n",
    "\n",
    "**Now, letâ€™s use Hadoop Counters to identify the number of complaints pertaining to debt collection, mortgage and other categories (all other categories get lumped into this one) in the consumer complaints dataset. Basically produce the distribution of the Product column in this dataset using counters (limited to 3 counters here).**\n",
    "\n",
    "**Hadoop offers Job Tracker, an UI tool to determine the status and statistics of all jobs. Using the job tracker UI, developers can view the Counters that have been created. Screenshot your  job tracker UI as your job completes and include it here. Make sure that your user defined counters are visible.** "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting mapper.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile mapper.py\n",
    "#!/usr/bin/python\n",
    "## mapper.py\n",
    "## Author: Jing Xu\n",
    "## Description: mapper code for HW3.1\n",
    "\n",
    "import sys\n",
    "import csv\n",
    "\n",
    "reader = csv.reader(sys.stdin)\n",
    "# read input from STDIN (standard input)\n",
    "for line in reader:\n",
    "    # split the line into words\n",
    "    product = line[1].lower()\n",
    "#        product = csvreader[1].lower() #assign product variable\n",
    "    if product == 'debt collection': #check if product matches debt collection to increment counters\n",
    "        sys.stderr.write(\"reporter:counter:DebtCounter,Total,1\\n\")\n",
    "        print '%s\\t%s' % (product, 1)\n",
    "    elif line[1].lower() == 'mortgage': #check if product matches mortgage to increment counters\n",
    "        sys.stderr.write(\"reporter:counter:MortgageCounter,Total,1\\n\")\n",
    "        print '%s\\t%s' % (product, 1)\n",
    "    else: #all other categories grouped to \"other\"\n",
    "        sys.stderr.write(\"reporter:counter:OtherCounter,Total,1\\n\")\n",
    "        print '%s\\t%s' % ('other', 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting reducer.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile reducer.py\n",
    "#!/usr/bin/python\n",
    "## reducer.py\n",
    "## Author: Jing Xu\n",
    "## Description: reducer code for HW3.1\n",
    "\n",
    "from operator import itemgetter\n",
    "import sys\n",
    "\n",
    "current_word = None\n",
    "current_count = 0\n",
    "word = None\n",
    "\n",
    "# input comes from STDIN\n",
    "for line in sys.stdin:\n",
    "    # remove leading and trailing whitespace\n",
    "    line = line.strip()\n",
    "    # parse the input we got from mapper.py\n",
    "    word, count = line.split('\\t', 1)\n",
    "    # convert count (currently a string) to int\n",
    "    try:\n",
    "        count = int(count)\n",
    "    except ValueError:\n",
    "        # count was not a number, so silently\n",
    "        # ignore/discard this line\n",
    "        continue\n",
    "    if current_word == word:\n",
    "        current_count += count\n",
    "    else:\n",
    "        if current_word:\n",
    "            # write result to STDOUT\n",
    "            print '%s\\t%s' % (current_word, current_count)\n",
    "        current_count = count\n",
    "        current_word = word\n",
    "\n",
    "# output the last word if needed\n",
    "if current_word == word:\n",
    "    print '%s\\t%s' % (current_word, current_count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting mapreduce.sh\n"
     ]
    }
   ],
   "source": [
    "%%writefile mapreduce.sh\n",
    "## mapreduce.sh\n",
    "## Author: Jing Xu\n",
    "## Description: mapreduce bash script for HW3.1\n",
    "\n",
    "hdfs dfs -mkdir /user # create hdfs folder\n",
    "wait\n",
    "hdfs dfs -mkdir /user/jing # create hdfs folder\n",
    "wait\n",
    "hadoop fs -put Consumer_Complaints.csv /user/jing # upload local file to hdfs folder\n",
    "wait\n",
    "\n",
    "# hadoop command to run streaming mapreduce job\n",
    "hadoop jar /Users/JingXu/Documents/hadoop-2.6.3/share/hadoop/tools/lib/hadoop-streaming-2.6.3.jar \\\n",
    "-D mapred.job.name=\"Count Job via Streaming\" \\\n",
    "-file mapper.py    -mapper mapper.py \\\n",
    "-file reducer.py   -reducer reducer.py \\\n",
    "-input /user/jing/* -output /user/jing/test-output1/\n",
    "wait\n",
    "\n",
    "hadoop dfs -cat /user/jing/test-output1/part-00000 # print contents of the word count output file\n",
    "wait\n",
    "\n",
    "hdfs dfs -rmr /user/jing # remove hdfs folder\n",
    "hdfs dfs -rmr /user # remove hdfs folder\n",
    "rm part-00000 # delete results \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# give correct permissions\n",
    "!chmod a+x reducer.py\n",
    "!chmod a+x mapper.py\n",
    "!chmod a+x mapreduce.sh"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "16/02/01 16:20:55 WARN util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "16/02/01 16:20:57 WARN util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "16/02/01 16:20:59 WARN util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "16/02/01 16:21:01 WARN streaming.StreamJob: -file option is deprecated, please use generic option -files instead.\n",
      "16/02/01 16:21:01 WARN util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "packageJobJar: [mapper.py, reducer.py] [] /var/folders/zs/k144hqks281fbt0x68c_zj9m0000gp/T/streamjob4137158095172437633.jar tmpDir=null\n",
      "16/02/01 16:21:01 INFO Configuration.deprecation: session.id is deprecated. Instead, use dfs.metrics.session-id\n",
      "16/02/01 16:21:01 INFO jvm.JvmMetrics: Initializing JVM Metrics with processName=JobTracker, sessionId=\n",
      "16/02/01 16:21:02 INFO jvm.JvmMetrics: Cannot initialize JVM Metrics with processName=JobTracker, sessionId= - already initialized\n",
      "16/02/01 16:21:02 INFO mapred.FileInputFormat: Total input paths to process : 1\n",
      "16/02/01 16:21:02 INFO mapreduce.JobSubmitter: number of splits:1\n",
      "16/02/01 16:21:02 INFO Configuration.deprecation: mapred.job.name is deprecated. Instead, use mapreduce.job.name\n",
      "16/02/01 16:21:02 INFO mapreduce.JobSubmitter: Submitting tokens for job: job_local506106942_0001\n",
      "16/02/01 16:21:02 INFO mapred.LocalDistributedCacheManager: Localized file:/Users/JingXu/Dropbox/DataScience/W261/W261/HW3/mapper.py as file:/tmp/hadoop-JingXu/mapred/local/1454372462677/mapper.py\n",
      "16/02/01 16:21:02 INFO mapred.LocalDistributedCacheManager: Localized file:/Users/JingXu/Dropbox/DataScience/W261/W261/HW3/reducer.py as file:/tmp/hadoop-JingXu/mapred/local/1454372462678/reducer.py\n",
      "16/02/01 16:21:02 INFO mapreduce.Job: The url to track the job: http://localhost:8080/\n",
      "16/02/01 16:21:02 INFO mapred.LocalJobRunner: OutputCommitter set in config null\n",
      "16/02/01 16:21:02 INFO mapreduce.Job: Running job: job_local506106942_0001\n",
      "16/02/01 16:21:02 INFO mapred.LocalJobRunner: OutputCommitter is org.apache.hadoop.mapred.FileOutputCommitter\n",
      "16/02/01 16:21:03 INFO mapred.LocalJobRunner: Waiting for map tasks\n",
      "16/02/01 16:21:03 INFO mapred.LocalJobRunner: Starting task: attempt_local506106942_0001_m_000000_0\n",
      "16/02/01 16:21:03 INFO util.ProcfsBasedProcessTree: ProcfsBasedProcessTree currently is supported only on Linux.\n",
      "16/02/01 16:21:03 INFO mapred.Task:  Using ResourceCalculatorProcessTree : null\n",
      "16/02/01 16:21:03 INFO mapred.MapTask: Processing split: hdfs://localhost:9000/user/jing/Consumer_Complaints.csv:0+50906486\n",
      "16/02/01 16:21:03 INFO mapred.MapTask: numReduceTasks: 1\n",
      "16/02/01 16:21:03 INFO mapred.MapTask: (EQUATOR) 0 kvi 26214396(104857584)\n",
      "16/02/01 16:21:03 INFO mapred.MapTask: mapreduce.task.io.sort.mb: 100\n",
      "16/02/01 16:21:03 INFO mapred.MapTask: soft limit at 83886080\n",
      "16/02/01 16:21:03 INFO mapred.MapTask: bufstart = 0; bufvoid = 104857600\n",
      "16/02/01 16:21:03 INFO mapred.MapTask: kvstart = 26214396; length = 6553600\n",
      "16/02/01 16:21:03 INFO mapred.MapTask: Map output collector class = org.apache.hadoop.mapred.MapTask$MapOutputBuffer\n",
      "16/02/01 16:21:03 INFO streaming.PipeMapRed: PipeMapRed exec [/Users/JingXu/Dropbox/DataScience/W261/W261/HW3/./mapper.py]\n",
      "16/02/01 16:21:03 INFO Configuration.deprecation: mapred.task.id is deprecated. Instead, use mapreduce.task.attempt.id\n",
      "16/02/01 16:21:03 INFO Configuration.deprecation: user.name is deprecated. Instead, use mapreduce.job.user.name\n",
      "16/02/01 16:21:03 INFO Configuration.deprecation: map.input.start is deprecated. Instead, use mapreduce.map.input.start\n",
      "16/02/01 16:21:03 INFO Configuration.deprecation: mapred.task.is.map is deprecated. Instead, use mapreduce.task.ismap\n",
      "16/02/01 16:21:03 INFO Configuration.deprecation: mapred.tip.id is deprecated. Instead, use mapreduce.task.id\n",
      "16/02/01 16:21:03 INFO Configuration.deprecation: mapred.skip.on is deprecated. Instead, use mapreduce.job.skiprecords\n",
      "16/02/01 16:21:03 INFO Configuration.deprecation: mapred.task.partition is deprecated. Instead, use mapreduce.task.partition\n",
      "16/02/01 16:21:03 INFO Configuration.deprecation: map.input.length is deprecated. Instead, use mapreduce.map.input.length\n",
      "16/02/01 16:21:03 INFO Configuration.deprecation: mapred.local.dir is deprecated. Instead, use mapreduce.cluster.local.dir\n",
      "16/02/01 16:21:03 INFO Configuration.deprecation: mapred.work.output.dir is deprecated. Instead, use mapreduce.task.output.dir\n",
      "16/02/01 16:21:03 INFO Configuration.deprecation: map.input.file is deprecated. Instead, use mapreduce.map.input.file\n",
      "16/02/01 16:21:03 INFO Configuration.deprecation: mapred.job.id is deprecated. Instead, use mapreduce.job.id\n",
      "16/02/01 16:21:03 INFO streaming.PipeMapRed: R/W/S=1/0/0 in:NA [rec/s] out:NA [rec/s]\n",
      "16/02/01 16:21:03 INFO streaming.PipeMapRed: R/W/S=10/0/0 in:NA [rec/s] out:NA [rec/s]\n",
      "16/02/01 16:21:03 INFO streaming.PipeMapRed: R/W/S=100/0/0 in:NA [rec/s] out:NA [rec/s]\n",
      "16/02/01 16:21:03 INFO streaming.PipeMapRed: R/W/S=1000/0/0 in:NA [rec/s] out:NA [rec/s]\n",
      "16/02/01 16:21:03 INFO streaming.PipeMapRed: Records R/W=1715/1\n",
      "16/02/01 16:21:03 INFO streaming.PipeMapRed: R/W/S=10000/8671/0 in:NA [rec/s] out:NA [rec/s]\n",
      "16/02/01 16:21:03 INFO mapreduce.Job: Job job_local506106942_0001 running in uber mode : false\n",
      "16/02/01 16:21:03 INFO mapreduce.Job:  map 0% reduce 0%\n",
      "16/02/01 16:21:04 INFO streaming.PipeMapRed: R/W/S=100000/98977/0 in:100000=100000/1 [rec/s] out:98977=98977/1 [rec/s]\n",
      "16/02/01 16:21:04 INFO streaming.PipeMapRed: R/W/S=200000/197176/0 in:200000=200000/1 [rec/s] out:197176=197176/1 [rec/s]\n",
      "16/02/01 16:21:05 INFO streaming.PipeMapRed: R/W/S=300000/297855/0 in:150000=300000/2 [rec/s] out:148927=297855/2 [rec/s]\n",
      "16/02/01 16:21:05 INFO streaming.PipeMapRed: MRErrorThread done\n",
      "16/02/01 16:21:05 INFO streaming.PipeMapRed: mapRedFinished\n",
      "16/02/01 16:21:05 INFO mapred.LocalJobRunner: \n",
      "16/02/01 16:21:05 INFO mapred.MapTask: Starting flush of map output\n",
      "16/02/01 16:21:05 INFO mapred.MapTask: Spilling map output\n",
      "16/02/01 16:21:05 INFO mapred.MapTask: bufstart = 0; bufend = 3324280; bufvoid = 104857600\n",
      "16/02/01 16:21:05 INFO mapred.MapTask: kvstart = 26214396(104857584); kvend = 24962748(99850992); length = 1251649/6553600\n",
      "16/02/01 16:21:06 INFO mapred.MapTask: Finished spill 0\n",
      "16/02/01 16:21:06 INFO mapred.Task: Task:attempt_local506106942_0001_m_000000_0 is done. And is in the process of committing\n",
      "16/02/01 16:21:06 INFO mapred.LocalJobRunner: Records R/W=1715/1\n",
      "16/02/01 16:21:06 INFO mapred.Task: Task 'attempt_local506106942_0001_m_000000_0' done.\n",
      "16/02/01 16:21:06 INFO mapred.LocalJobRunner: Finishing task: attempt_local506106942_0001_m_000000_0\n",
      "16/02/01 16:21:06 INFO mapred.LocalJobRunner: map task executor complete.\n",
      "16/02/01 16:21:06 INFO mapred.LocalJobRunner: Waiting for reduce tasks\n",
      "16/02/01 16:21:06 INFO mapred.LocalJobRunner: Starting task: attempt_local506106942_0001_r_000000_0\n",
      "16/02/01 16:21:06 INFO util.ProcfsBasedProcessTree: ProcfsBasedProcessTree currently is supported only on Linux.\n",
      "16/02/01 16:21:06 INFO mapred.Task:  Using ResourceCalculatorProcessTree : null\n",
      "16/02/01 16:21:06 INFO mapred.ReduceTask: Using ShuffleConsumerPlugin: org.apache.hadoop.mapreduce.task.reduce.Shuffle@43ff9c06\n",
      "16/02/01 16:21:06 INFO reduce.MergeManagerImpl: MergerManager: memoryLimit=333971456, maxSingleShuffleLimit=83492864, mergeThreshold=220421168, ioSortFactor=10, memToMemMergeOutputsThreshold=10\n",
      "16/02/01 16:21:06 INFO reduce.EventFetcher: attempt_local506106942_0001_r_000000_0 Thread started: EventFetcher for fetching Map Completion Events\n",
      "16/02/01 16:21:06 INFO reduce.LocalFetcher: localfetcher#1 about to shuffle output of map attempt_local506106942_0001_m_000000_0 decomp: 3950108 len: 3950112 to MEMORY\n",
      "16/02/01 16:21:06 INFO reduce.InMemoryMapOutput: Read 3950108 bytes from map-output for attempt_local506106942_0001_m_000000_0\n",
      "16/02/01 16:21:06 INFO reduce.MergeManagerImpl: closeInMemoryFile -> map-output of size: 3950108, inMemoryMapOutputs.size() -> 1, commitMemory -> 0, usedMemory ->3950108\n",
      "16/02/01 16:21:06 INFO reduce.EventFetcher: EventFetcher is interrupted.. Returning\n",
      "16/02/01 16:21:06 INFO mapred.LocalJobRunner: 1 / 1 copied.\n",
      "16/02/01 16:21:06 INFO reduce.MergeManagerImpl: finalMerge called with 1 in-memory map-outputs and 0 on-disk map-outputs\n",
      "16/02/01 16:21:06 INFO mapred.Merger: Merging 1 sorted segments\n",
      "16/02/01 16:21:06 INFO mapred.Merger: Down to the last merge-pass, with 1 segments left of total size: 3950090 bytes\n",
      "16/02/01 16:21:06 INFO reduce.MergeManagerImpl: Merged 1 segments, 3950108 bytes to disk to satisfy reduce memory limit\n",
      "16/02/01 16:21:06 INFO reduce.MergeManagerImpl: Merging 1 files, 3950112 bytes from disk\n",
      "16/02/01 16:21:06 INFO reduce.MergeManagerImpl: Merging 0 segments, 0 bytes from memory into reduce\n",
      "16/02/01 16:21:06 INFO mapred.Merger: Merging 1 sorted segments\n",
      "16/02/01 16:21:06 INFO mapred.Merger: Down to the last merge-pass, with 1 segments left of total size: 3950090 bytes\n",
      "16/02/01 16:21:06 INFO mapred.LocalJobRunner: 1 / 1 copied.\n",
      "16/02/01 16:21:06 INFO streaming.PipeMapRed: PipeMapRed exec [/Users/JingXu/Dropbox/DataScience/W261/W261/HW3/./reducer.py]\n",
      "16/02/01 16:21:06 INFO Configuration.deprecation: mapred.job.tracker is deprecated. Instead, use mapreduce.jobtracker.address\n",
      "16/02/01 16:21:06 INFO Configuration.deprecation: mapred.map.tasks is deprecated. Instead, use mapreduce.job.maps\n",
      "16/02/01 16:21:06 INFO streaming.PipeMapRed: R/W/S=1/0/0 in:NA [rec/s] out:NA [rec/s]\n",
      "16/02/01 16:21:06 INFO streaming.PipeMapRed: R/W/S=10/0/0 in:NA [rec/s] out:NA [rec/s]\n",
      "16/02/01 16:21:06 INFO streaming.PipeMapRed: R/W/S=100/0/0 in:NA [rec/s] out:NA [rec/s]\n",
      "16/02/01 16:21:06 INFO streaming.PipeMapRed: R/W/S=1000/0/0 in:NA [rec/s] out:NA [rec/s]\n",
      "16/02/01 16:21:06 INFO streaming.PipeMapRed: R/W/S=10000/0/0 in:NA [rec/s] out:NA [rec/s]\n",
      "16/02/01 16:21:06 INFO streaming.PipeMapRed: R/W/S=100000/0/0 in:NA [rec/s] out:NA [rec/s]\n",
      "16/02/01 16:21:06 INFO mapreduce.Job:  map 100% reduce 0%\n",
      "16/02/01 16:21:07 INFO streaming.PipeMapRed: R/W/S=200000/0/0 in:NA [rec/s] out:NA [rec/s]\n",
      "16/02/01 16:21:07 INFO streaming.PipeMapRed: R/W/S=300000/0/0 in:NA [rec/s] out:NA [rec/s]\n",
      "16/02/01 16:21:07 INFO streaming.PipeMapRed: MRErrorThread done\n",
      "16/02/01 16:21:07 INFO streaming.PipeMapRed: Records R/W=312913/1\n",
      "16/02/01 16:21:07 INFO streaming.PipeMapRed: mapRedFinished\n",
      "16/02/01 16:21:07 INFO mapred.Task: Task:attempt_local506106942_0001_r_000000_0 is done. And is in the process of committing\n",
      "16/02/01 16:21:07 INFO mapred.LocalJobRunner: 1 / 1 copied.\n",
      "16/02/01 16:21:07 INFO mapred.Task: Task attempt_local506106942_0001_r_000000_0 is allowed to commit now\n",
      "16/02/01 16:21:07 INFO output.FileOutputCommitter: Saved output of task 'attempt_local506106942_0001_r_000000_0' to hdfs://localhost:9000/user/jing/test-output1/_temporary/0/task_local506106942_0001_r_000000\n",
      "16/02/01 16:21:07 INFO mapred.LocalJobRunner: Records R/W=312913/1 > reduce\n",
      "16/02/01 16:21:07 INFO mapred.Task: Task 'attempt_local506106942_0001_r_000000_0' done.\n",
      "16/02/01 16:21:07 INFO mapred.LocalJobRunner: Finishing task: attempt_local506106942_0001_r_000000_0\n",
      "16/02/01 16:21:07 INFO mapred.LocalJobRunner: reduce task executor complete.\n",
      "16/02/01 16:21:07 INFO mapreduce.Job:  map 100% reduce 100%\n",
      "16/02/01 16:21:07 INFO mapreduce.Job: Job job_local506106942_0001 completed successfully\n",
      "16/02/01 16:21:08 INFO mapreduce.Job: Counters: 38\n",
      "\tFile System Counters\n",
      "\t\tFILE: Number of bytes read=7906446\n",
      "\t\tFILE: Number of bytes written=12374668\n",
      "\t\tFILE: Number of read operations=0\n",
      "\t\tFILE: Number of large read operations=0\n",
      "\t\tFILE: Number of write operations=0\n",
      "\t\tHDFS: Number of bytes read=101812972\n",
      "\t\tHDFS: Number of bytes written=51\n",
      "\t\tHDFS: Number of read operations=15\n",
      "\t\tHDFS: Number of large read operations=0\n",
      "\t\tHDFS: Number of write operations=4\n",
      "\tMap-Reduce Framework\n",
      "\t\tMap input records=312913\n",
      "\t\tMap output records=312913\n",
      "\t\tMap output bytes=3324280\n",
      "\t\tMap output materialized bytes=3950112\n",
      "\t\tInput split bytes=107\n",
      "\t\tCombine input records=0\n",
      "\t\tCombine output records=0\n",
      "\t\tReduce input groups=3\n",
      "\t\tReduce shuffle bytes=3950112\n",
      "\t\tReduce input records=312913\n",
      "\t\tReduce output records=3\n",
      "\t\tSpilled Records=625826\n",
      "\t\tShuffled Maps =1\n",
      "\t\tFailed Shuffles=0\n",
      "\t\tMerged Map outputs=1\n",
      "\t\tGC time elapsed (ms)=25\n",
      "\t\tTotal committed heap usage (bytes)=630194176\n",
      "\tDebtCounter\n",
      "\t\tTotal=44372\n",
      "\tMortgageCounter\n",
      "\t\tTotal=125752\n",
      "\tOtherCounter\n",
      "\t\tTotal=142789\n",
      "\tShuffle Errors\n",
      "\t\tBAD_ID=0\n",
      "\t\tCONNECTION=0\n",
      "\t\tIO_ERROR=0\n",
      "\t\tWRONG_LENGTH=0\n",
      "\t\tWRONG_MAP=0\n",
      "\t\tWRONG_REDUCE=0\n",
      "\tFile Input Format Counters \n",
      "\t\tBytes Read=50906486\n",
      "\tFile Output Format Counters \n",
      "\t\tBytes Written=51\n",
      "16/02/01 16:21:08 INFO streaming.StreamJob: Output directory: /user/jing/test-output1/\n",
      "DEPRECATED: Use of this script to execute hdfs command is deprecated.\n",
      "Instead use the hdfs command for it.\n",
      "\n",
      "16/02/01 16:21:09 WARN util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "debt collection\t44372\n",
      "mortgage\t125752\n",
      "other\t142789\n",
      "rmr: DEPRECATED: Please use 'rm -r' instead.\n",
      "16/02/01 16:21:10 WARN util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "16/02/01 16:21:11 INFO fs.TrashPolicyDefault: Namenode trash configuration: Deletion interval = 0 minutes, Emptier interval = 0 minutes.\n",
      "Deleted /user/jing\n",
      "rmr: DEPRECATED: Please use 'rm -r' instead.\n",
      "16/02/01 16:21:12 WARN util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "16/02/01 16:21:13 INFO fs.TrashPolicyDefault: Namenode trash configuration: Deletion interval = 0 minutes, Emptier interval = 0 minutes.\n",
      "Deleted /user\n",
      "rm: part-00000: No such file or directory\n"
     ]
    }
   ],
   "source": [
    "!./mapreduce.sh"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "debt collection\t44372\n",
    "\n",
    "mortgage\t125752\n",
    "\n",
    "other\t142789"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**HW3.2 Analyze the performance of your Mappers, Combiners and Reducers using Counters**\n",
    "\n",
    "**For this brief study the Input file will be one record (the next line only): \n",
    "foo foo quux labs foo bar quux**\n",
    "\n",
    "\n",
    "**Perform a word count analysis of this single record dataset using a Mapper and Reducer based WordCount (i.e., no combiners are used here) using user defined Counters to count up how many time the mapper and reducer are called. What is the value of your user defined Mapper Counter, and Reducer Counter after completing this word count job. The answer  should be 1 and 4 respectively. Please explain.**\n",
    "\n",
    "**Please use mulitple mappers and reducers for these jobs (at least 2 mappers and 2 reducers).\n",
    "Perform a word count analysis of the Issue column of the Consumer Complaints  Dataset using a Mapper and Reducer based WordCount (i.e., no combiners used anywhere)  using user defined Counters to count up how many time the mapper and reducer are called. What is the value of your user defined Mapper Counter, and Reducer Counter after completing your word count job.**\n",
    "\n",
    "**Perform a word count analysis of the Issue column of the Consumer Complaints  Dataset using a Mapper, Reducer, and standalone combiner (i.e., not an in-memory combiner) based WordCount using user defined Counters to count up how many time the mapper, combiner, reducer are called. What is the value of your user defined Mapper Counter, and Reducer Counter after completing your word count job. \n",
    "Using a single reducer: What are the top 50 most frequent terms in your word count analysis? Present the top 50 terms and their frequency and their relative frequency. Present the top 50 terms and their frequency and their relative frequency. If there are ties please sort the tokens in alphanumeric/string order. Present bottom 10 tokens (least frequent items).** "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting mapper.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile mapper.py\n",
    "#!/usr/bin/python\n",
    "## mapper.py\n",
    "## Author: Jing Xu\n",
    "## Description: mapper code for HW3.2.1\n",
    "\n",
    "import sys\n",
    "import re\n",
    "\n",
    "sys.stderr.write(\"reporter:counter:MapCounter,Instances,1\\n\")\n",
    "\n",
    "WORD_RE = re.compile(r\"[\\w']+\") #Compile regex to easily parse complete words\n",
    "\n",
    "# read input from STDIN (standard input)\n",
    "for line in sys.stdin:\n",
    "    # remove leading and trailing whitespace\n",
    "    line = line.strip()\n",
    "    # split the line into words\n",
    "    words=re.findall(WORD_RE,line) #create list of words\n",
    "    # increase counters\n",
    "    for word in words:\n",
    "        word = word.lower()\n",
    "        # write the results to STDOUT (standard output);\n",
    "        # what we output here will be the input for the\n",
    "        # Reduce step, i.e. the input for reducer.py\n",
    "        # tab-delimited; the trivial word count is 1\n",
    "        print '%s\\t%s' % (word, 1)\n",
    "    sys.stderr.write(\"reporter:counter:MapCounter,Lines,1\\n\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting reducer.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile reducer.py\n",
    "#!/usr/bin/python\n",
    "## reducer.py\n",
    "## Author: Jing Xu\n",
    "## Description: reducer code for HW3.2.1\n",
    "\n",
    "from operator import itemgetter\n",
    "import sys\n",
    "\n",
    "current_word = None\n",
    "current_count = 0\n",
    "word = None\n",
    "\n",
    "# input comes from STDIN\n",
    "for line in sys.stdin:\n",
    "    # remove leading and trailing whitespace\n",
    "    line = line.strip()\n",
    "    # parse the input we got from mapper.py\n",
    "    word, count = line.split('\\t', 1)\n",
    "    # convert count (currently a string) to int\n",
    "    try:\n",
    "        count = int(count)\n",
    "    except ValueError:\n",
    "        # count was not a number, so silently\n",
    "        # ignore/discard this line\n",
    "        continue\n",
    "    if current_word == word:\n",
    "        current_count += count\n",
    "    else:\n",
    "        if current_word:\n",
    "            # write result to STDOUT\n",
    "            print '%s\\t%s' % (current_word, current_count)\n",
    "        current_count = count\n",
    "        current_word = word\n",
    "\n",
    "if current_word == word:\n",
    "    print '%s\\t%s' % (current_word, current_count)\n",
    "\n",
    "sys.stderr.write(\"reporter:counter:ReducerCounter,Total,1\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting mapreduce.sh\n"
     ]
    }
   ],
   "source": [
    "%%writefile mapreduce.sh\n",
    "## mapreduce.sh\n",
    "## Author: Jing Xu\n",
    "## Description: mapreduce bash script for HW3.2.1\n",
    "\n",
    "echo \"foo foo quux labs foo bar quux\" > hw3-2.txt\n",
    "\n",
    "hdfs dfs -mkdir /user # create hdfs folder\n",
    "wait\n",
    "hdfs dfs -mkdir /user/jing # create hdfs folder\n",
    "wait\n",
    "hadoop fs -put hw3-2.txt /user/jing # upload local file to hdfs folder\n",
    "wait\n",
    "\n",
    "# hadoop command to run streaming mapreduce job\n",
    "hadoop jar /Users/JingXu/Documents/hadoop-2.6.3/share/hadoop/tools/lib/hadoop-streaming-2.6.3.jar \\\n",
    "-D mapred.job.name=\"Count Job via Streaming\" \\\n",
    "-D mapred.map.tasks=1 \\\n",
    "-D mapred.reduce.tasks=4 \\\n",
    "-file mapper.py    -mapper mapper.py \\\n",
    "-file reducer.py   -reducer reducer.py \\\n",
    "-input /user/jing/* -output /user/jing/test-output1/\n",
    "wait\n",
    "\n",
    "hadoop dfs -cat /user/jing/test-output1/part-00000\n",
    "\n",
    "hdfs dfs -rmr /user/jing # remove hdfs folder\n",
    "hdfs dfs -rmr /user # remove hdfs folder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "16/02/01 16:22:25 WARN util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "16/02/01 16:22:27 WARN util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "16/02/01 16:22:29 WARN util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "16/02/01 16:22:30 WARN streaming.StreamJob: -file option is deprecated, please use generic option -files instead.\n",
      "16/02/01 16:22:31 WARN util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "packageJobJar: [mapper.py, reducer.py] [] /var/folders/zs/k144hqks281fbt0x68c_zj9m0000gp/T/streamjob8936948989538245615.jar tmpDir=null\n",
      "16/02/01 16:22:31 INFO Configuration.deprecation: session.id is deprecated. Instead, use dfs.metrics.session-id\n",
      "16/02/01 16:22:31 INFO jvm.JvmMetrics: Initializing JVM Metrics with processName=JobTracker, sessionId=\n",
      "16/02/01 16:22:31 INFO jvm.JvmMetrics: Cannot initialize JVM Metrics with processName=JobTracker, sessionId= - already initialized\n",
      "16/02/01 16:22:32 INFO mapred.FileInputFormat: Total input paths to process : 1\n",
      "16/02/01 16:22:32 INFO mapreduce.JobSubmitter: number of splits:1\n",
      "16/02/01 16:22:32 INFO Configuration.deprecation: mapred.reduce.tasks is deprecated. Instead, use mapreduce.job.reduces\n",
      "16/02/01 16:22:32 INFO Configuration.deprecation: mapred.job.name is deprecated. Instead, use mapreduce.job.name\n",
      "16/02/01 16:22:32 INFO Configuration.deprecation: mapred.map.tasks is deprecated. Instead, use mapreduce.job.maps\n",
      "16/02/01 16:22:32 INFO mapreduce.JobSubmitter: Submitting tokens for job: job_local1154151827_0001\n",
      "16/02/01 16:22:32 INFO mapred.LocalDistributedCacheManager: Localized file:/Users/JingXu/Dropbox/DataScience/W261/W261/HW3/mapper.py as file:/tmp/hadoop-JingXu/mapred/local/1454372552419/mapper.py\n",
      "16/02/01 16:22:32 INFO mapred.LocalDistributedCacheManager: Localized file:/Users/JingXu/Dropbox/DataScience/W261/W261/HW3/reducer.py as file:/tmp/hadoop-JingXu/mapred/local/1454372552420/reducer.py\n",
      "16/02/01 16:22:32 INFO mapreduce.Job: The url to track the job: http://localhost:8080/\n",
      "16/02/01 16:22:32 INFO mapred.LocalJobRunner: OutputCommitter set in config null\n",
      "16/02/01 16:22:32 INFO mapreduce.Job: Running job: job_local1154151827_0001\n",
      "16/02/01 16:22:32 INFO mapred.LocalJobRunner: OutputCommitter is org.apache.hadoop.mapred.FileOutputCommitter\n",
      "16/02/01 16:22:32 INFO mapred.LocalJobRunner: Waiting for map tasks\n",
      "16/02/01 16:22:32 INFO mapred.LocalJobRunner: Starting task: attempt_local1154151827_0001_m_000000_0\n",
      "16/02/01 16:22:32 INFO util.ProcfsBasedProcessTree: ProcfsBasedProcessTree currently is supported only on Linux.\n",
      "16/02/01 16:22:32 INFO mapred.Task:  Using ResourceCalculatorProcessTree : null\n",
      "16/02/01 16:22:32 INFO mapred.MapTask: Processing split: hdfs://localhost:9000/user/jing/hw3-2.txt:0+31\n",
      "16/02/01 16:22:32 INFO mapred.MapTask: numReduceTasks: 4\n",
      "16/02/01 16:22:32 INFO mapred.MapTask: (EQUATOR) 0 kvi 26214396(104857584)\n",
      "16/02/01 16:22:32 INFO mapred.MapTask: mapreduce.task.io.sort.mb: 100\n",
      "16/02/01 16:22:32 INFO mapred.MapTask: soft limit at 83886080\n",
      "16/02/01 16:22:32 INFO mapred.MapTask: bufstart = 0; bufvoid = 104857600\n",
      "16/02/01 16:22:32 INFO mapred.MapTask: kvstart = 26214396; length = 6553600\n",
      "16/02/01 16:22:32 INFO mapred.MapTask: Map output collector class = org.apache.hadoop.mapred.MapTask$MapOutputBuffer\n",
      "16/02/01 16:22:32 INFO streaming.PipeMapRed: PipeMapRed exec [/Users/JingXu/Dropbox/DataScience/W261/W261/HW3/./mapper.py]\n",
      "16/02/01 16:22:32 INFO Configuration.deprecation: mapred.task.id is deprecated. Instead, use mapreduce.task.attempt.id\n",
      "16/02/01 16:22:32 INFO Configuration.deprecation: user.name is deprecated. Instead, use mapreduce.job.user.name\n",
      "16/02/01 16:22:32 INFO Configuration.deprecation: map.input.start is deprecated. Instead, use mapreduce.map.input.start\n",
      "16/02/01 16:22:32 INFO Configuration.deprecation: mapred.task.is.map is deprecated. Instead, use mapreduce.task.ismap\n",
      "16/02/01 16:22:32 INFO Configuration.deprecation: mapred.tip.id is deprecated. Instead, use mapreduce.task.id\n",
      "16/02/01 16:22:32 INFO Configuration.deprecation: mapred.skip.on is deprecated. Instead, use mapreduce.job.skiprecords\n",
      "16/02/01 16:22:32 INFO Configuration.deprecation: mapred.task.partition is deprecated. Instead, use mapreduce.task.partition\n",
      "16/02/01 16:22:32 INFO Configuration.deprecation: map.input.length is deprecated. Instead, use mapreduce.map.input.length\n",
      "16/02/01 16:22:32 INFO Configuration.deprecation: mapred.local.dir is deprecated. Instead, use mapreduce.cluster.local.dir\n",
      "16/02/01 16:22:32 INFO Configuration.deprecation: mapred.work.output.dir is deprecated. Instead, use mapreduce.task.output.dir\n",
      "16/02/01 16:22:32 INFO Configuration.deprecation: map.input.file is deprecated. Instead, use mapreduce.map.input.file\n",
      "16/02/01 16:22:32 INFO Configuration.deprecation: mapred.job.id is deprecated. Instead, use mapreduce.job.id\n",
      "16/02/01 16:22:32 INFO streaming.PipeMapRed: R/W/S=1/0/0 in:NA [rec/s] out:NA [rec/s]\n",
      "16/02/01 16:22:32 INFO streaming.PipeMapRed: MRErrorThread done\n",
      "16/02/01 16:22:32 INFO streaming.PipeMapRed: Records R/W=1/1\n",
      "16/02/01 16:22:32 INFO streaming.PipeMapRed: mapRedFinished\n",
      "16/02/01 16:22:32 INFO mapred.LocalJobRunner: \n",
      "16/02/01 16:22:32 INFO mapred.MapTask: Starting flush of map output\n",
      "16/02/01 16:22:32 INFO mapred.MapTask: Spilling map output\n",
      "16/02/01 16:22:32 INFO mapred.MapTask: bufstart = 0; bufend = 45; bufvoid = 104857600\n",
      "16/02/01 16:22:32 INFO mapred.MapTask: kvstart = 26214396(104857584); kvend = 26214372(104857488); length = 25/6553600\n",
      "16/02/01 16:22:32 INFO mapred.MapTask: Finished spill 0\n",
      "16/02/01 16:22:32 INFO mapred.Task: Task:attempt_local1154151827_0001_m_000000_0 is done. And is in the process of committing\n",
      "16/02/01 16:22:32 INFO mapred.LocalJobRunner: Records R/W=1/1\n",
      "16/02/01 16:22:32 INFO mapred.Task: Task 'attempt_local1154151827_0001_m_000000_0' done.\n",
      "16/02/01 16:22:32 INFO mapred.LocalJobRunner: Finishing task: attempt_local1154151827_0001_m_000000_0\n",
      "16/02/01 16:22:32 INFO mapred.LocalJobRunner: map task executor complete.\n",
      "16/02/01 16:22:32 INFO mapred.LocalJobRunner: Waiting for reduce tasks\n",
      "16/02/01 16:22:32 INFO mapred.LocalJobRunner: Starting task: attempt_local1154151827_0001_r_000000_0\n",
      "16/02/01 16:22:32 INFO util.ProcfsBasedProcessTree: ProcfsBasedProcessTree currently is supported only on Linux.\n",
      "16/02/01 16:22:32 INFO mapred.Task:  Using ResourceCalculatorProcessTree : null\n",
      "16/02/01 16:22:32 INFO mapred.ReduceTask: Using ShuffleConsumerPlugin: org.apache.hadoop.mapreduce.task.reduce.Shuffle@cf55979\n",
      "16/02/01 16:22:32 INFO reduce.MergeManagerImpl: MergerManager: memoryLimit=333971456, maxSingleShuffleLimit=83492864, mergeThreshold=220421168, ioSortFactor=10, memToMemMergeOutputsThreshold=10\n",
      "16/02/01 16:22:32 INFO reduce.EventFetcher: attempt_local1154151827_0001_r_000000_0 Thread started: EventFetcher for fetching Map Completion Events\n",
      "16/02/01 16:22:32 INFO reduce.LocalFetcher: localfetcher#1 about to shuffle output of map attempt_local1154151827_0001_m_000000_0 decomp: 20 len: 24 to MEMORY\n",
      "16/02/01 16:22:32 INFO reduce.InMemoryMapOutput: Read 20 bytes from map-output for attempt_local1154151827_0001_m_000000_0\n",
      "16/02/01 16:22:32 INFO reduce.MergeManagerImpl: closeInMemoryFile -> map-output of size: 20, inMemoryMapOutputs.size() -> 1, commitMemory -> 0, usedMemory ->20\n",
      "16/02/01 16:22:32 INFO reduce.EventFetcher: EventFetcher is interrupted.. Returning\n",
      "16/02/01 16:22:32 INFO mapred.LocalJobRunner: 1 / 1 copied.\n",
      "16/02/01 16:22:32 INFO reduce.MergeManagerImpl: finalMerge called with 1 in-memory map-outputs and 0 on-disk map-outputs\n",
      "16/02/01 16:22:32 INFO mapred.Merger: Merging 1 sorted segments\n",
      "16/02/01 16:22:32 INFO mapred.Merger: Down to the last merge-pass, with 1 segments left of total size: 13 bytes\n",
      "16/02/01 16:22:32 INFO reduce.MergeManagerImpl: Merged 1 segments, 20 bytes to disk to satisfy reduce memory limit\n",
      "16/02/01 16:22:32 INFO reduce.MergeManagerImpl: Merging 1 files, 24 bytes from disk\n",
      "16/02/01 16:22:32 INFO reduce.MergeManagerImpl: Merging 0 segments, 0 bytes from memory into reduce\n",
      "16/02/01 16:22:32 INFO mapred.Merger: Merging 1 sorted segments\n",
      "16/02/01 16:22:32 INFO mapred.Merger: Down to the last merge-pass, with 1 segments left of total size: 13 bytes\n",
      "16/02/01 16:22:32 INFO mapred.LocalJobRunner: 1 / 1 copied.\n",
      "16/02/01 16:22:32 INFO streaming.PipeMapRed: PipeMapRed exec [/Users/JingXu/Dropbox/DataScience/W261/W261/HW3/./reducer.py]\n",
      "16/02/01 16:22:32 INFO Configuration.deprecation: mapred.job.tracker is deprecated. Instead, use mapreduce.jobtracker.address\n",
      "16/02/01 16:22:32 INFO Configuration.deprecation: mapred.map.tasks is deprecated. Instead, use mapreduce.job.maps\n",
      "16/02/01 16:22:32 INFO streaming.PipeMapRed: R/W/S=1/0/0 in:NA [rec/s] out:NA [rec/s]\n",
      "16/02/01 16:22:32 INFO streaming.PipeMapRed: Records R/W=2/1\n",
      "16/02/01 16:22:32 INFO streaming.PipeMapRed: MRErrorThread done\n",
      "16/02/01 16:22:32 INFO streaming.PipeMapRed: mapRedFinished\n",
      "16/02/01 16:22:33 INFO mapred.Task: Task:attempt_local1154151827_0001_r_000000_0 is done. And is in the process of committing\n",
      "16/02/01 16:22:33 INFO mapred.LocalJobRunner: 1 / 1 copied.\n",
      "16/02/01 16:22:33 INFO mapred.Task: Task attempt_local1154151827_0001_r_000000_0 is allowed to commit now\n",
      "16/02/01 16:22:33 INFO output.FileOutputCommitter: Saved output of task 'attempt_local1154151827_0001_r_000000_0' to hdfs://localhost:9000/user/jing/test-output1/_temporary/0/task_local1154151827_0001_r_000000\n",
      "16/02/01 16:22:33 INFO mapred.LocalJobRunner: Records R/W=2/1 > reduce\n",
      "16/02/01 16:22:33 INFO mapred.Task: Task 'attempt_local1154151827_0001_r_000000_0' done.\n",
      "16/02/01 16:22:33 INFO mapred.LocalJobRunner: Finishing task: attempt_local1154151827_0001_r_000000_0\n",
      "16/02/01 16:22:33 INFO mapred.LocalJobRunner: Starting task: attempt_local1154151827_0001_r_000001_0\n",
      "16/02/01 16:22:33 INFO util.ProcfsBasedProcessTree: ProcfsBasedProcessTree currently is supported only on Linux.\n",
      "16/02/01 16:22:33 INFO mapred.Task:  Using ResourceCalculatorProcessTree : null\n",
      "16/02/01 16:22:33 INFO mapred.ReduceTask: Using ShuffleConsumerPlugin: org.apache.hadoop.mapreduce.task.reduce.Shuffle@44c512da\n",
      "16/02/01 16:22:33 INFO reduce.MergeManagerImpl: MergerManager: memoryLimit=333971456, maxSingleShuffleLimit=83492864, mergeThreshold=220421168, ioSortFactor=10, memToMemMergeOutputsThreshold=10\n",
      "16/02/01 16:22:33 INFO reduce.EventFetcher: attempt_local1154151827_0001_r_000001_0 Thread started: EventFetcher for fetching Map Completion Events\n",
      "16/02/01 16:22:33 INFO reduce.LocalFetcher: localfetcher#2 about to shuffle output of map attempt_local1154151827_0001_m_000000_0 decomp: 26 len: 30 to MEMORY\n",
      "16/02/01 16:22:33 INFO reduce.InMemoryMapOutput: Read 26 bytes from map-output for attempt_local1154151827_0001_m_000000_0\n",
      "16/02/01 16:22:33 INFO reduce.MergeManagerImpl: closeInMemoryFile -> map-output of size: 26, inMemoryMapOutputs.size() -> 1, commitMemory -> 0, usedMemory ->26\n",
      "16/02/01 16:22:33 INFO reduce.EventFetcher: EventFetcher is interrupted.. Returning\n",
      "16/02/01 16:22:33 INFO mapred.LocalJobRunner: 1 / 1 copied.\n",
      "16/02/01 16:22:33 INFO reduce.MergeManagerImpl: finalMerge called with 1 in-memory map-outputs and 0 on-disk map-outputs\n",
      "16/02/01 16:22:33 INFO mapred.Merger: Merging 1 sorted segments\n",
      "16/02/01 16:22:33 INFO mapred.Merger: Down to the last merge-pass, with 1 segments left of total size: 20 bytes\n",
      "16/02/01 16:22:33 INFO reduce.MergeManagerImpl: Merged 1 segments, 26 bytes to disk to satisfy reduce memory limit\n",
      "16/02/01 16:22:33 INFO reduce.MergeManagerImpl: Merging 1 files, 30 bytes from disk\n",
      "16/02/01 16:22:33 INFO reduce.MergeManagerImpl: Merging 0 segments, 0 bytes from memory into reduce\n",
      "16/02/01 16:22:33 INFO mapred.Merger: Merging 1 sorted segments\n",
      "16/02/01 16:22:33 INFO mapred.Merger: Down to the last merge-pass, with 1 segments left of total size: 20 bytes\n",
      "16/02/01 16:22:33 INFO mapred.LocalJobRunner: 1 / 1 copied.\n",
      "16/02/01 16:22:33 INFO streaming.PipeMapRed: PipeMapRed exec [/Users/JingXu/Dropbox/DataScience/W261/W261/HW3/./reducer.py]\n",
      "16/02/01 16:22:33 INFO streaming.PipeMapRed: R/W/S=1/0/0 in:NA [rec/s] out:NA [rec/s]\n",
      "16/02/01 16:22:33 INFO streaming.PipeMapRed: Records R/W=3/1\n",
      "16/02/01 16:22:33 INFO streaming.PipeMapRed: MRErrorThread done\n",
      "16/02/01 16:22:33 INFO streaming.PipeMapRed: mapRedFinished\n",
      "16/02/01 16:22:33 INFO mapred.Task: Task:attempt_local1154151827_0001_r_000001_0 is done. And is in the process of committing\n",
      "16/02/01 16:22:33 INFO mapred.LocalJobRunner: 1 / 1 copied.\n",
      "16/02/01 16:22:33 INFO mapred.Task: Task attempt_local1154151827_0001_r_000001_0 is allowed to commit now\n",
      "16/02/01 16:22:33 INFO output.FileOutputCommitter: Saved output of task 'attempt_local1154151827_0001_r_000001_0' to hdfs://localhost:9000/user/jing/test-output1/_temporary/0/task_local1154151827_0001_r_000001\n",
      "16/02/01 16:22:33 INFO mapred.LocalJobRunner: Records R/W=3/1 > reduce\n",
      "16/02/01 16:22:33 INFO mapred.Task: Task 'attempt_local1154151827_0001_r_000001_0' done.\n",
      "16/02/01 16:22:33 INFO mapred.LocalJobRunner: Finishing task: attempt_local1154151827_0001_r_000001_0\n",
      "16/02/01 16:22:33 INFO mapred.LocalJobRunner: Starting task: attempt_local1154151827_0001_r_000002_0\n",
      "16/02/01 16:22:33 INFO util.ProcfsBasedProcessTree: ProcfsBasedProcessTree currently is supported only on Linux.\n",
      "16/02/01 16:22:33 INFO mapred.Task:  Using ResourceCalculatorProcessTree : null\n",
      "16/02/01 16:22:33 INFO mapred.ReduceTask: Using ShuffleConsumerPlugin: org.apache.hadoop.mapreduce.task.reduce.Shuffle@72f3c793\n",
      "16/02/01 16:22:33 INFO reduce.MergeManagerImpl: MergerManager: memoryLimit=333971456, maxSingleShuffleLimit=83492864, mergeThreshold=220421168, ioSortFactor=10, memToMemMergeOutputsThreshold=10\n",
      "16/02/01 16:22:33 INFO reduce.EventFetcher: attempt_local1154151827_0001_r_000002_0 Thread started: EventFetcher for fetching Map Completion Events\n",
      "16/02/01 16:22:33 INFO reduce.LocalFetcher: localfetcher#3 about to shuffle output of map attempt_local1154151827_0001_m_000000_0 decomp: 10 len: 14 to MEMORY\n",
      "16/02/01 16:22:33 INFO reduce.InMemoryMapOutput: Read 10 bytes from map-output for attempt_local1154151827_0001_m_000000_0\n",
      "16/02/01 16:22:33 INFO reduce.MergeManagerImpl: closeInMemoryFile -> map-output of size: 10, inMemoryMapOutputs.size() -> 1, commitMemory -> 0, usedMemory ->10\n",
      "16/02/01 16:22:33 INFO reduce.EventFetcher: EventFetcher is interrupted.. Returning\n",
      "16/02/01 16:22:33 INFO mapred.LocalJobRunner: 1 / 1 copied.\n",
      "16/02/01 16:22:33 INFO reduce.MergeManagerImpl: finalMerge called with 1 in-memory map-outputs and 0 on-disk map-outputs\n",
      "16/02/01 16:22:33 INFO mapred.Merger: Merging 1 sorted segments\n",
      "16/02/01 16:22:33 INFO mapred.Merger: Down to the last merge-pass, with 1 segments left of total size: 4 bytes\n",
      "16/02/01 16:22:33 INFO reduce.MergeManagerImpl: Merged 1 segments, 10 bytes to disk to satisfy reduce memory limit\n",
      "16/02/01 16:22:33 INFO reduce.MergeManagerImpl: Merging 1 files, 14 bytes from disk\n",
      "16/02/01 16:22:33 INFO reduce.MergeManagerImpl: Merging 0 segments, 0 bytes from memory into reduce\n",
      "16/02/01 16:22:33 INFO mapred.Merger: Merging 1 sorted segments\n",
      "16/02/01 16:22:33 INFO mapred.Merger: Down to the last merge-pass, with 1 segments left of total size: 4 bytes\n",
      "16/02/01 16:22:33 INFO mapred.LocalJobRunner: 1 / 1 copied.\n",
      "16/02/01 16:22:33 INFO streaming.PipeMapRed: PipeMapRed exec [/Users/JingXu/Dropbox/DataScience/W261/W261/HW3/./reducer.py]\n",
      "16/02/01 16:22:33 INFO streaming.PipeMapRed: R/W/S=1/0/0 in:NA [rec/s] out:NA [rec/s]\n",
      "16/02/01 16:22:33 INFO streaming.PipeMapRed: Records R/W=1/1\n",
      "16/02/01 16:22:33 INFO streaming.PipeMapRed: MRErrorThread done\n",
      "16/02/01 16:22:33 INFO streaming.PipeMapRed: mapRedFinished\n",
      "16/02/01 16:22:33 INFO mapred.Task: Task:attempt_local1154151827_0001_r_000002_0 is done. And is in the process of committing\n",
      "16/02/01 16:22:33 INFO mapred.LocalJobRunner: 1 / 1 copied.\n",
      "16/02/01 16:22:33 INFO mapred.Task: Task attempt_local1154151827_0001_r_000002_0 is allowed to commit now\n",
      "16/02/01 16:22:33 INFO output.FileOutputCommitter: Saved output of task 'attempt_local1154151827_0001_r_000002_0' to hdfs://localhost:9000/user/jing/test-output1/_temporary/0/task_local1154151827_0001_r_000002\n",
      "16/02/01 16:22:33 INFO mapred.LocalJobRunner: Records R/W=1/1 > reduce\n",
      "16/02/01 16:22:33 INFO mapred.Task: Task 'attempt_local1154151827_0001_r_000002_0' done.\n",
      "16/02/01 16:22:33 INFO mapred.LocalJobRunner: Finishing task: attempt_local1154151827_0001_r_000002_0\n",
      "16/02/01 16:22:33 INFO mapred.LocalJobRunner: Starting task: attempt_local1154151827_0001_r_000003_0\n",
      "16/02/01 16:22:33 INFO util.ProcfsBasedProcessTree: ProcfsBasedProcessTree currently is supported only on Linux.\n",
      "16/02/01 16:22:33 INFO mapred.Task:  Using ResourceCalculatorProcessTree : null\n",
      "16/02/01 16:22:33 INFO mapred.ReduceTask: Using ShuffleConsumerPlugin: org.apache.hadoop.mapreduce.task.reduce.Shuffle@1c692098\n",
      "16/02/01 16:22:33 INFO reduce.MergeManagerImpl: MergerManager: memoryLimit=333971456, maxSingleShuffleLimit=83492864, mergeThreshold=220421168, ioSortFactor=10, memToMemMergeOutputsThreshold=10\n",
      "16/02/01 16:22:33 INFO reduce.EventFetcher: attempt_local1154151827_0001_r_000003_0 Thread started: EventFetcher for fetching Map Completion Events\n",
      "16/02/01 16:22:33 INFO reduce.LocalFetcher: localfetcher#4 about to shuffle output of map attempt_local1154151827_0001_m_000000_0 decomp: 11 len: 15 to MEMORY\n",
      "16/02/01 16:22:33 INFO reduce.InMemoryMapOutput: Read 11 bytes from map-output for attempt_local1154151827_0001_m_000000_0\n",
      "16/02/01 16:22:33 INFO reduce.MergeManagerImpl: closeInMemoryFile -> map-output of size: 11, inMemoryMapOutputs.size() -> 1, commitMemory -> 0, usedMemory ->11\n",
      "16/02/01 16:22:33 INFO reduce.EventFetcher: EventFetcher is interrupted.. Returning\n",
      "16/02/01 16:22:33 INFO mapred.LocalJobRunner: 1 / 1 copied.\n",
      "16/02/01 16:22:33 INFO reduce.MergeManagerImpl: finalMerge called with 1 in-memory map-outputs and 0 on-disk map-outputs\n",
      "16/02/01 16:22:33 INFO mapred.Merger: Merging 1 sorted segments\n",
      "16/02/01 16:22:33 INFO mapred.Merger: Down to the last merge-pass, with 1 segments left of total size: 4 bytes\n",
      "16/02/01 16:22:33 INFO reduce.MergeManagerImpl: Merged 1 segments, 11 bytes to disk to satisfy reduce memory limit\n",
      "16/02/01 16:22:33 INFO reduce.MergeManagerImpl: Merging 1 files, 15 bytes from disk\n",
      "16/02/01 16:22:33 INFO reduce.MergeManagerImpl: Merging 0 segments, 0 bytes from memory into reduce\n",
      "16/02/01 16:22:33 INFO mapred.Merger: Merging 1 sorted segments\n",
      "16/02/01 16:22:33 INFO mapred.Merger: Down to the last merge-pass, with 1 segments left of total size: 4 bytes\n",
      "16/02/01 16:22:33 INFO mapred.LocalJobRunner: 1 / 1 copied.\n",
      "16/02/01 16:22:33 INFO streaming.PipeMapRed: PipeMapRed exec [/Users/JingXu/Dropbox/DataScience/W261/W261/HW3/./reducer.py]\n",
      "16/02/01 16:22:33 INFO streaming.PipeMapRed: R/W/S=1/0/0 in:NA [rec/s] out:NA [rec/s]\n",
      "16/02/01 16:22:33 INFO streaming.PipeMapRed: Records R/W=1/1\n",
      "16/02/01 16:22:33 INFO streaming.PipeMapRed: MRErrorThread done\n",
      "16/02/01 16:22:33 INFO streaming.PipeMapRed: mapRedFinished\n",
      "16/02/01 16:22:33 INFO mapred.Task: Task:attempt_local1154151827_0001_r_000003_0 is done. And is in the process of committing\n",
      "16/02/01 16:22:33 INFO mapred.LocalJobRunner: 1 / 1 copied.\n",
      "16/02/01 16:22:33 INFO mapred.Task: Task attempt_local1154151827_0001_r_000003_0 is allowed to commit now\n",
      "16/02/01 16:22:33 INFO output.FileOutputCommitter: Saved output of task 'attempt_local1154151827_0001_r_000003_0' to hdfs://localhost:9000/user/jing/test-output1/_temporary/0/task_local1154151827_0001_r_000003\n",
      "16/02/01 16:22:33 INFO mapred.LocalJobRunner: Records R/W=1/1 > reduce\n",
      "16/02/01 16:22:33 INFO mapred.Task: Task 'attempt_local1154151827_0001_r_000003_0' done.\n",
      "16/02/01 16:22:33 INFO mapred.LocalJobRunner: Finishing task: attempt_local1154151827_0001_r_000003_0\n",
      "16/02/01 16:22:33 INFO mapred.LocalJobRunner: reduce task executor complete.\n",
      "16/02/01 16:22:33 INFO mapreduce.Job: Job job_local1154151827_0001 running in uber mode : false\n",
      "16/02/01 16:22:33 INFO mapreduce.Job:  map 100% reduce 100%\n",
      "16/02/01 16:22:33 INFO mapreduce.Job: Job job_local1154151827_0001 completed successfully\n",
      "16/02/01 16:22:33 INFO mapreduce.Job: Counters: 38\n",
      "\tFile System Counters\n",
      "\t\tFILE: Number of bytes read=17751\n",
      "\t\tFILE: Number of bytes written=1320229\n",
      "\t\tFILE: Number of read operations=0\n",
      "\t\tFILE: Number of large read operations=0\n",
      "\t\tFILE: Number of write operations=0\n",
      "\t\tHDFS: Number of bytes read=155\n",
      "\t\tHDFS: Number of bytes written=65\n",
      "\t\tHDFS: Number of read operations=60\n",
      "\t\tHDFS: Number of large read operations=0\n",
      "\t\tHDFS: Number of write operations=25\n",
      "\tMap-Reduce Framework\n",
      "\t\tMap input records=1\n",
      "\t\tMap output records=7\n",
      "\t\tMap output bytes=45\n",
      "\t\tMap output materialized bytes=83\n",
      "\t\tInput split bytes=93\n",
      "\t\tCombine input records=0\n",
      "\t\tCombine output records=0\n",
      "\t\tReduce input groups=4\n",
      "\t\tReduce shuffle bytes=83\n",
      "\t\tReduce input records=7\n",
      "\t\tReduce output records=4\n",
      "\t\tSpilled Records=14\n",
      "\t\tShuffled Maps =4\n",
      "\t\tFailed Shuffles=0\n",
      "\t\tMerged Map outputs=4\n",
      "\t\tGC time elapsed (ms)=10\n",
      "\t\tTotal committed heap usage (bytes)=1557659648\n",
      "\tMapCounter\n",
      "\t\tInstances=1\n",
      "\t\tLines=1\n",
      "\tReducerCounter\n",
      "\t\tTotal=4\n",
      "\tShuffle Errors\n",
      "\t\tBAD_ID=0\n",
      "\t\tCONNECTION=0\n",
      "\t\tIO_ERROR=0\n",
      "\t\tWRONG_LENGTH=0\n",
      "\t\tWRONG_MAP=0\n",
      "\t\tWRONG_REDUCE=0\n",
      "\tFile Input Format Counters \n",
      "\t\tBytes Read=31\n",
      "\tFile Output Format Counters \n",
      "\t\tBytes Written=26\n",
      "16/02/01 16:22:33 INFO streaming.StreamJob: Output directory: /user/jing/test-output1/\n",
      "DEPRECATED: Use of this script to execute hdfs command is deprecated.\n",
      "Instead use the hdfs command for it.\n",
      "\n",
      "16/02/01 16:22:34 WARN util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "quux\t2\n",
      "rmr: DEPRECATED: Please use 'rm -r' instead.\n",
      "16/02/01 16:22:36 WARN util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "16/02/01 16:22:37 INFO fs.TrashPolicyDefault: Namenode trash configuration: Deletion interval = 0 minutes, Emptier interval = 0 minutes.\n",
      "Deleted /user/jing\n",
      "rmr: DEPRECATED: Please use 'rm -r' instead.\n",
      "16/02/01 16:22:38 WARN util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "16/02/01 16:22:38 INFO fs.TrashPolicyDefault: Namenode trash configuration: Deletion interval = 0 minutes, Emptier interval = 0 minutes.\n",
      "Deleted /user\n"
     ]
    }
   ],
   "source": [
    "!./mapreduce.sh"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "MapCounter\n",
    "\t\n",
    "    Total=1\n",
    "\n",
    "ReduceCounter\n",
    "\t\n",
    "    Total=4\n",
    "\n",
    "This indicates that 1 mapper is being called within the Hadoop MR framework, because the input file has only 1 line that is being read. 4 reducers are being called because that is the amount I set to be called."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting mapper.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile mapper.py\n",
    "#!/usr/bin/python\n",
    "## mapper.py\n",
    "## Author: Jing Xu\n",
    "## Description: mapper code for HW3.2.2\n",
    "\n",
    "import sys\n",
    "import csv\n",
    "import re\n",
    "\n",
    "sys.stderr.write(\"reporter:counter:MapCounter,Instances,1\\n\")\n",
    "\n",
    "WORD_RE = re.compile(r\"[\\w']+\") #Compile regex to easily parse complete words\n",
    "\n",
    "reader = csv.reader(sys.stdin)\n",
    "# read input from STDIN (standard input)\n",
    "for line in reader:\n",
    "    # split the line into words\n",
    "    issue = line[3]\n",
    "    words = re.findall(WORD_RE,issue) #create list of words\n",
    "    for word in words:\n",
    "        print '%s\\t%s' % (word.lower(), 1)\n",
    "    sys.stderr.write(\"reporter:counter:MapperCounter,Lines,1\\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting reducer.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile reducer.py\n",
    "#!/usr/bin/python\n",
    "## reducer.py\n",
    "## Author: Jing Xu\n",
    "## Description: reducer code for HW3.2.2\n",
    "\n",
    "import sys\n",
    "\n",
    "current_word = None\n",
    "current_count = 0\n",
    "word = None\n",
    "\n",
    "# input comes from STDIN\n",
    "for line in sys.stdin:\n",
    "    # parse the input we got from mapper.py\n",
    "    word, count = line.split('\\t', 1)\n",
    "    # convert count (currently a string) to int\n",
    "    try:\n",
    "        count = int(count)\n",
    "    except ValueError:\n",
    "        # count was not a number, so silently\n",
    "        # ignore/discard this line\n",
    "        continue\n",
    "    if current_word == word:\n",
    "        current_count += count\n",
    "    else:\n",
    "        if current_word:\n",
    "            # write result to STDOUT\n",
    "            print '%s\\t%d' % (current_word, current_count)\n",
    "        current_count = count\n",
    "        current_word = word\n",
    "    sys.stderr.write(\"reporter:counter:ReducerCounter,Lines,1\\n\")\n",
    "        \n",
    "# do not forget to output the last word if needed!\n",
    "if current_word == word:\n",
    "    print '%s\\t%d' % (current_word, current_count)\n",
    "\n",
    "sys.stderr.write(\"reporter:counter:ReducerCounter,Instances,1\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting mapreduce.sh\n"
     ]
    }
   ],
   "source": [
    "%%writefile mapreduce.sh\n",
    "## mapreduce.sh\n",
    "## Author: Jing Xu\n",
    "## Description: mapreduce bash script for HW3.2.2\n",
    "\n",
    "hdfs dfs -mkdir /user # create hdfs folder\n",
    "wait\n",
    "hdfs dfs -mkdir /user/jing # create hdfs folder\n",
    "wait\n",
    "hadoop fs -put Consumer_Complaints.csv /user/jing # upload local file to hdfs folder\n",
    "wait\n",
    "\n",
    "# hadoop command to run streaming mapreduce job\n",
    "#sort primarly by numeric descending in 2nd key, then alphabetically on 1st key\n",
    "hadoop jar /Users/JingXu/Documents/hadoop-2.6.3/share/hadoop/tools/lib/hadoop-streaming-2.6.3.jar \\\n",
    "-D mapred.job.name=\"Count Job via Streaming\" \\\n",
    "-D mapred.map.tasks=2 \\\n",
    "-D mapred.reduce.tasks=2 \\\n",
    "-D stream.num.map.output.key.fields=2 \\\n",
    "-D mapreduce.job.output.key.comparator.class=org.apache.hadoop.mapred.lib.KeyFieldBasedComparator \\\n",
    "-D mapred.text.keycomparator.options=\"-k2,2nr -k1,1\" \\\n",
    "-file mapper.py    -mapper mapper.py \\\n",
    "-file reducer.py   -reducer reducer.py \\\n",
    "-input /user/jing/* -output /user/jing/test-output1/\n",
    "wait\n",
    "\n",
    "hadoop dfs -cat /user/jing/test-output1/part-00000\n",
    "wait\n",
    "\n",
    "hdfs dfs -rmr /user/jing # remove hdfs folder\n",
    "hdfs dfs -rmr /user # remove hdfs folder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "16/02/01 18:30:32 WARN util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "16/02/01 18:30:34 WARN util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "16/02/01 18:30:36 WARN util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "16/02/01 18:30:38 WARN streaming.StreamJob: -file option is deprecated, please use generic option -files instead.\n",
      "16/02/01 18:30:39 WARN util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "packageJobJar: [mapper.py, reducer.py] [] /var/folders/zs/k144hqks281fbt0x68c_zj9m0000gp/T/streamjob4520530247144752552.jar tmpDir=null\n",
      "16/02/01 18:30:39 INFO Configuration.deprecation: session.id is deprecated. Instead, use dfs.metrics.session-id\n",
      "16/02/01 18:30:39 INFO jvm.JvmMetrics: Initializing JVM Metrics with processName=JobTracker, sessionId=\n",
      "16/02/01 18:30:39 INFO jvm.JvmMetrics: Cannot initialize JVM Metrics with processName=JobTracker, sessionId= - already initialized\n",
      "16/02/01 18:30:40 INFO mapred.FileInputFormat: Total input paths to process : 1\n",
      "16/02/01 18:30:40 INFO mapreduce.JobSubmitter: number of splits:1\n",
      "16/02/01 18:30:40 INFO Configuration.deprecation: mapred.reduce.tasks is deprecated. Instead, use mapreduce.job.reduces\n",
      "16/02/01 18:30:40 INFO Configuration.deprecation: mapred.job.name is deprecated. Instead, use mapreduce.job.name\n",
      "16/02/01 18:30:40 INFO Configuration.deprecation: mapred.map.tasks is deprecated. Instead, use mapreduce.job.maps\n",
      "16/02/01 18:30:40 INFO mapreduce.JobSubmitter: Submitting tokens for job: job_local2103077367_0001\n",
      "16/02/01 18:30:40 INFO mapred.LocalDistributedCacheManager: Localized file:/Users/JingXu/Dropbox/DataScience/W261/W261/HW3/mapper.py as file:/tmp/hadoop-JingXu/mapred/local/1454380240803/mapper.py\n",
      "16/02/01 18:30:41 INFO mapred.LocalDistributedCacheManager: Localized file:/Users/JingXu/Dropbox/DataScience/W261/W261/HW3/reducer.py as file:/tmp/hadoop-JingXu/mapred/local/1454380240804/reducer.py\n",
      "16/02/01 18:30:41 INFO mapreduce.Job: The url to track the job: http://localhost:8080/\n",
      "16/02/01 18:30:41 INFO mapred.LocalJobRunner: OutputCommitter set in config null\n",
      "16/02/01 18:30:41 INFO mapreduce.Job: Running job: job_local2103077367_0001\n",
      "16/02/01 18:30:41 INFO mapred.LocalJobRunner: OutputCommitter is org.apache.hadoop.mapred.FileOutputCommitter\n",
      "16/02/01 18:30:41 INFO mapred.LocalJobRunner: Waiting for map tasks\n",
      "16/02/01 18:30:41 INFO mapred.LocalJobRunner: Starting task: attempt_local2103077367_0001_m_000000_0\n",
      "16/02/01 18:30:41 INFO util.ProcfsBasedProcessTree: ProcfsBasedProcessTree currently is supported only on Linux.\n",
      "16/02/01 18:30:41 INFO mapred.Task:  Using ResourceCalculatorProcessTree : null\n",
      "16/02/01 18:30:41 INFO mapred.MapTask: Processing split: hdfs://localhost:9000/user/jing/Consumer_Complaints.csv:0+50906486\n",
      "16/02/01 18:30:41 INFO mapred.MapTask: numReduceTasks: 2\n",
      "16/02/01 18:30:41 INFO mapred.MapTask: (EQUATOR) 0 kvi 26214396(104857584)\n",
      "16/02/01 18:30:41 INFO mapred.MapTask: mapreduce.task.io.sort.mb: 100\n",
      "16/02/01 18:30:41 INFO mapred.MapTask: soft limit at 83886080\n",
      "16/02/01 18:30:41 INFO mapred.MapTask: bufstart = 0; bufvoid = 104857600\n",
      "16/02/01 18:30:41 INFO mapred.MapTask: kvstart = 26214396; length = 6553600\n",
      "16/02/01 18:30:41 INFO mapred.MapTask: Map output collector class = org.apache.hadoop.mapred.MapTask$MapOutputBuffer\n",
      "16/02/01 18:30:41 INFO streaming.PipeMapRed: PipeMapRed exec [/Users/JingXu/Dropbox/DataScience/W261/W261/HW3/./mapper.py]\n",
      "16/02/01 18:30:41 INFO Configuration.deprecation: mapred.task.id is deprecated. Instead, use mapreduce.task.attempt.id\n",
      "16/02/01 18:30:41 INFO Configuration.deprecation: user.name is deprecated. Instead, use mapreduce.job.user.name\n",
      "16/02/01 18:30:41 INFO Configuration.deprecation: map.input.start is deprecated. Instead, use mapreduce.map.input.start\n",
      "16/02/01 18:30:41 INFO Configuration.deprecation: mapred.task.is.map is deprecated. Instead, use mapreduce.task.ismap\n",
      "16/02/01 18:30:41 INFO Configuration.deprecation: mapred.tip.id is deprecated. Instead, use mapreduce.task.id\n",
      "16/02/01 18:30:41 INFO Configuration.deprecation: mapred.skip.on is deprecated. Instead, use mapreduce.job.skiprecords\n",
      "16/02/01 18:30:41 INFO Configuration.deprecation: mapred.task.partition is deprecated. Instead, use mapreduce.task.partition\n",
      "16/02/01 18:30:41 INFO Configuration.deprecation: map.input.length is deprecated. Instead, use mapreduce.map.input.length\n",
      "16/02/01 18:30:41 INFO Configuration.deprecation: mapred.local.dir is deprecated. Instead, use mapreduce.cluster.local.dir\n",
      "16/02/01 18:30:41 INFO Configuration.deprecation: mapred.work.output.dir is deprecated. Instead, use mapreduce.task.output.dir\n",
      "16/02/01 18:30:41 INFO Configuration.deprecation: map.input.file is deprecated. Instead, use mapreduce.map.input.file\n",
      "16/02/01 18:30:41 INFO Configuration.deprecation: mapred.job.id is deprecated. Instead, use mapreduce.job.id\n",
      "16/02/01 18:30:41 INFO streaming.PipeMapRed: R/W/S=1/0/0 in:NA [rec/s] out:NA [rec/s]\n",
      "16/02/01 18:30:41 INFO streaming.PipeMapRed: R/W/S=10/0/0 in:NA [rec/s] out:NA [rec/s]\n",
      "16/02/01 18:30:41 INFO streaming.PipeMapRed: R/W/S=100/0/0 in:NA [rec/s] out:NA [rec/s]\n",
      "16/02/01 18:30:41 INFO streaming.PipeMapRed: Records R/W=849/1\n",
      "16/02/01 18:30:41 INFO streaming.PipeMapRed: R/W/S=1000/456/0 in:NA [rec/s] out:NA [rec/s]\n",
      "16/02/01 18:30:41 INFO streaming.PipeMapRed: R/W/S=10000/41101/0 in:NA [rec/s] out:NA [rec/s]\n",
      "16/02/01 18:30:42 INFO mapreduce.Job: Job job_local2103077367_0001 running in uber mode : false\n",
      "16/02/01 18:30:42 INFO mapreduce.Job:  map 0% reduce 0%\n",
      "16/02/01 18:30:43 INFO streaming.PipeMapRed: R/W/S=100000/448120/0 in:50000=100000/2 [rec/s] out:224060=448120/2 [rec/s]\n",
      "16/02/01 18:30:45 INFO streaming.PipeMapRed: R/W/S=200000/884533/0 in:66666=200000/3 [rec/s] out:294844=884533/3 [rec/s]\n",
      "16/02/01 18:30:46 INFO streaming.PipeMapRed: R/W/S=300000/1296450/0 in:60000=300000/5 [rec/s] out:259290=1296450/5 [rec/s]\n",
      "16/02/01 18:30:47 INFO streaming.PipeMapRed: MRErrorThread done\n",
      "16/02/01 18:30:47 INFO streaming.PipeMapRed: mapRedFinished\n",
      "16/02/01 18:30:47 INFO mapred.LocalJobRunner: \n",
      "16/02/01 18:30:47 INFO mapred.MapTask: Starting flush of map output\n",
      "16/02/01 18:30:47 INFO mapred.MapTask: Spilling map output\n",
      "16/02/01 18:30:47 INFO mapred.MapTask: bufstart = 0; bufend = 14773024; bufvoid = 104857600\n",
      "16/02/01 18:30:47 INFO mapred.MapTask: kvstart = 26214396(104857584); kvend = 20821164(83284656); length = 5393233/6553600\n",
      "16/02/01 18:30:47 INFO mapred.LocalJobRunner: Records R/W=849/1 > sort\n",
      "16/02/01 18:30:48 INFO mapreduce.Job:  map 67% reduce 0%\n",
      "16/02/01 18:30:48 INFO mapred.MapTask: Finished spill 0\n",
      "16/02/01 18:30:48 INFO mapred.Task: Task:attempt_local2103077367_0001_m_000000_0 is done. And is in the process of committing\n",
      "16/02/01 18:30:48 INFO mapred.LocalJobRunner: Records R/W=849/1\n",
      "16/02/01 18:30:48 INFO mapred.Task: Task 'attempt_local2103077367_0001_m_000000_0' done.\n",
      "16/02/01 18:30:48 INFO mapred.LocalJobRunner: Finishing task: attempt_local2103077367_0001_m_000000_0\n",
      "16/02/01 18:30:48 INFO mapred.LocalJobRunner: map task executor complete.\n",
      "16/02/01 18:30:48 INFO mapred.LocalJobRunner: Waiting for reduce tasks\n",
      "16/02/01 18:30:48 INFO mapred.LocalJobRunner: Starting task: attempt_local2103077367_0001_r_000000_0\n",
      "16/02/01 18:30:48 INFO util.ProcfsBasedProcessTree: ProcfsBasedProcessTree currently is supported only on Linux.\n",
      "16/02/01 18:30:48 INFO mapred.Task:  Using ResourceCalculatorProcessTree : null\n",
      "16/02/01 18:30:48 INFO mapred.ReduceTask: Using ShuffleConsumerPlugin: org.apache.hadoop.mapreduce.task.reduce.Shuffle@5b87e83e\n",
      "16/02/01 18:30:48 INFO reduce.MergeManagerImpl: MergerManager: memoryLimit=333971456, maxSingleShuffleLimit=83492864, mergeThreshold=220421168, ioSortFactor=10, memToMemMergeOutputsThreshold=10\n",
      "16/02/01 18:30:48 INFO reduce.EventFetcher: attempt_local2103077367_0001_r_000000_0 Thread started: EventFetcher for fetching Map Completion Events\n",
      "16/02/01 18:30:48 INFO reduce.LocalFetcher: localfetcher#1 about to shuffle output of map attempt_local2103077367_0001_m_000000_0 decomp: 8485867 len: 8485871 to MEMORY\n",
      "16/02/01 18:30:48 INFO reduce.InMemoryMapOutput: Read 8485867 bytes from map-output for attempt_local2103077367_0001_m_000000_0\n",
      "16/02/01 18:30:48 INFO reduce.MergeManagerImpl: closeInMemoryFile -> map-output of size: 8485867, inMemoryMapOutputs.size() -> 1, commitMemory -> 0, usedMemory ->8485867\n",
      "16/02/01 18:30:48 INFO reduce.EventFetcher: EventFetcher is interrupted.. Returning\n",
      "16/02/01 18:30:48 INFO mapred.LocalJobRunner: 1 / 1 copied.\n",
      "16/02/01 18:30:48 INFO reduce.MergeManagerImpl: finalMerge called with 1 in-memory map-outputs and 0 on-disk map-outputs\n",
      "16/02/01 18:30:48 INFO mapred.Merger: Merging 1 sorted segments\n",
      "16/02/01 18:30:48 INFO mapred.Merger: Down to the last merge-pass, with 1 segments left of total size: 8485861 bytes\n",
      "16/02/01 18:30:49 INFO mapreduce.Job:  map 100% reduce 0%\n",
      "16/02/01 18:30:49 INFO reduce.MergeManagerImpl: Merged 1 segments, 8485867 bytes to disk to satisfy reduce memory limit\n",
      "16/02/01 18:30:49 INFO reduce.MergeManagerImpl: Merging 1 files, 8485871 bytes from disk\n",
      "16/02/01 18:30:49 INFO reduce.MergeManagerImpl: Merging 0 segments, 0 bytes from memory into reduce\n",
      "16/02/01 18:30:49 INFO mapred.Merger: Merging 1 sorted segments\n",
      "16/02/01 18:30:49 INFO mapred.Merger: Down to the last merge-pass, with 1 segments left of total size: 8485861 bytes\n",
      "16/02/01 18:30:49 INFO mapred.LocalJobRunner: 1 / 1 copied.\n",
      "16/02/01 18:30:49 INFO streaming.PipeMapRed: PipeMapRed exec [/Users/JingXu/Dropbox/DataScience/W261/W261/HW3/./reducer.py]\n",
      "16/02/01 18:30:49 INFO Configuration.deprecation: mapred.job.tracker is deprecated. Instead, use mapreduce.jobtracker.address\n",
      "16/02/01 18:30:49 INFO Configuration.deprecation: mapred.map.tasks is deprecated. Instead, use mapreduce.job.maps\n",
      "16/02/01 18:30:49 INFO streaming.PipeMapRed: R/W/S=1/0/0 in:NA [rec/s] out:NA [rec/s]\n",
      "16/02/01 18:30:49 INFO streaming.PipeMapRed: R/W/S=10/0/0 in:NA [rec/s] out:NA [rec/s]\n",
      "16/02/01 18:30:49 INFO streaming.PipeMapRed: R/W/S=100/0/0 in:NA [rec/s] out:NA [rec/s]\n",
      "16/02/01 18:30:49 INFO streaming.PipeMapRed: R/W/S=1000/0/0 in:NA [rec/s] out:NA [rec/s]\n",
      "16/02/01 18:30:49 INFO streaming.PipeMapRed: R/W/S=10000/0/0 in:NA [rec/s] out:NA [rec/s]\n",
      "16/02/01 18:30:50 INFO streaming.PipeMapRed: R/W/S=100000/0/0 in:NA [rec/s] out:NA [rec/s]\n",
      "16/02/01 18:30:50 INFO streaming.PipeMapRed: R/W/S=200000/0/0 in:NA [rec/s] out:NA [rec/s]\n",
      "16/02/01 18:30:50 INFO streaming.PipeMapRed: R/W/S=300000/0/0 in:300000=300000/1 [rec/s] out:0=0/1 [rec/s]\n",
      "16/02/01 18:30:51 INFO streaming.PipeMapRed: R/W/S=400000/0/0 in:400000=400000/1 [rec/s] out:0=0/1 [rec/s]\n",
      "16/02/01 18:30:51 INFO streaming.PipeMapRed: R/W/S=500000/0/0 in:250000=500000/2 [rec/s] out:0=0/2 [rec/s]\n",
      "16/02/01 18:30:51 INFO streaming.PipeMapRed: R/W/S=600000/0/0 in:300000=600000/2 [rec/s] out:0=0/2 [rec/s]\n",
      "16/02/01 18:30:52 INFO streaming.PipeMapRed: MRErrorThread done\n",
      "16/02/01 18:30:52 INFO streaming.PipeMapRed: Records R/W=687449/1\n",
      "16/02/01 18:30:52 INFO streaming.PipeMapRed: mapRedFinished\n",
      "16/02/01 18:30:52 INFO mapred.Task: Task:attempt_local2103077367_0001_r_000000_0 is done. And is in the process of committing\n",
      "16/02/01 18:30:52 INFO mapred.LocalJobRunner: 1 / 1 copied.\n",
      "16/02/01 18:30:52 INFO mapred.Task: Task attempt_local2103077367_0001_r_000000_0 is allowed to commit now\n",
      "16/02/01 18:30:52 INFO output.FileOutputCommitter: Saved output of task 'attempt_local2103077367_0001_r_000000_0' to hdfs://localhost:9000/user/jing/test-output1/_temporary/0/task_local2103077367_0001_r_000000\n",
      "16/02/01 18:30:52 INFO mapred.LocalJobRunner: Records R/W=687449/1 > reduce\n",
      "16/02/01 18:30:52 INFO mapred.Task: Task 'attempt_local2103077367_0001_r_000000_0' done.\n",
      "16/02/01 18:30:52 INFO mapred.LocalJobRunner: Finishing task: attempt_local2103077367_0001_r_000000_0\n",
      "16/02/01 18:30:52 INFO mapred.LocalJobRunner: Starting task: attempt_local2103077367_0001_r_000001_0\n",
      "16/02/01 18:30:52 INFO util.ProcfsBasedProcessTree: ProcfsBasedProcessTree currently is supported only on Linux.\n",
      "16/02/01 18:30:52 INFO mapred.Task:  Using ResourceCalculatorProcessTree : null\n",
      "16/02/01 18:30:52 INFO mapred.ReduceTask: Using ShuffleConsumerPlugin: org.apache.hadoop.mapreduce.task.reduce.Shuffle@4ad5e1aa\n",
      "16/02/01 18:30:52 INFO reduce.MergeManagerImpl: MergerManager: memoryLimit=333971456, maxSingleShuffleLimit=83492864, mergeThreshold=220421168, ioSortFactor=10, memToMemMergeOutputsThreshold=10\n",
      "16/02/01 18:30:52 INFO reduce.EventFetcher: attempt_local2103077367_0001_r_000001_0 Thread started: EventFetcher for fetching Map Completion Events\n",
      "16/02/01 18:30:52 INFO reduce.LocalFetcher: localfetcher#2 about to shuffle output of map attempt_local2103077367_0001_m_000000_0 decomp: 8983779 len: 8983783 to MEMORY\n",
      "16/02/01 18:30:52 INFO reduce.InMemoryMapOutput: Read 8983779 bytes from map-output for attempt_local2103077367_0001_m_000000_0\n",
      "16/02/01 18:30:52 INFO reduce.MergeManagerImpl: closeInMemoryFile -> map-output of size: 8983779, inMemoryMapOutputs.size() -> 1, commitMemory -> 0, usedMemory ->8983779\n",
      "16/02/01 18:30:52 INFO reduce.EventFetcher: EventFetcher is interrupted.. Returning\n",
      "16/02/01 18:30:52 INFO mapred.LocalJobRunner: 1 / 1 copied.\n",
      "16/02/01 18:30:52 INFO reduce.MergeManagerImpl: finalMerge called with 1 in-memory map-outputs and 0 on-disk map-outputs\n",
      "16/02/01 18:30:52 INFO mapred.Merger: Merging 1 sorted segments\n",
      "16/02/01 18:30:52 INFO mapred.Merger: Down to the last merge-pass, with 1 segments left of total size: 8983768 bytes\n",
      "16/02/01 18:30:52 INFO reduce.MergeManagerImpl: Merged 1 segments, 8983779 bytes to disk to satisfy reduce memory limit\n",
      "16/02/01 18:30:52 INFO reduce.MergeManagerImpl: Merging 1 files, 8983783 bytes from disk\n",
      "16/02/01 18:30:52 INFO reduce.MergeManagerImpl: Merging 0 segments, 0 bytes from memory into reduce\n",
      "16/02/01 18:30:52 INFO mapred.Merger: Merging 1 sorted segments\n",
      "16/02/01 18:30:52 INFO mapred.Merger: Down to the last merge-pass, with 1 segments left of total size: 8983768 bytes\n",
      "16/02/01 18:30:52 INFO mapred.LocalJobRunner: 1 / 1 copied.\n",
      "16/02/01 18:30:52 INFO streaming.PipeMapRed: PipeMapRed exec [/Users/JingXu/Dropbox/DataScience/W261/W261/HW3/./reducer.py]\n",
      "16/02/01 18:30:52 INFO streaming.PipeMapRed: R/W/S=1/0/0 in:NA [rec/s] out:NA [rec/s]\n",
      "16/02/01 18:30:52 INFO streaming.PipeMapRed: R/W/S=10/0/0 in:NA [rec/s] out:NA [rec/s]\n",
      "16/02/01 18:30:52 INFO streaming.PipeMapRed: R/W/S=100/0/0 in:NA [rec/s] out:NA [rec/s]\n",
      "16/02/01 18:30:52 INFO streaming.PipeMapRed: R/W/S=1000/0/0 in:NA [rec/s] out:NA [rec/s]\n",
      "16/02/01 18:30:52 INFO streaming.PipeMapRed: R/W/S=10000/0/0 in:NA [rec/s] out:NA [rec/s]\n",
      "16/02/01 18:30:53 INFO streaming.PipeMapRed: R/W/S=100000/0/0 in:NA [rec/s] out:NA [rec/s]\n",
      "16/02/01 18:30:53 INFO mapreduce.Job:  map 100% reduce 50%\n",
      "16/02/01 18:30:53 INFO streaming.PipeMapRed: R/W/S=200000/0/0 in:NA [rec/s] out:NA [rec/s]\n",
      "16/02/01 18:30:54 INFO streaming.PipeMapRed: R/W/S=300000/0/0 in:300000=300000/1 [rec/s] out:0=0/1 [rec/s]\n",
      "16/02/01 18:30:54 INFO streaming.PipeMapRed: R/W/S=400000/0/0 in:400000=400000/1 [rec/s] out:0=0/1 [rec/s]\n",
      "16/02/01 18:30:55 INFO streaming.PipeMapRed: R/W/S=500000/0/0 in:250000=500000/2 [rec/s] out:0=0/2 [rec/s]\n",
      "16/02/01 18:30:55 INFO streaming.PipeMapRed: R/W/S=600000/0/0 in:300000=600000/2 [rec/s] out:0=0/2 [rec/s]\n",
      "16/02/01 18:30:55 INFO streaming.PipeMapRed: Records R/W=660860/1\n",
      "16/02/01 18:30:55 INFO streaming.PipeMapRed: MRErrorThread done\n",
      "16/02/01 18:30:55 INFO streaming.PipeMapRed: mapRedFinished\n",
      "16/02/01 18:30:55 INFO mapred.Task: Task:attempt_local2103077367_0001_r_000001_0 is done. And is in the process of committing\n",
      "16/02/01 18:30:55 INFO mapred.LocalJobRunner: 1 / 1 copied.\n",
      "16/02/01 18:30:55 INFO mapred.Task: Task attempt_local2103077367_0001_r_000001_0 is allowed to commit now\n",
      "16/02/01 18:30:55 INFO output.FileOutputCommitter: Saved output of task 'attempt_local2103077367_0001_r_000001_0' to hdfs://localhost:9000/user/jing/test-output1/_temporary/0/task_local2103077367_0001_r_000001\n",
      "16/02/01 18:30:55 INFO mapred.LocalJobRunner: Records R/W=660860/1 > reduce\n",
      "16/02/01 18:30:55 INFO mapred.Task: Task 'attempt_local2103077367_0001_r_000001_0' done.\n",
      "16/02/01 18:30:55 INFO mapred.LocalJobRunner: Finishing task: attempt_local2103077367_0001_r_000001_0\n",
      "16/02/01 18:30:55 INFO mapred.LocalJobRunner: reduce task executor complete.\n",
      "16/02/01 18:30:56 INFO mapreduce.Job:  map 100% reduce 100%\n",
      "16/02/01 18:30:56 INFO mapreduce.Job: Job job_local2103077367_0001 completed successfully\n",
      "16/02/01 18:30:56 INFO mapreduce.Job: Counters: 39\n",
      "\tFile System Counters\n",
      "\t\tFILE: Number of bytes read=51927907\n",
      "\t\tFILE: Number of bytes written=79158671\n",
      "\t\tFILE: Number of read operations=0\n",
      "\t\tFILE: Number of large read operations=0\n",
      "\t\tFILE: Number of write operations=0\n",
      "\t\tHDFS: Number of bytes read=152719458\n",
      "\t\tHDFS: Number of bytes written=3205\n",
      "\t\tHDFS: Number of read operations=27\n",
      "\t\tHDFS: Number of large read operations=0\n",
      "\t\tHDFS: Number of write operations=9\n",
      "\tMap-Reduce Framework\n",
      "\t\tMap input records=312913\n",
      "\t\tMap output records=1348309\n",
      "\t\tMap output bytes=14773024\n",
      "\t\tMap output materialized bytes=17469654\n",
      "\t\tInput split bytes=107\n",
      "\t\tCombine input records=0\n",
      "\t\tCombine output records=0\n",
      "\t\tReduce input groups=174\n",
      "\t\tReduce shuffle bytes=17469654\n",
      "\t\tReduce input records=1348309\n",
      "\t\tReduce output records=174\n",
      "\t\tSpilled Records=2696618\n",
      "\t\tShuffled Maps =2\n",
      "\t\tFailed Shuffles=0\n",
      "\t\tMerged Map outputs=2\n",
      "\t\tGC time elapsed (ms)=71\n",
      "\t\tTotal committed heap usage (bytes)=871890944\n",
      "\tMapCounter\n",
      "\t\tInstances=1\n",
      "\tMapperCounter\n",
      "\t\tLines=312913\n",
      "\tReducerCounter\n",
      "\t\tInstances=2\n",
      "\t\tLines=1348309\n",
      "\tShuffle Errors\n",
      "\t\tBAD_ID=0\n",
      "\t\tCONNECTION=0\n",
      "\t\tIO_ERROR=0\n",
      "\t\tWRONG_LENGTH=0\n",
      "\t\tWRONG_MAP=0\n",
      "\t\tWRONG_REDUCE=0\n",
      "\tFile Input Format Counters \n",
      "\t\tBytes Read=50906486\n",
      "\tFile Output Format Counters \n",
      "\t\tBytes Written=2174\n",
      "16/02/01 18:30:56 INFO streaming.StreamJob: Output directory: /user/jing/test-output1/\n",
      "DEPRECATED: Use of this script to execute hdfs command is deprecated.\n",
      "Instead use the hdfs command for it.\n",
      "\n",
      "16/02/01 18:30:57 WARN util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "a\t3503\n",
      "account\t57448\n",
      "acct\t163\n",
      "an\t2964\n",
      "and\t16448\n",
      "applied\t139\n",
      "apr\t3431\n",
      "arbitration\t168\n",
      "available\t274\n",
      "bankruptcy\t222\n",
      "being\t5663\n",
      "billing\t8158\n",
      "broker\t8625\n",
      "by\t5663\n",
      "can't\t1999\n",
      "cash\t240\n",
      "caused\t5663\n",
      "changes\t350\n",
      "charges\t131\n",
      "checks\t75\n",
      "closing\t19000\n",
      "company's\t4858\n",
      "cont'd\t17972\n",
      "convenience\t75\n",
      "credit\t55251\n",
      "debt\t27874\n",
      "delay\t243\n",
      "delinquent\t1061\n",
      "deposits\t10555\n",
      "determination\t1490\n",
      "did\t139\n",
      "disclosure\t7655\n",
      "disputes\t6938\n",
      "escrow\t36767\n",
      "expect\t807\n",
      "false\t3621\n",
      "fees\t807\n",
      "for\t929\n",
      "foreclosure\t70487\n",
      "i\t925\n",
      "incorrect\t29133\n",
      "issuance\t640\n",
      "issue\t1099\n",
      "making\t3226\n",
      "management\t16205\n",
      "not\t18477\n",
      "of\t13983\n",
      "on\t29069\n",
      "or\t40508\n",
      "overlimit\t127\n",
      "owed\t17972\n",
      "payments\t39993\n",
      "payoff\t1155\n",
      "process\t5505\n",
      "processing\t243\n",
      "promised\t274\n",
      "protection\t4139\n",
      "receive\t139\n",
      "received\t216\n",
      "relations\t1367\n",
      "repay\t1647\n",
      "repaying\t3844\n",
      "representation\t3621\n",
      "sale\t139\n",
      "service\t1518\n",
      "servicer\t1944\n",
      "settlement\t4350\n",
      "statement\t1220\n",
      "tactics\t8671\n",
      "terms\t350\n",
      "the\t6248\n",
      "theft\t3276\n",
      "threatening\t2964\n",
      "to\t8401\n",
      "transfer\t597\n",
      "unable\t8178\n",
      "unsolicited\t640\n",
      "use\t1477\n",
      "verification\t7655\n",
      "was\t274\n",
      "workout\t350\n",
      "wrong\t169\n",
      "you\t3821\n",
      "your\t3844\n",
      "rmr: DEPRECATED: Please use 'rm -r' instead.\n",
      "16/02/01 18:30:59 WARN util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "16/02/01 18:31:00 INFO fs.TrashPolicyDefault: Namenode trash configuration: Deletion interval = 0 minutes, Emptier interval = 0 minutes.\n",
      "Deleted /user/jing\n",
      "rmr: DEPRECATED: Please use 'rm -r' instead.\n",
      "16/02/01 18:31:02 WARN util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "16/02/01 18:31:02 INFO fs.TrashPolicyDefault: Namenode trash configuration: Deletion interval = 0 minutes, Emptier interval = 0 minutes.\n",
      "Deleted /user\n"
     ]
    }
   ],
   "source": [
    "!./mapreduce.sh"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are now 2 mapper instances and 312913 mapper line calls, one for each line in the file. There are now 2 reducer instances and 8350086 reducer word calls, one for each word in the Issues column. The two reducer outputs are not displaying a total sort - they are sorted individually within each reducer. This is not optimal."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting mapper.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile mapper.py\n",
    "#!/usr/bin/python\n",
    "## mapper.py\n",
    "## Author: Jing Xu\n",
    "## Description: mapper code for HW3.2.3\n",
    "\n",
    "import sys\n",
    "import csv\n",
    "import re\n",
    "\n",
    "sys.stderr.write(\"reporter:counter:MapperCounter,Instance,1\\n\")\n",
    "\n",
    "WORD_RE = re.compile(r\"[\\w']+\") #Compile regex to easily parse complete words\n",
    "\n",
    "reader = csv.reader(sys.stdin)\n",
    "# read input from STDIN (standard input)\n",
    "for line in reader:\n",
    "    # split the line into words\n",
    "    issue = line[3]\n",
    "    words = re.findall(WORD_RE,issue) #create list of words\n",
    "    for word in words:\n",
    "        print '%s\\t%s' % (word.lower(), 1)\n",
    "    sys.stderr.write(\"reporter:counter:MapperCounter,Line,1\\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting combiner.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile combiner.py\n",
    "#!/usr/bin/python\n",
    "## combiner.py\n",
    "## Author: Jing Xu\n",
    "## Description: combiner code for HW3.2.3\n",
    "\n",
    "import sys\n",
    "\n",
    "current_word = None\n",
    "current_count = 0\n",
    "word = None\n",
    "\n",
    "sys.stderr.write(\"reporter:counter:CombinerCounter,Instance,1\\n\")\n",
    "\n",
    "# input comes from STDIN\n",
    "for line in sys.stdin:\n",
    "    # parse the input we got from mapper.py\n",
    "    word, count = line.split('\\t', 1)\n",
    "    # convert count (currently a string) to int\n",
    "    try:\n",
    "        count = int(count)\n",
    "    except ValueError:\n",
    "        # count was not a number, so silently\n",
    "        # ignore/discard this line\n",
    "        continue\n",
    "    if current_word == word:\n",
    "        current_count += count\n",
    "    else:\n",
    "        if current_word:\n",
    "            # write result to STDOUT\n",
    "            print '%s\\t%s' % (current_word, current_count)\n",
    "        current_count = count\n",
    "        current_word = word\n",
    "    sys.stderr.write(\"reporter:counter:CombinerCounter,Line,1\\n\")\n",
    "\n",
    "# do not forget to output the last word if needed!\n",
    "if current_word == word:\n",
    "    print '%s\\t%s' % (current_word, current_count)\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting reducer.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile reducer.py\n",
    "#!/usr/bin/python\n",
    "## reducer.py\n",
    "## Author: Jing Xu\n",
    "## Description: reducer code for HW3.2.3\n",
    "\n",
    "import sys\n",
    "\n",
    "current_word = None\n",
    "current_count = 0\n",
    "word = None\n",
    "\n",
    "# input comes from STDIN\n",
    "for line in sys.stdin:\n",
    "    # parse the input we got from mapper.py\n",
    "    word, count = line.split('\\t', 1)\n",
    "    # convert count (currently a string) to int\n",
    "    try:\n",
    "        count = int(count)\n",
    "    except ValueError:\n",
    "        # count was not a number, so silently\n",
    "        # ignore/discard this line\n",
    "        continue\n",
    "    if current_word == word:\n",
    "        current_count += count\n",
    "    else:\n",
    "        if current_word:\n",
    "            # write result to STDOUT\n",
    "            print '%s\\t%s' % (current_word, current_count)\n",
    "        current_count = count\n",
    "        current_word = word\n",
    "    sys.stderr.write(\"reporter:counter:ReducerCounter,Line,1\\n\")\n",
    "\n",
    "# do not forget to output the last word if needed!\n",
    "if current_word == word:\n",
    "    print '%s\\t%s' % (current_word, current_count)\n",
    "\n",
    "sys.stderr.write(\"reporter:counter:ReducerCounter,Instance,1\\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting mapreduce.sh\n"
     ]
    }
   ],
   "source": [
    "%%writefile mapreduce.sh\n",
    "## mapreduce.sh\n",
    "## Author: Jing Xu\n",
    "## Description: mapreduce bash script for HW3.2.2\n",
    "\n",
    "hdfs dfs -mkdir /user # create hdfs folder\n",
    "wait\n",
    "hdfs dfs -mkdir /user/jing # create hdfs folder\n",
    "wait\n",
    "hadoop fs -put Consumer_Complaints.csv /user/jing # upload local file to hdfs folder\n",
    "wait\n",
    "\n",
    "# hadoop command to run streaming mapreduce job\n",
    "hadoop jar /Users/JingXu/Documents/hadoop-2.6.3/share/hadoop/tools/lib/hadoop-streaming-2.6.3.jar \\\n",
    "-D mapred.job.name=\"Count Job via Streaming\" \\\n",
    "-D mapred.map.tasks=2 \\\n",
    "-D mapred.reduce.tasks=2 \\\n",
    "-file mapper.py    -mapper mapper.py \\\n",
    "-file reducer.py   -reducer reducer.py \\\n",
    "-file reducer.py   -combiner combiner.py \\\n",
    "-input /user/jing/* -output /user/jing/test-output1/\n",
    "wait\n",
    "\n",
    "hadoop dfs -cat /user/jing/test-output1/part-00000\n",
    "wait\n",
    "\n",
    "hdfs dfs -rmr /user/jing # remove hdfs folder\n",
    "hdfs dfs -rmr /user # remove hdfs folder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "!chmod a+x combiner.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "16/02/01 16:26:00 WARN util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "16/02/01 16:26:01 WARN util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "16/02/01 16:26:03 WARN util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "16/02/01 16:26:05 WARN streaming.StreamJob: -file option is deprecated, please use generic option -files instead.\n",
      "16/02/01 16:26:05 WARN util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "packageJobJar: [mapper.py, reducer.py, reducer.py] [] /var/folders/zs/k144hqks281fbt0x68c_zj9m0000gp/T/streamjob7066278641568308671.jar tmpDir=null\n",
      "16/02/01 16:26:06 INFO Configuration.deprecation: session.id is deprecated. Instead, use dfs.metrics.session-id\n",
      "16/02/01 16:26:06 INFO jvm.JvmMetrics: Initializing JVM Metrics with processName=JobTracker, sessionId=\n",
      "16/02/01 16:26:06 INFO jvm.JvmMetrics: Cannot initialize JVM Metrics with processName=JobTracker, sessionId= - already initialized\n",
      "16/02/01 16:26:06 INFO mapred.FileInputFormat: Total input paths to process : 1\n",
      "16/02/01 16:26:07 INFO mapreduce.JobSubmitter: number of splits:1\n",
      "16/02/01 16:26:07 INFO Configuration.deprecation: mapred.reduce.tasks is deprecated. Instead, use mapreduce.job.reduces\n",
      "16/02/01 16:26:07 INFO Configuration.deprecation: mapred.job.name is deprecated. Instead, use mapreduce.job.name\n",
      "16/02/01 16:26:07 INFO Configuration.deprecation: mapred.map.tasks is deprecated. Instead, use mapreduce.job.maps\n",
      "16/02/01 16:26:07 INFO mapreduce.JobSubmitter: Submitting tokens for job: job_local1317367831_0001\n",
      "16/02/01 16:26:07 INFO mapred.LocalDistributedCacheManager: Localized file:/Users/JingXu/Dropbox/DataScience/W261/W261/HW3/mapper.py as file:/tmp/hadoop-JingXu/mapred/local/1454372767334/mapper.py\n",
      "16/02/01 16:26:07 INFO mapred.LocalDistributedCacheManager: Localized file:/Users/JingXu/Dropbox/DataScience/W261/W261/HW3/reducer.py as file:/tmp/hadoop-JingXu/mapred/local/1454372767335/reducer.py\n",
      "16/02/01 16:26:07 INFO mapreduce.Job: The url to track the job: http://localhost:8080/\n",
      "16/02/01 16:26:07 INFO mapred.LocalJobRunner: OutputCommitter set in config null\n",
      "16/02/01 16:26:07 INFO mapreduce.Job: Running job: job_local1317367831_0001\n",
      "16/02/01 16:26:07 INFO mapred.LocalJobRunner: OutputCommitter is org.apache.hadoop.mapred.FileOutputCommitter\n",
      "16/02/01 16:26:07 INFO mapred.LocalJobRunner: Waiting for map tasks\n",
      "16/02/01 16:26:07 INFO mapred.LocalJobRunner: Starting task: attempt_local1317367831_0001_m_000000_0\n",
      "16/02/01 16:26:07 INFO util.ProcfsBasedProcessTree: ProcfsBasedProcessTree currently is supported only on Linux.\n",
      "16/02/01 16:26:07 INFO mapred.Task:  Using ResourceCalculatorProcessTree : null\n",
      "16/02/01 16:26:07 INFO mapred.MapTask: Processing split: hdfs://localhost:9000/user/jing/Consumer_Complaints.csv:0+50906486\n",
      "16/02/01 16:26:07 INFO mapred.MapTask: numReduceTasks: 2\n",
      "16/02/01 16:26:07 INFO mapred.MapTask: (EQUATOR) 0 kvi 26214396(104857584)\n",
      "16/02/01 16:26:07 INFO mapred.MapTask: mapreduce.task.io.sort.mb: 100\n",
      "16/02/01 16:26:07 INFO mapred.MapTask: soft limit at 83886080\n",
      "16/02/01 16:26:07 INFO mapred.MapTask: bufstart = 0; bufvoid = 104857600\n",
      "16/02/01 16:26:07 INFO mapred.MapTask: kvstart = 26214396; length = 6553600\n",
      "16/02/01 16:26:07 INFO mapred.MapTask: Map output collector class = org.apache.hadoop.mapred.MapTask$MapOutputBuffer\n",
      "16/02/01 16:26:07 INFO streaming.PipeMapRed: PipeMapRed exec [/Users/JingXu/Dropbox/DataScience/W261/W261/HW3/./mapper.py]\n",
      "16/02/01 16:26:07 INFO Configuration.deprecation: mapred.task.id is deprecated. Instead, use mapreduce.task.attempt.id\n",
      "16/02/01 16:26:07 INFO Configuration.deprecation: user.name is deprecated. Instead, use mapreduce.job.user.name\n",
      "16/02/01 16:26:07 INFO Configuration.deprecation: map.input.start is deprecated. Instead, use mapreduce.map.input.start\n",
      "16/02/01 16:26:07 INFO Configuration.deprecation: mapred.task.is.map is deprecated. Instead, use mapreduce.task.ismap\n",
      "16/02/01 16:26:07 INFO Configuration.deprecation: mapred.tip.id is deprecated. Instead, use mapreduce.task.id\n",
      "16/02/01 16:26:07 INFO Configuration.deprecation: mapred.skip.on is deprecated. Instead, use mapreduce.job.skiprecords\n",
      "16/02/01 16:26:07 INFO Configuration.deprecation: mapred.task.partition is deprecated. Instead, use mapreduce.task.partition\n",
      "16/02/01 16:26:07 INFO Configuration.deprecation: map.input.length is deprecated. Instead, use mapreduce.map.input.length\n",
      "16/02/01 16:26:07 INFO Configuration.deprecation: mapred.local.dir is deprecated. Instead, use mapreduce.cluster.local.dir\n",
      "16/02/01 16:26:07 INFO Configuration.deprecation: mapred.work.output.dir is deprecated. Instead, use mapreduce.task.output.dir\n",
      "16/02/01 16:26:07 INFO Configuration.deprecation: map.input.file is deprecated. Instead, use mapreduce.map.input.file\n",
      "16/02/01 16:26:07 INFO Configuration.deprecation: mapred.job.id is deprecated. Instead, use mapreduce.job.id\n",
      "16/02/01 16:26:07 INFO streaming.PipeMapRed: R/W/S=1/0/0 in:NA [rec/s] out:NA [rec/s]\n",
      "16/02/01 16:26:07 INFO streaming.PipeMapRed: R/W/S=10/0/0 in:NA [rec/s] out:NA [rec/s]\n",
      "16/02/01 16:26:07 INFO streaming.PipeMapRed: R/W/S=100/0/0 in:NA [rec/s] out:NA [rec/s]\n",
      "16/02/01 16:26:07 INFO streaming.PipeMapRed: Records R/W=880/1\n",
      "16/02/01 16:26:07 INFO streaming.PipeMapRed: R/W/S=1000/186/0 in:NA [rec/s] out:NA [rec/s]\n",
      "16/02/01 16:26:08 INFO streaming.PipeMapRed: R/W/S=10000/41101/0 in:NA [rec/s] out:NA [rec/s]\n",
      "16/02/01 16:26:08 INFO mapreduce.Job: Job job_local1317367831_0001 running in uber mode : false\n",
      "16/02/01 16:26:08 INFO mapreduce.Job:  map 0% reduce 0%\n",
      "16/02/01 16:26:09 INFO streaming.PipeMapRed: R/W/S=100000/448120/0 in:100000=100000/1 [rec/s] out:448120=448120/1 [rec/s]\n",
      "16/02/01 16:26:11 INFO streaming.PipeMapRed: R/W/S=200000/884533/0 in:66666=200000/3 [rec/s] out:294844=884533/3 [rec/s]\n",
      "16/02/01 16:26:12 INFO streaming.PipeMapRed: R/W/S=300000/1296450/0 in:75000=300000/4 [rec/s] out:324112=1296450/4 [rec/s]\n",
      "16/02/01 16:26:12 INFO streaming.PipeMapRed: MRErrorThread done\n",
      "16/02/01 16:26:12 INFO streaming.PipeMapRed: mapRedFinished\n",
      "16/02/01 16:26:12 INFO mapred.LocalJobRunner: \n",
      "16/02/01 16:26:12 INFO mapred.MapTask: Starting flush of map output\n",
      "16/02/01 16:26:12 INFO mapred.MapTask: Spilling map output\n",
      "16/02/01 16:26:12 INFO mapred.MapTask: bufstart = 0; bufend = 13424715; bufvoid = 104857600\n",
      "16/02/01 16:26:12 INFO mapred.MapTask: kvstart = 26214396(104857584); kvend = 20821164(83284656); length = 5393233/6553600\n",
      "16/02/01 16:26:13 INFO streaming.PipeMapRed: PipeMapRed exec [/Users/JingXu/Dropbox/DataScience/W261/W261/HW3/./combiner.py]\n",
      "16/02/01 16:26:13 INFO Configuration.deprecation: mapred.skip.map.auto.incr.proc.count is deprecated. Instead, use mapreduce.map.skip.proc-count.auto-incr\n",
      "16/02/01 16:26:13 INFO streaming.PipeMapRed: R/W/S=1/0/0 in:NA [rec/s] out:NA [rec/s]\n",
      "16/02/01 16:26:13 INFO streaming.PipeMapRed: R/W/S=10/0/0 in:NA [rec/s] out:NA [rec/s]\n",
      "16/02/01 16:26:13 INFO streaming.PipeMapRed: R/W/S=100/0/0 in:NA [rec/s] out:NA [rec/s]\n",
      "16/02/01 16:26:13 INFO streaming.PipeMapRed: R/W/S=1000/0/0 in:NA [rec/s] out:NA [rec/s]\n",
      "16/02/01 16:26:13 INFO streaming.PipeMapRed: R/W/S=10000/0/0 in:NA [rec/s] out:NA [rec/s]\n",
      "16/02/01 16:26:13 INFO streaming.PipeMapRed: R/W/S=100000/0/0 in:NA [rec/s] out:NA [rec/s]\n",
      "16/02/01 16:26:13 INFO mapred.LocalJobRunner: Records R/W=880/1 > sort\n",
      "16/02/01 16:26:13 INFO streaming.PipeMapRed: R/W/S=200000/0/0 in:NA [rec/s] out:NA [rec/s]\n",
      "16/02/01 16:26:14 INFO streaming.PipeMapRed: R/W/S=300000/0/0 in:300000=300000/1 [rec/s] out:0=0/1 [rec/s]\n",
      "16/02/01 16:26:14 INFO streaming.PipeMapRed: R/W/S=400000/0/0 in:400000=400000/1 [rec/s] out:0=0/1 [rec/s]\n",
      "16/02/01 16:26:14 INFO mapreduce.Job:  map 67% reduce 0%\n",
      "16/02/01 16:26:14 INFO streaming.PipeMapRed: R/W/S=500000/0/0 in:500000=500000/1 [rec/s] out:0=0/1 [rec/s]\n",
      "16/02/01 16:26:15 INFO streaming.PipeMapRed: R/W/S=600000/0/0 in:600000=600000/1 [rec/s] out:0=0/1 [rec/s]\n",
      "16/02/01 16:26:15 INFO streaming.PipeMapRed: Records R/W=687449/1\n",
      "16/02/01 16:26:15 INFO streaming.PipeMapRed: MRErrorThread done\n",
      "16/02/01 16:26:15 INFO streaming.PipeMapRed: mapRedFinished\n",
      "16/02/01 16:26:15 INFO streaming.PipeMapRed: PipeMapRed exec [/Users/JingXu/Dropbox/DataScience/W261/W261/HW3/./combiner.py]\n",
      "16/02/01 16:26:15 INFO Configuration.deprecation: mapred.skip.reduce.auto.incr.proc.count is deprecated. Instead, use mapreduce.reduce.skip.proc-count.auto-incr\n",
      "16/02/01 16:26:15 INFO streaming.PipeMapRed: R/W/S=1/0/0 in:NA [rec/s] out:NA [rec/s]\n",
      "16/02/01 16:26:15 INFO streaming.PipeMapRed: R/W/S=10/0/0 in:NA [rec/s] out:NA [rec/s]\n",
      "16/02/01 16:26:15 INFO streaming.PipeMapRed: R/W/S=100/0/0 in:NA [rec/s] out:NA [rec/s]\n",
      "16/02/01 16:26:15 INFO streaming.PipeMapRed: R/W/S=1000/0/0 in:NA [rec/s] out:NA [rec/s]\n",
      "16/02/01 16:26:15 INFO streaming.PipeMapRed: R/W/S=10000/0/0 in:NA [rec/s] out:NA [rec/s]\n",
      "16/02/01 16:26:15 INFO streaming.PipeMapRed: R/W/S=100000/0/0 in:NA [rec/s] out:NA [rec/s]\n",
      "16/02/01 16:26:16 INFO streaming.PipeMapRed: R/W/S=200000/0/0 in:NA [rec/s] out:NA [rec/s]\n",
      "16/02/01 16:26:16 INFO streaming.PipeMapRed: R/W/S=300000/0/0 in:NA [rec/s] out:NA [rec/s]\n",
      "16/02/01 16:26:16 INFO mapred.LocalJobRunner: Records R/W=687449/1 > sort\n",
      "16/02/01 16:26:16 INFO streaming.PipeMapRed: R/W/S=400000/0/0 in:400000=400000/1 [rec/s] out:0=0/1 [rec/s]\n",
      "16/02/01 16:26:17 INFO streaming.PipeMapRed: R/W/S=500000/0/0 in:500000=500000/1 [rec/s] out:0=0/1 [rec/s]\n",
      "16/02/01 16:26:17 INFO streaming.PipeMapRed: R/W/S=600000/0/0 in:300000=600000/2 [rec/s] out:0=0/2 [rec/s]\n",
      "16/02/01 16:26:17 INFO streaming.PipeMapRed: Records R/W=660860/1\n",
      "16/02/01 16:26:17 INFO streaming.PipeMapRed: MRErrorThread done\n",
      "16/02/01 16:26:17 INFO streaming.PipeMapRed: mapRedFinished\n",
      "16/02/01 16:26:17 INFO mapred.MapTask: Finished spill 0\n",
      "16/02/01 16:26:17 INFO mapred.Task: Task:attempt_local1317367831_0001_m_000000_0 is done. And is in the process of committing\n",
      "16/02/01 16:26:17 INFO mapred.LocalJobRunner: Records R/W=660860/1\n",
      "16/02/01 16:26:17 INFO mapred.Task: Task 'attempt_local1317367831_0001_m_000000_0' done.\n",
      "16/02/01 16:26:17 INFO mapred.LocalJobRunner: Finishing task: attempt_local1317367831_0001_m_000000_0\n",
      "16/02/01 16:26:17 INFO mapred.LocalJobRunner: map task executor complete.\n",
      "16/02/01 16:26:17 INFO mapred.LocalJobRunner: Waiting for reduce tasks\n",
      "16/02/01 16:26:17 INFO mapred.LocalJobRunner: Starting task: attempt_local1317367831_0001_r_000000_0\n",
      "16/02/01 16:26:17 INFO util.ProcfsBasedProcessTree: ProcfsBasedProcessTree currently is supported only on Linux.\n",
      "16/02/01 16:26:17 INFO mapred.Task:  Using ResourceCalculatorProcessTree : null\n",
      "16/02/01 16:26:17 INFO mapred.ReduceTask: Using ShuffleConsumerPlugin: org.apache.hadoop.mapreduce.task.reduce.Shuffle@afb9176\n",
      "16/02/01 16:26:17 INFO reduce.MergeManagerImpl: MergerManager: memoryLimit=333971456, maxSingleShuffleLimit=83492864, mergeThreshold=220421168, ioSortFactor=10, memToMemMergeOutputsThreshold=10\n",
      "16/02/01 16:26:17 INFO reduce.EventFetcher: attempt_local1317367831_0001_r_000000_0 Thread started: EventFetcher for fetching Map Completion Events\n",
      "16/02/01 16:26:17 INFO reduce.LocalFetcher: localfetcher#1 about to shuffle output of map attempt_local1317367831_0001_m_000000_0 decomp: 1201 len: 1205 to MEMORY\n",
      "16/02/01 16:26:17 INFO reduce.InMemoryMapOutput: Read 1201 bytes from map-output for attempt_local1317367831_0001_m_000000_0\n",
      "16/02/01 16:26:17 INFO reduce.MergeManagerImpl: closeInMemoryFile -> map-output of size: 1201, inMemoryMapOutputs.size() -> 1, commitMemory -> 0, usedMemory ->1201\n",
      "16/02/01 16:26:17 INFO reduce.EventFetcher: EventFetcher is interrupted.. Returning\n",
      "16/02/01 16:26:17 INFO mapred.LocalJobRunner: 1 / 1 copied.\n",
      "16/02/01 16:26:17 INFO reduce.MergeManagerImpl: finalMerge called with 1 in-memory map-outputs and 0 on-disk map-outputs\n",
      "16/02/01 16:26:17 INFO mapred.Merger: Merging 1 sorted segments\n",
      "16/02/01 16:26:17 INFO mapred.Merger: Down to the last merge-pass, with 1 segments left of total size: 1197 bytes\n",
      "16/02/01 16:26:17 INFO reduce.MergeManagerImpl: Merged 1 segments, 1201 bytes to disk to satisfy reduce memory limit\n",
      "16/02/01 16:26:17 INFO reduce.MergeManagerImpl: Merging 1 files, 1205 bytes from disk\n",
      "16/02/01 16:26:17 INFO reduce.MergeManagerImpl: Merging 0 segments, 0 bytes from memory into reduce\n",
      "16/02/01 16:26:17 INFO mapred.Merger: Merging 1 sorted segments\n",
      "16/02/01 16:26:17 INFO mapred.Merger: Down to the last merge-pass, with 1 segments left of total size: 1197 bytes\n",
      "16/02/01 16:26:17 INFO mapred.LocalJobRunner: 1 / 1 copied.\n",
      "16/02/01 16:26:17 INFO streaming.PipeMapRed: PipeMapRed exec [/Users/JingXu/Dropbox/DataScience/W261/W261/HW3/./reducer.py]\n",
      "16/02/01 16:26:17 INFO Configuration.deprecation: mapred.job.tracker is deprecated. Instead, use mapreduce.jobtracker.address\n",
      "16/02/01 16:26:17 INFO Configuration.deprecation: mapred.map.tasks is deprecated. Instead, use mapreduce.job.maps\n",
      "16/02/01 16:26:17 INFO streaming.PipeMapRed: R/W/S=1/0/0 in:NA [rec/s] out:NA [rec/s]\n",
      "16/02/01 16:26:17 INFO streaming.PipeMapRed: R/W/S=10/0/0 in:NA [rec/s] out:NA [rec/s]\n",
      "16/02/01 16:26:17 INFO streaming.PipeMapRed: MRErrorThread done\n",
      "16/02/01 16:26:17 INFO streaming.PipeMapRed: Records R/W=84/1\n",
      "16/02/01 16:26:17 INFO streaming.PipeMapRed: mapRedFinished\n",
      "16/02/01 16:26:18 INFO mapred.Task: Task:attempt_local1317367831_0001_r_000000_0 is done. And is in the process of committing\n",
      "16/02/01 16:26:18 INFO mapred.LocalJobRunner: 1 / 1 copied.\n",
      "16/02/01 16:26:18 INFO mapred.Task: Task attempt_local1317367831_0001_r_000000_0 is allowed to commit now\n",
      "16/02/01 16:26:18 INFO output.FileOutputCommitter: Saved output of task 'attempt_local1317367831_0001_r_000000_0' to hdfs://localhost:9000/user/jing/test-output1/_temporary/0/task_local1317367831_0001_r_000000\n",
      "16/02/01 16:26:18 INFO mapred.LocalJobRunner: Records R/W=84/1 > reduce\n",
      "16/02/01 16:26:18 INFO mapred.Task: Task 'attempt_local1317367831_0001_r_000000_0' done.\n",
      "16/02/01 16:26:18 INFO mapred.LocalJobRunner: Finishing task: attempt_local1317367831_0001_r_000000_0\n",
      "16/02/01 16:26:18 INFO mapred.LocalJobRunner: Starting task: attempt_local1317367831_0001_r_000001_0\n",
      "16/02/01 16:26:18 INFO util.ProcfsBasedProcessTree: ProcfsBasedProcessTree currently is supported only on Linux.\n",
      "16/02/01 16:26:18 INFO mapred.Task:  Using ResourceCalculatorProcessTree : null\n",
      "16/02/01 16:26:18 INFO mapred.ReduceTask: Using ShuffleConsumerPlugin: org.apache.hadoop.mapreduce.task.reduce.Shuffle@6b8d2c12\n",
      "16/02/01 16:26:18 INFO reduce.MergeManagerImpl: MergerManager: memoryLimit=333971456, maxSingleShuffleLimit=83492864, mergeThreshold=220421168, ioSortFactor=10, memToMemMergeOutputsThreshold=10\n",
      "16/02/01 16:26:18 INFO reduce.EventFetcher: attempt_local1317367831_0001_r_000001_0 Thread started: EventFetcher for fetching Map Completion Events\n",
      "16/02/01 16:26:18 INFO reduce.LocalFetcher: localfetcher#2 about to shuffle output of map attempt_local1317367831_0001_m_000000_0 decomp: 1325 len: 1329 to MEMORY\n",
      "16/02/01 16:26:18 INFO reduce.InMemoryMapOutput: Read 1325 bytes from map-output for attempt_local1317367831_0001_m_000000_0\n",
      "16/02/01 16:26:18 INFO reduce.MergeManagerImpl: closeInMemoryFile -> map-output of size: 1325, inMemoryMapOutputs.size() -> 1, commitMemory -> 0, usedMemory ->1325\n",
      "16/02/01 16:26:18 INFO reduce.EventFetcher: EventFetcher is interrupted.. Returning\n",
      "16/02/01 16:26:18 INFO mapred.LocalJobRunner: 1 / 1 copied.\n",
      "16/02/01 16:26:18 INFO reduce.MergeManagerImpl: finalMerge called with 1 in-memory map-outputs and 0 on-disk map-outputs\n",
      "16/02/01 16:26:18 INFO mapred.Merger: Merging 1 sorted segments\n",
      "16/02/01 16:26:18 INFO mapred.Merger: Down to the last merge-pass, with 1 segments left of total size: 1316 bytes\n",
      "16/02/01 16:26:18 INFO reduce.MergeManagerImpl: Merged 1 segments, 1325 bytes to disk to satisfy reduce memory limit\n",
      "16/02/01 16:26:18 INFO reduce.MergeManagerImpl: Merging 1 files, 1329 bytes from disk\n",
      "16/02/01 16:26:18 INFO reduce.MergeManagerImpl: Merging 0 segments, 0 bytes from memory into reduce\n",
      "16/02/01 16:26:18 INFO mapred.Merger: Merging 1 sorted segments\n",
      "16/02/01 16:26:18 INFO mapred.Merger: Down to the last merge-pass, with 1 segments left of total size: 1316 bytes\n",
      "16/02/01 16:26:18 INFO mapred.LocalJobRunner: 1 / 1 copied.\n",
      "16/02/01 16:26:18 INFO streaming.PipeMapRed: PipeMapRed exec [/Users/JingXu/Dropbox/DataScience/W261/W261/HW3/./reducer.py]\n",
      "16/02/01 16:26:18 INFO streaming.PipeMapRed: R/W/S=1/0/0 in:NA [rec/s] out:NA [rec/s]\n",
      "16/02/01 16:26:18 INFO streaming.PipeMapRed: R/W/S=10/0/0 in:NA [rec/s] out:NA [rec/s]\n",
      "16/02/01 16:26:18 INFO streaming.PipeMapRed: Records R/W=90/1\n",
      "16/02/01 16:26:18 INFO streaming.PipeMapRed: MRErrorThread done\n",
      "16/02/01 16:26:18 INFO streaming.PipeMapRed: mapRedFinished\n",
      "16/02/01 16:26:18 INFO mapred.Task: Task:attempt_local1317367831_0001_r_000001_0 is done. And is in the process of committing\n",
      "16/02/01 16:26:18 INFO mapred.LocalJobRunner: 1 / 1 copied.\n",
      "16/02/01 16:26:18 INFO mapred.Task: Task attempt_local1317367831_0001_r_000001_0 is allowed to commit now\n",
      "16/02/01 16:26:18 INFO output.FileOutputCommitter: Saved output of task 'attempt_local1317367831_0001_r_000001_0' to hdfs://localhost:9000/user/jing/test-output1/_temporary/0/task_local1317367831_0001_r_000001\n",
      "16/02/01 16:26:18 INFO mapred.LocalJobRunner: Records R/W=90/1 > reduce\n",
      "16/02/01 16:26:18 INFO mapred.Task: Task 'attempt_local1317367831_0001_r_000001_0' done.\n",
      "16/02/01 16:26:18 INFO mapred.LocalJobRunner: Finishing task: attempt_local1317367831_0001_r_000001_0\n",
      "16/02/01 16:26:18 INFO mapred.LocalJobRunner: reduce task executor complete.\n",
      "16/02/01 16:26:18 INFO mapreduce.Job:  map 100% reduce 100%\n",
      "16/02/01 16:26:18 INFO mapreduce.Job: Job job_local1317367831_0001 completed successfully\n",
      "16/02/01 16:26:18 INFO mapreduce.Job: Counters: 41\n",
      "\tFile System Counters\n",
      "\t\tFILE: Number of bytes read=18761\n",
      "\t\tFILE: Number of bytes written=805159\n",
      "\t\tFILE: Number of read operations=0\n",
      "\t\tFILE: Number of large read operations=0\n",
      "\t\tFILE: Number of write operations=0\n",
      "\t\tHDFS: Number of bytes read=152719458\n",
      "\t\tHDFS: Number of bytes written=3205\n",
      "\t\tHDFS: Number of read operations=27\n",
      "\t\tHDFS: Number of large read operations=0\n",
      "\t\tHDFS: Number of write operations=9\n",
      "\tMap-Reduce Framework\n",
      "\t\tMap input records=312913\n",
      "\t\tMap output records=1348309\n",
      "\t\tMap output bytes=13424715\n",
      "\t\tMap output materialized bytes=2534\n",
      "\t\tInput split bytes=107\n",
      "\t\tCombine input records=1348309\n",
      "\t\tCombine output records=174\n",
      "\t\tReduce input groups=174\n",
      "\t\tReduce shuffle bytes=2534\n",
      "\t\tReduce input records=174\n",
      "\t\tReduce output records=174\n",
      "\t\tSpilled Records=348\n",
      "\t\tShuffled Maps =2\n",
      "\t\tFailed Shuffles=0\n",
      "\t\tMerged Map outputs=2\n",
      "\t\tGC time elapsed (ms)=63\n",
      "\t\tTotal committed heap usage (bytes)=863502336\n",
      "\tCombinerCounter\n",
      "\t\tInstance=2\n",
      "\t\tLine=1348309\n",
      "\tMapperCounter\n",
      "\t\tInstance=1\n",
      "\t\tLine=312913\n",
      "\tReducerCounter\n",
      "\t\tInstance=2\n",
      "\t\tLine=174\n",
      "\tShuffle Errors\n",
      "\t\tBAD_ID=0\n",
      "\t\tCONNECTION=0\n",
      "\t\tIO_ERROR=0\n",
      "\t\tWRONG_LENGTH=0\n",
      "\t\tWRONG_MAP=0\n",
      "\t\tWRONG_REDUCE=0\n",
      "\tFile Input Format Counters \n",
      "\t\tBytes Read=50906486\n",
      "\tFile Output Format Counters \n",
      "\t\tBytes Written=2174\n",
      "16/02/01 16:26:18 INFO streaming.StreamJob: Output directory: /user/jing/test-output1/\n",
      "DEPRECATED: Use of this script to execute hdfs command is deprecated.\n",
      "Instead use the hdfs command for it.\n",
      "\n",
      "16/02/01 16:26:19 WARN util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "a\t3503\n",
      "account\t57448\n",
      "acct\t163\n",
      "an\t2964\n",
      "and\t16448\n",
      "applied\t139\n",
      "apr\t3431\n",
      "arbitration\t168\n",
      "available\t274\n",
      "bankruptcy\t222\n",
      "being\t5663\n",
      "billing\t8158\n",
      "broker\t8625\n",
      "by\t5663\n",
      "can't\t1999\n",
      "cash\t240\n",
      "caused\t5663\n",
      "changes\t350\n",
      "charges\t131\n",
      "checks\t75\n",
      "closing\t19000\n",
      "company's\t4858\n",
      "cont'd\t17972\n",
      "convenience\t75\n",
      "credit\t55251\n",
      "debt\t27874\n",
      "delay\t243\n",
      "delinquent\t1061\n",
      "deposits\t10555\n",
      "determination\t1490\n",
      "did\t139\n",
      "disclosure\t7655\n",
      "disputes\t6938\n",
      "escrow\t36767\n",
      "expect\t807\n",
      "false\t3621\n",
      "fees\t807\n",
      "for\t929\n",
      "foreclosure\t70487\n",
      "i\t925\n",
      "incorrect\t29133\n",
      "issuance\t640\n",
      "issue\t1099\n",
      "making\t3226\n",
      "management\t16205\n",
      "not\t18477\n",
      "of\t13983\n",
      "on\t29069\n",
      "or\t40508\n",
      "overlimit\t127\n",
      "owed\t17972\n",
      "payments\t39993\n",
      "payoff\t1155\n",
      "process\t5505\n",
      "processing\t243\n",
      "promised\t274\n",
      "protection\t4139\n",
      "receive\t139\n",
      "received\t216\n",
      "relations\t1367\n",
      "repay\t1647\n",
      "repaying\t3844\n",
      "representation\t3621\n",
      "sale\t139\n",
      "service\t1518\n",
      "servicer\t1944\n",
      "settlement\t4350\n",
      "statement\t1220\n",
      "tactics\t8671\n",
      "terms\t350\n",
      "the\t6248\n",
      "theft\t3276\n",
      "threatening\t2964\n",
      "to\t8401\n",
      "transfer\t597\n",
      "unable\t8178\n",
      "unsolicited\t640\n",
      "use\t1477\n",
      "verification\t7655\n",
      "was\t274\n",
      "workout\t350\n",
      "wrong\t169\n",
      "you\t3821\n",
      "your\t3844\n",
      "rmr: DEPRECATED: Please use 'rm -r' instead.\n",
      "16/02/01 16:26:21 WARN util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "16/02/01 16:26:22 INFO fs.TrashPolicyDefault: Namenode trash configuration: Deletion interval = 0 minutes, Emptier interval = 0 minutes.\n",
      "Deleted /user/jing\n",
      "rmr: DEPRECATED: Please use 'rm -r' instead.\n",
      "16/02/01 16:26:23 WARN util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "16/02/01 16:26:24 INFO fs.TrashPolicyDefault: Namenode trash configuration: Deletion interval = 0 minutes, Emptier interval = 0 minutes.\n",
      "Deleted /user\n"
     ]
    }
   ],
   "source": [
    "!./mapreduce.sh"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "CombinerCounter\n",
    "\n",
    "    Instance=4\n",
    "    Line=1348309\n",
    "\n",
    "MapperCounter\n",
    "\n",
    "    Instance=2\n",
    "    Line=312913\n",
    "\t\n",
    "ReducerCounter\n",
    "\n",
    "    Instance=2\n",
    "    Line=174"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting mapper.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile mapper.py\n",
    "#!/usr/bin/python\n",
    "## mapper.py\n",
    "## Author: Jing Xu\n",
    "## Description: mapper code for HW3.2.4\n",
    "\n",
    "import sys\n",
    "import csv\n",
    "import re\n",
    "\n",
    "WORD_RE = re.compile(r\"[\\w']+\") #Compile regex to easily parse complete words\n",
    "\n",
    "reader = csv.reader(sys.stdin)\n",
    "# read input from STDIN (standard input)\n",
    "for line in reader:\n",
    "    # split the line into words\n",
    "    issue = line[3]\n",
    "    words = re.findall(WORD_RE,issue) #create list of words\n",
    "    for word in words:\n",
    "        print '%s\\t%d' % ('*', 1)\n",
    "        print '%s\\t%d' % (word.lower(), 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting mapper2.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile mapper2.py\n",
    "#!/usr/bin/python\n",
    "## mapper2.py\n",
    "## Author: Jing Xu\n",
    "## Description: mapper2 code for HW3.2.4\n",
    "\n",
    "import sys\n",
    "\n",
    "# read input from STDIN (standard input)\n",
    "for line in sys.stdin:\n",
    "    # split the line into words\n",
    "    line = line.strip('\\n')\n",
    "    word, count = line.split('\\t', 1)\n",
    "    count = int(count)\n",
    "    print '%s\\t%d' % (word, count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting reducer.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile reducer.py\n",
    "#!/usr/bin/python\n",
    "## combiner.py\n",
    "## Author: Jing Xu\n",
    "## Description: combiner code for HW3.2.4\n",
    "\n",
    "import sys\n",
    "\n",
    "current_word = None\n",
    "current_count = 0\n",
    "word = None\n",
    "\n",
    "# input comes from STDIN\n",
    "for line in sys.stdin:\n",
    "    # parse the input we got from mapper.py\n",
    "    word, count = line.split('\\t', 1)\n",
    "    # convert count (currently a string) to int\n",
    "    try:\n",
    "        count = int(count)\n",
    "    except ValueError:\n",
    "        # count was not a number, so silently\n",
    "        # ignore/discard this line\n",
    "        continue\n",
    "    if current_word == word:\n",
    "        current_count += count\n",
    "    else:\n",
    "        if current_word:\n",
    "            # write result to STDOUT\n",
    "            print '%s\\t%d' % (current_word, current_count)\n",
    "        current_count = count\n",
    "        current_word = word\n",
    "\n",
    "# do not forget to output the last word if needed!\n",
    "if current_word == word:\n",
    "    print '%s\\t%d' % (current_word, current_count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting reducer2.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile reducer2.py\n",
    "#!/usr/bin/python\n",
    "## reducer2.py\n",
    "## Author: Jing Xu\n",
    "## Description: reducer2 code for HW3.2.4\n",
    "\n",
    "import sys, Queue\n",
    "\n",
    "n_max, n_min = 50, 10 #want top 50 largest numbers and 10 smallest numbers\n",
    "q_max = Queue.Queue(n_max) #queue for largest values\n",
    "a_min = [] #list for smallest values\n",
    "total_words = 0 #total words used to calculate relative frequency\n",
    "\n",
    "for line in sys.stdin:\n",
    "    line = line.strip('\\n') #clean line\n",
    "    rec = line.split('\\t', 1) #split line\n",
    "    if rec[0] == '*': #if key is *, indicates it is the total count value\n",
    "        rec[1] = rec[1].strip('\\t')\n",
    "        total_words = int(rec[1]) #assign count as total_words\n",
    "    else:\n",
    "        rec[1] = rec[1].strip('\\t')\n",
    "        rec[1] = int(rec[1])\n",
    "        # put the smalles value in the queue\n",
    "        if len(a_min) < n_min:\n",
    "            a_min.append([rec[0], rec[1]])\n",
    "        # whatever left is the biggest\n",
    "        if q_max.full():\n",
    "            q_max.get()\n",
    "        q_max.put([rec[0], rec[1]])\n",
    "\n",
    "print '\\n%d smallest records:' %n_min\n",
    "for record in a_min:\n",
    "    freq = float(record[1])/float(total_words) #calculate relative frequency\n",
    "    record.append(freq) #append relative frequency to output \n",
    "    print record\n",
    "\n",
    "print '\\n%d biggest records:' %n_max\n",
    "for i in range(n_max):\n",
    "    value = q_max.get() \n",
    "    freq = float(value[1])/float(total_words) #calculate relative frequency\n",
    "    value.append(freq) #append relative frequency to output \n",
    "    print value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting mapreduce.sh\n"
     ]
    }
   ],
   "source": [
    "%%writefile mapreduce.sh\n",
    "## mapreduce.sh\n",
    "## Author: Jing Xu\n",
    "## Description: mapreduce bash script for HW3.2.4\n",
    "\n",
    "hdfs dfs -mkdir /user # create hdfs folder\n",
    "wait\n",
    "hdfs dfs -mkdir /user/jing # create hdfs folder\n",
    "wait\n",
    "hadoop fs -put Consumer_Complaints.csv /user/jing # upload local file to hdfs folder\n",
    "wait\n",
    "\n",
    "# hadoop command to run streaming mapreduce job\n",
    "hadoop jar /Users/JingXu/Documents/hadoop-2.6.3/share/hadoop/tools/lib/hadoop-streaming-2.6.3.jar \\\n",
    "-D mapred.job.name=\"Count Job via Streaming\" \\\n",
    "-D mapred.map.tasks=1 \\\n",
    "-D mapred.reduce.tasks=1 \\\n",
    "-file mapper.py    -mapper mapper.py \\\n",
    "-file reducer.py   -reducer reducer.py \\\n",
    "-input /user/jing/* -output /user/jing/test-output1/\n",
    "\n",
    "wait\n",
    "\n",
    "# second hadoop command to run streaming mapreduce job\n",
    "hadoop jar /Users/JingXu/Documents/hadoop-2.6.3/share/hadoop/tools/lib/hadoop-streaming-2.6.3.jar \\\n",
    "-D mapred.job.name=\"Count Job via Streaming\" \\\n",
    "-D mapred.map.tasks=1 \\\n",
    "-D mapred.reduce.tasks=1 \\\n",
    "-D stream.num.map.output.key.fields=2 \\\n",
    "-D mapred.output.key.comparator.class=org.apache.hadoop.mapred.lib.KeyFieldBasedComparator \\\n",
    "-D mapred.text.key.comparator.options=\"-k2n -k1\" \\ #sort primarly by numeric descending in 2nd key, then alphabetically on 1st key\n",
    "-file mapper2.py    -mapper mapper2.py \\\n",
    "-file reducer2.py   -reducer reducer2.py \\\n",
    "-input /user/jing/test-output1/part-00000 -output /user/jing/test-output2/\n",
    "\n",
    "wait\n",
    "\n",
    "hadoop dfs -cat /user/jing/test-output2/part-00000 #examine hdfs file output\n",
    "wait\n",
    "\n",
    "hdfs dfs -rmr /user/jing # remove hdfs folder\n",
    "hdfs dfs -rmr /user # remove hdfs folder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "16/02/01 16:30:57 WARN util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "16/02/01 16:30:59 WARN util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "16/02/01 16:31:00 WARN util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "16/02/01 16:31:02 WARN streaming.StreamJob: -file option is deprecated, please use generic option -files instead.\n",
      "16/02/01 16:31:02 WARN util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "packageJobJar: [mapper.py, reducer.py] [] /var/folders/zs/k144hqks281fbt0x68c_zj9m0000gp/T/streamjob7591024917523853133.jar tmpDir=null\n",
      "16/02/01 16:31:03 INFO Configuration.deprecation: session.id is deprecated. Instead, use dfs.metrics.session-id\n",
      "16/02/01 16:31:03 INFO jvm.JvmMetrics: Initializing JVM Metrics with processName=JobTracker, sessionId=\n",
      "16/02/01 16:31:03 INFO jvm.JvmMetrics: Cannot initialize JVM Metrics with processName=JobTracker, sessionId= - already initialized\n",
      "16/02/01 16:31:03 INFO mapred.FileInputFormat: Total input paths to process : 1\n",
      "16/02/01 16:31:03 INFO mapreduce.JobSubmitter: number of splits:1\n",
      "16/02/01 16:31:03 INFO Configuration.deprecation: mapred.reduce.tasks is deprecated. Instead, use mapreduce.job.reduces\n",
      "16/02/01 16:31:03 INFO Configuration.deprecation: mapred.job.name is deprecated. Instead, use mapreduce.job.name\n",
      "16/02/01 16:31:03 INFO Configuration.deprecation: mapred.map.tasks is deprecated. Instead, use mapreduce.job.maps\n",
      "16/02/01 16:31:03 INFO mapreduce.JobSubmitter: Submitting tokens for job: job_local2102783744_0001\n",
      "16/02/01 16:31:04 INFO mapred.LocalDistributedCacheManager: Localized file:/Users/JingXu/Dropbox/DataScience/W261/W261/HW3/mapper.py as file:/tmp/hadoop-JingXu/mapred/local/1454373064055/mapper.py\n",
      "16/02/01 16:31:04 INFO mapred.LocalDistributedCacheManager: Localized file:/Users/JingXu/Dropbox/DataScience/W261/W261/HW3/reducer.py as file:/tmp/hadoop-JingXu/mapred/local/1454373064056/reducer.py\n",
      "16/02/01 16:31:04 INFO mapreduce.Job: The url to track the job: http://localhost:8080/\n",
      "16/02/01 16:31:04 INFO mapred.LocalJobRunner: OutputCommitter set in config null\n",
      "16/02/01 16:31:04 INFO mapreduce.Job: Running job: job_local2102783744_0001\n",
      "16/02/01 16:31:04 INFO mapred.LocalJobRunner: OutputCommitter is org.apache.hadoop.mapred.FileOutputCommitter\n",
      "16/02/01 16:31:04 INFO mapred.LocalJobRunner: Waiting for map tasks\n",
      "16/02/01 16:31:04 INFO mapred.LocalJobRunner: Starting task: attempt_local2102783744_0001_m_000000_0\n",
      "16/02/01 16:31:04 INFO util.ProcfsBasedProcessTree: ProcfsBasedProcessTree currently is supported only on Linux.\n",
      "16/02/01 16:31:04 INFO mapred.Task:  Using ResourceCalculatorProcessTree : null\n",
      "16/02/01 16:31:04 INFO mapred.MapTask: Processing split: hdfs://localhost:9000/user/jing/Consumer_Complaints.csv:0+50906486\n",
      "16/02/01 16:31:04 INFO mapred.MapTask: numReduceTasks: 1\n",
      "16/02/01 16:31:04 INFO mapred.MapTask: (EQUATOR) 0 kvi 26214396(104857584)\n",
      "16/02/01 16:31:04 INFO mapred.MapTask: mapreduce.task.io.sort.mb: 100\n",
      "16/02/01 16:31:04 INFO mapred.MapTask: soft limit at 83886080\n",
      "16/02/01 16:31:04 INFO mapred.MapTask: bufstart = 0; bufvoid = 104857600\n",
      "16/02/01 16:31:04 INFO mapred.MapTask: kvstart = 26214396; length = 6553600\n",
      "16/02/01 16:31:04 INFO mapred.MapTask: Map output collector class = org.apache.hadoop.mapred.MapTask$MapOutputBuffer\n",
      "16/02/01 16:31:04 INFO streaming.PipeMapRed: PipeMapRed exec [/Users/JingXu/Dropbox/DataScience/W261/W261/HW3/./mapper.py]\n",
      "16/02/01 16:31:04 INFO Configuration.deprecation: mapred.task.id is deprecated. Instead, use mapreduce.task.attempt.id\n",
      "16/02/01 16:31:04 INFO Configuration.deprecation: user.name is deprecated. Instead, use mapreduce.job.user.name\n",
      "16/02/01 16:31:04 INFO Configuration.deprecation: map.input.start is deprecated. Instead, use mapreduce.map.input.start\n",
      "16/02/01 16:31:04 INFO Configuration.deprecation: mapred.task.is.map is deprecated. Instead, use mapreduce.task.ismap\n",
      "16/02/01 16:31:04 INFO Configuration.deprecation: mapred.tip.id is deprecated. Instead, use mapreduce.task.id\n",
      "16/02/01 16:31:04 INFO Configuration.deprecation: mapred.skip.on is deprecated. Instead, use mapreduce.job.skiprecords\n",
      "16/02/01 16:31:04 INFO Configuration.deprecation: mapred.task.partition is deprecated. Instead, use mapreduce.task.partition\n",
      "16/02/01 16:31:04 INFO Configuration.deprecation: map.input.length is deprecated. Instead, use mapreduce.map.input.length\n",
      "16/02/01 16:31:04 INFO Configuration.deprecation: mapred.local.dir is deprecated. Instead, use mapreduce.cluster.local.dir\n",
      "16/02/01 16:31:04 INFO Configuration.deprecation: mapred.work.output.dir is deprecated. Instead, use mapreduce.task.output.dir\n",
      "16/02/01 16:31:04 INFO Configuration.deprecation: map.input.file is deprecated. Instead, use mapreduce.map.input.file\n",
      "16/02/01 16:31:04 INFO Configuration.deprecation: mapred.job.id is deprecated. Instead, use mapreduce.job.id\n",
      "16/02/01 16:31:04 INFO streaming.PipeMapRed: R/W/S=1/0/0 in:NA [rec/s] out:NA [rec/s]\n",
      "16/02/01 16:31:04 INFO streaming.PipeMapRed: R/W/S=10/0/0 in:NA [rec/s] out:NA [rec/s]\n",
      "16/02/01 16:31:04 INFO streaming.PipeMapRed: R/W/S=100/0/0 in:NA [rec/s] out:NA [rec/s]\n",
      "16/02/01 16:31:04 INFO streaming.PipeMapRed: Records R/W=779/1\n",
      "16/02/01 16:31:04 INFO streaming.PipeMapRed: R/W/S=1000/275/0 in:NA [rec/s] out:NA [rec/s]\n",
      "16/02/01 16:31:04 INFO streaming.PipeMapRed: R/W/S=10000/82125/0 in:NA [rec/s] out:NA [rec/s]\n",
      "16/02/01 16:31:05 INFO mapreduce.Job: Job job_local2102783744_0001 running in uber mode : false\n",
      "16/02/01 16:31:05 INFO mapreduce.Job:  map 0% reduce 0%\n",
      "16/02/01 16:31:06 INFO streaming.PipeMapRed: R/W/S=100000/897653/0 in:50000=100000/2 [rec/s] out:448826=897653/2 [rec/s]\n",
      "16/02/01 16:31:08 INFO streaming.PipeMapRed: R/W/S=200000/1769184/0 in:50000=200000/4 [rec/s] out:442296=1769184/4 [rec/s]\n",
      "16/02/01 16:31:10 INFO mapred.LocalJobRunner: Records R/W=779/1 > map\n",
      "16/02/01 16:31:10 INFO streaming.PipeMapRed: R/W/S=300000/2591682/0 in:50000=300000/6 [rec/s] out:431947=2591682/6 [rec/s]\n",
      "16/02/01 16:31:10 INFO streaming.PipeMapRed: MRErrorThread done\n",
      "16/02/01 16:31:10 INFO streaming.PipeMapRed: mapRedFinished\n",
      "16/02/01 16:31:10 INFO mapred.LocalJobRunner: Records R/W=779/1 > map\n",
      "16/02/01 16:31:10 INFO mapred.MapTask: Starting flush of map output\n",
      "16/02/01 16:31:10 INFO mapred.MapTask: Spilling map output\n",
      "16/02/01 16:31:10 INFO mapred.MapTask: bufstart = 0; bufend = 18817951; bufvoid = 104857600\n",
      "16/02/01 16:31:10 INFO mapred.MapTask: kvstart = 26214396(104857584); kvend = 15427928(61711712); length = 10786469/6553600\n",
      "16/02/01 16:31:11 INFO mapreduce.Job:  map 60% reduce 0%\n",
      "16/02/01 16:31:13 INFO mapred.LocalJobRunner: Records R/W=779/1 > sort\n",
      "16/02/01 16:31:13 INFO mapred.MapTask: Finished spill 0\n",
      "16/02/01 16:31:13 INFO mapred.Task: Task:attempt_local2102783744_0001_m_000000_0 is done. And is in the process of committing\n",
      "16/02/01 16:31:13 INFO mapred.LocalJobRunner: Records R/W=779/1\n",
      "16/02/01 16:31:13 INFO mapred.Task: Task 'attempt_local2102783744_0001_m_000000_0' done.\n",
      "16/02/01 16:31:13 INFO mapred.LocalJobRunner: Finishing task: attempt_local2102783744_0001_m_000000_0\n",
      "16/02/01 16:31:13 INFO mapred.LocalJobRunner: map task executor complete.\n",
      "16/02/01 16:31:13 INFO mapred.LocalJobRunner: Waiting for reduce tasks\n",
      "16/02/01 16:31:13 INFO mapred.LocalJobRunner: Starting task: attempt_local2102783744_0001_r_000000_0\n",
      "16/02/01 16:31:13 INFO util.ProcfsBasedProcessTree: ProcfsBasedProcessTree currently is supported only on Linux.\n",
      "16/02/01 16:31:13 INFO mapred.Task:  Using ResourceCalculatorProcessTree : null\n",
      "16/02/01 16:31:13 INFO mapred.ReduceTask: Using ShuffleConsumerPlugin: org.apache.hadoop.mapreduce.task.reduce.Shuffle@2f94abe0\n",
      "16/02/01 16:31:13 INFO reduce.MergeManagerImpl: MergerManager: memoryLimit=333971456, maxSingleShuffleLimit=83492864, mergeThreshold=220421168, ioSortFactor=10, memToMemMergeOutputsThreshold=10\n",
      "16/02/01 16:31:13 INFO reduce.EventFetcher: attempt_local2102783744_0001_r_000000_0 Thread started: EventFetcher for fetching Map Completion Events\n",
      "16/02/01 16:31:13 INFO reduce.LocalFetcher: localfetcher#1 about to shuffle output of map attempt_local2102783744_0001_m_000000_0 decomp: 24211189 len: 24211193 to MEMORY\n",
      "16/02/01 16:31:13 INFO reduce.InMemoryMapOutput: Read 24211189 bytes from map-output for attempt_local2102783744_0001_m_000000_0\n",
      "16/02/01 16:31:13 INFO reduce.MergeManagerImpl: closeInMemoryFile -> map-output of size: 24211189, inMemoryMapOutputs.size() -> 1, commitMemory -> 0, usedMemory ->24211189\n",
      "16/02/01 16:31:13 INFO reduce.EventFetcher: EventFetcher is interrupted.. Returning\n",
      "16/02/01 16:31:13 INFO mapred.LocalJobRunner: 1 / 1 copied.\n",
      "16/02/01 16:31:13 INFO reduce.MergeManagerImpl: finalMerge called with 1 in-memory map-outputs and 0 on-disk map-outputs\n",
      "16/02/01 16:31:13 INFO mapred.Merger: Merging 1 sorted segments\n",
      "16/02/01 16:31:13 INFO mapred.Merger: Down to the last merge-pass, with 1 segments left of total size: 24211185 bytes\n",
      "16/02/01 16:31:14 INFO mapreduce.Job:  map 100% reduce 0%\n",
      "16/02/01 16:31:15 INFO reduce.MergeManagerImpl: Merged 1 segments, 24211189 bytes to disk to satisfy reduce memory limit\n",
      "16/02/01 16:31:15 INFO reduce.MergeManagerImpl: Merging 1 files, 24211193 bytes from disk\n",
      "16/02/01 16:31:15 INFO reduce.MergeManagerImpl: Merging 0 segments, 0 bytes from memory into reduce\n",
      "16/02/01 16:31:15 INFO mapred.Merger: Merging 1 sorted segments\n",
      "16/02/01 16:31:15 INFO mapred.Merger: Down to the last merge-pass, with 1 segments left of total size: 24211185 bytes\n",
      "16/02/01 16:31:15 INFO mapred.LocalJobRunner: 1 / 1 copied.\n",
      "16/02/01 16:31:15 INFO streaming.PipeMapRed: PipeMapRed exec [/Users/JingXu/Dropbox/DataScience/W261/W261/HW3/./reducer.py]\n",
      "16/02/01 16:31:15 INFO Configuration.deprecation: mapred.job.tracker is deprecated. Instead, use mapreduce.jobtracker.address\n",
      "16/02/01 16:31:15 INFO Configuration.deprecation: mapred.map.tasks is deprecated. Instead, use mapreduce.job.maps\n",
      "16/02/01 16:31:15 INFO streaming.PipeMapRed: R/W/S=1/0/0 in:NA [rec/s] out:NA [rec/s]\n",
      "16/02/01 16:31:15 INFO streaming.PipeMapRed: R/W/S=10/0/0 in:NA [rec/s] out:NA [rec/s]\n",
      "16/02/01 16:31:15 INFO streaming.PipeMapRed: R/W/S=100/0/0 in:NA [rec/s] out:NA [rec/s]\n",
      "16/02/01 16:31:15 INFO streaming.PipeMapRed: R/W/S=1000/0/0 in:NA [rec/s] out:NA [rec/s]\n",
      "16/02/01 16:31:15 INFO streaming.PipeMapRed: R/W/S=10000/0/0 in:NA [rec/s] out:NA [rec/s]\n",
      "16/02/01 16:31:15 INFO streaming.PipeMapRed: R/W/S=100000/0/0 in:NA [rec/s] out:NA [rec/s]\n",
      "16/02/01 16:31:15 INFO streaming.PipeMapRed: R/W/S=200000/0/0 in:NA [rec/s] out:NA [rec/s]\n",
      "16/02/01 16:31:15 INFO streaming.PipeMapRed: R/W/S=300000/0/0 in:NA [rec/s] out:NA [rec/s]\n",
      "16/02/01 16:31:16 INFO streaming.PipeMapRed: R/W/S=400000/0/0 in:NA [rec/s] out:NA [rec/s]\n",
      "16/02/01 16:31:16 INFO streaming.PipeMapRed: R/W/S=500000/0/0 in:NA [rec/s] out:NA [rec/s]\n",
      "16/02/01 16:31:16 INFO streaming.PipeMapRed: R/W/S=600000/0/0 in:600000=600000/1 [rec/s] out:0=0/1 [rec/s]\n",
      "16/02/01 16:31:16 INFO streaming.PipeMapRed: R/W/S=700000/0/0 in:700000=700000/1 [rec/s] out:0=0/1 [rec/s]\n",
      "16/02/01 16:31:16 INFO streaming.PipeMapRed: R/W/S=800000/0/0 in:800000=800000/1 [rec/s] out:0=0/1 [rec/s]\n",
      "16/02/01 16:31:16 INFO streaming.PipeMapRed: R/W/S=900000/0/0 in:900000=900000/1 [rec/s] out:0=0/1 [rec/s]\n",
      "16/02/01 16:31:17 INFO streaming.PipeMapRed: R/W/S=1000000/0/0 in:1000000=1000000/1 [rec/s] out:0=0/1 [rec/s]\n",
      "16/02/01 16:31:17 INFO streaming.PipeMapRed: R/W/S=1100000/0/0 in:1100000=1100000/1 [rec/s] out:0=0/1 [rec/s]\n",
      "16/02/01 16:31:17 INFO streaming.PipeMapRed: R/W/S=1200000/0/0 in:600000=1200000/2 [rec/s] out:0=0/2 [rec/s]\n",
      "16/02/01 16:31:17 INFO streaming.PipeMapRed: R/W/S=1300000/0/0 in:650000=1300000/2 [rec/s] out:0=0/2 [rec/s]\n",
      "16/02/01 16:31:17 INFO streaming.PipeMapRed: R/W/S=1400000/0/0 in:700000=1400000/2 [rec/s] out:0=0/2 [rec/s]\n",
      "16/02/01 16:31:17 INFO streaming.PipeMapRed: R/W/S=1500000/0/0 in:750000=1500000/2 [rec/s] out:0=0/2 [rec/s]\n",
      "16/02/01 16:31:18 INFO streaming.PipeMapRed: R/W/S=1600000/0/0 in:800000=1600000/2 [rec/s] out:0=0/2 [rec/s]\n",
      "16/02/01 16:31:18 INFO streaming.PipeMapRed: R/W/S=1700000/0/0 in:850000=1700000/2 [rec/s] out:0=0/2 [rec/s]\n",
      "16/02/01 16:31:18 INFO streaming.PipeMapRed: R/W/S=1800000/0/0 in:600000=1800000/3 [rec/s] out:0=0/3 [rec/s]\n",
      "16/02/01 16:31:18 INFO streaming.PipeMapRed: R/W/S=1900000/0/0 in:633333=1900000/3 [rec/s] out:0=0/3 [rec/s]\n",
      "16/02/01 16:31:18 INFO streaming.PipeMapRed: R/W/S=2000000/0/0 in:666666=2000000/3 [rec/s] out:0=0/3 [rec/s]\n",
      "16/02/01 16:31:18 INFO streaming.PipeMapRed: R/W/S=2100000/0/0 in:700000=2100000/3 [rec/s] out:0=0/3 [rec/s]\n",
      "16/02/01 16:31:19 INFO streaming.PipeMapRed: R/W/S=2200000/0/0 in:733333=2200000/3 [rec/s] out:0=0/3 [rec/s]\n",
      "16/02/01 16:31:19 INFO streaming.PipeMapRed: R/W/S=2300000/0/0 in:766666=2300000/3 [rec/s] out:0=0/3 [rec/s]\n",
      "16/02/01 16:31:19 INFO streaming.PipeMapRed: R/W/S=2400000/0/0 in:600000=2400000/4 [rec/s] out:0=0/4 [rec/s]\n",
      "16/02/01 16:31:19 INFO mapred.LocalJobRunner: reduce > reduce\n",
      "16/02/01 16:31:19 INFO streaming.PipeMapRed: R/W/S=2500000/0/0 in:625000=2500000/4 [rec/s] out:0=0/4 [rec/s]\n",
      "16/02/01 16:31:19 INFO streaming.PipeMapRed: R/W/S=2600000/0/0 in:650000=2600000/4 [rec/s] out:0=0/4 [rec/s]\n",
      "16/02/01 16:31:20 INFO streaming.PipeMapRed: MRErrorThread done\n",
      "16/02/01 16:31:20 INFO streaming.PipeMapRed: Records R/W=2696618/1\n",
      "16/02/01 16:31:20 INFO streaming.PipeMapRed: mapRedFinished\n",
      "16/02/01 16:31:20 INFO mapred.Task: Task:attempt_local2102783744_0001_r_000000_0 is done. And is in the process of committing\n",
      "16/02/01 16:31:20 INFO mapred.LocalJobRunner: reduce > reduce\n",
      "16/02/01 16:31:20 INFO mapred.Task: Task attempt_local2102783744_0001_r_000000_0 is allowed to commit now\n",
      "16/02/01 16:31:20 INFO output.FileOutputCommitter: Saved output of task 'attempt_local2102783744_0001_r_000000_0' to hdfs://localhost:9000/user/jing/test-output1/_temporary/0/task_local2102783744_0001_r_000000\n",
      "16/02/01 16:31:20 INFO mapred.LocalJobRunner: Records R/W=2696618/1 > reduce\n",
      "16/02/01 16:31:20 INFO mapred.Task: Task 'attempt_local2102783744_0001_r_000000_0' done.\n",
      "16/02/01 16:31:20 INFO mapred.LocalJobRunner: Finishing task: attempt_local2102783744_0001_r_000000_0\n",
      "16/02/01 16:31:20 INFO mapred.LocalJobRunner: reduce task executor complete.\n",
      "16/02/01 16:31:20 INFO mapreduce.Job:  map 100% reduce 100%\n",
      "16/02/01 16:31:20 INFO mapreduce.Job: Job job_local2102783744_0001 completed successfully\n",
      "16/02/01 16:31:20 INFO mapreduce.Job: Counters: 35\n",
      "\tFile System Counters\n",
      "\t\tFILE: Number of bytes read=48427436\n",
      "\t\tFILE: Number of bytes written=73159911\n",
      "\t\tFILE: Number of read operations=0\n",
      "\t\tFILE: Number of large read operations=0\n",
      "\t\tFILE: Number of write operations=0\n",
      "\t\tHDFS: Number of bytes read=101812972\n",
      "\t\tHDFS: Number of bytes written=2184\n",
      "\t\tHDFS: Number of read operations=15\n",
      "\t\tHDFS: Number of large read operations=0\n",
      "\t\tHDFS: Number of write operations=4\n",
      "\tMap-Reduce Framework\n",
      "\t\tMap input records=312913\n",
      "\t\tMap output records=2696618\n",
      "\t\tMap output bytes=18817951\n",
      "\t\tMap output materialized bytes=24211193\n",
      "\t\tInput split bytes=107\n",
      "\t\tCombine input records=0\n",
      "\t\tCombine output records=0\n",
      "\t\tReduce input groups=175\n",
      "\t\tReduce shuffle bytes=24211193\n",
      "\t\tReduce input records=2696618\n",
      "\t\tReduce output records=175\n",
      "\t\tSpilled Records=5393236\n",
      "\t\tShuffled Maps =1\n",
      "\t\tFailed Shuffles=0\n",
      "\t\tMerged Map outputs=1\n",
      "\t\tGC time elapsed (ms)=9\n",
      "\t\tTotal committed heap usage (bytes)=650117120\n",
      "\tShuffle Errors\n",
      "\t\tBAD_ID=0\n",
      "\t\tCONNECTION=0\n",
      "\t\tIO_ERROR=0\n",
      "\t\tWRONG_LENGTH=0\n",
      "\t\tWRONG_MAP=0\n",
      "\t\tWRONG_REDUCE=0\n",
      "\tFile Input Format Counters \n",
      "\t\tBytes Read=50906486\n",
      "\tFile Output Format Counters \n",
      "\t\tBytes Written=2184\n",
      "16/02/01 16:31:20 INFO streaming.StreamJob: Output directory: /user/jing/test-output1/\n",
      "Found 13 unexpected arguments on the command line [ #sort, primarly, by, numeric, descending, in, 2nd, key,, then, alphabetically, on, 1st, key]\n",
      "Try -help for more information\n",
      "Streaming Command Failed!\n",
      "./mapreduce.sh: line 31: -file: command not found\n",
      "DEPRECATED: Use of this script to execute hdfs command is deprecated.\n",
      "Instead use the hdfs command for it.\n",
      "\n",
      "16/02/01 16:31:22 WARN util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "cat: `/user/jing/test-output2/part-00000': No such file or directory\n",
      "rmr: DEPRECATED: Please use 'rm -r' instead.\n",
      "16/02/01 16:31:23 WARN util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "16/02/01 16:31:24 INFO fs.TrashPolicyDefault: Namenode trash configuration: Deletion interval = 0 minutes, Emptier interval = 0 minutes.\n",
      "Deleted /user/jing\n",
      "rmr: DEPRECATED: Please use 'rm -r' instead.\n",
      "16/02/01 16:31:25 WARN util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "16/02/01 16:31:25 INFO fs.TrashPolicyDefault: Namenode trash configuration: Deletion interval = 0 minutes, Emptier interval = 0 minutes.\n",
      "Deleted /user\n"
     ]
    }
   ],
   "source": [
    "!./mapreduce.sh"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**10 smallest records:**\n",
    "\n",
    "['disclosures', 64, 4.746686404970967e-05]\t\n",
    "['missing', 64, 4.746686404970967e-05]\t\n",
    "['amt', 71, 5.265855230514667e-05]\t\n",
    "['day', 71, 5.265855230514667e-05]\t\n",
    "['checks', 75, 5.5625231308253527e-05]\t\n",
    "['convenience', 75, 5.5625231308253527e-05]\t\n",
    "['credited', 92, 6.823361707145766e-05]\t\n",
    "['payment', 92, 6.823361707145766e-05]\t\n",
    "['amount', 98, 7.268363557611794e-05]\t\n",
    "['apply', 118, 8.751703059165221e-05]\t\n",
    "\n",
    "**50 biggest records:**\n",
    "\n",
    "['being', 5663, 0.004200075798648529]\t\n",
    "['by', 5663, 0.004200075798648529]\t\n",
    "['caused', 5663, 0.004200075798648529]\t\n",
    "['funds', 5663, 0.004200075798648529]\t\n",
    "['low', 5663, 0.004200075798648529]\t\n",
    "['the', 6248, 0.004633952602852907]\t\n",
    "['lease', 6337, 0.004699961210672034]\t\n",
    "['reporting', 6559, 0.004864611895344465]\t\n",
    "['disputes', 6938, 0.005145704730888839]\t\n",
    "['disclosure', 7655, 0.005677481942195743]\t\n",
    "['verification', 7655, 0.005677481942195743]\t\n",
    "['other', 7886, 0.005848807654625164]\t\n",
    "['billing', 8158, 0.00605054182683643]\t\n",
    "['unable', 8178, 0.006065375221851964]\t\n",
    "['to', 8401, 0.006230767576275172]\t\n",
    "['broker', 8625, 0.006396901600449155]\t\n",
    "['mortgage', 8625, 0.006396901600449155]\t\n",
    "['originator', 8625, 0.006396901600449155]\t\n",
    "['communication', 8671, 0.006431018408984884]\t\n",
    "['tactics', 8671, 0.006431018408984884]\t\n",
    "['application', 8868, 0.006577127349887897]\t\n",
    "['problems', 9484, 0.007033995916366352]\t\n",
    "['deposits', 10555, 0.007828324219448212]\t\n",
    "['withdrawals', 10555, 0.007828324219448212]\t\n",
    "['my', 10731, 0.007958858095584914]\t\n",
    "['of', 13983, 0.010370768125110787]\t\n",
    "['management', 16205, 0.012018758311336645]\t\n",
    "['opening', 16205, 0.012018758311336645]\t\n",
    "['and', 16448, 0.012198984060775386]\t\n",
    "['attempts', 17972, 0.013329288760959098]\t\n",
    "['collect', 17972, 0.013329288760959098]\t\n",
    "[\"cont'd\", 17972, 0.013329288760959098]\t\n",
    "['owed', 17972, 0.013329288760959098]\t\n",
    "['not', 18477, 0.013703831985101339]\t\n",
    "['closing', 19000, 0.014091725264757559]\t\n",
    "['debt', 27874, 0.020673302633150117]\t\n",
    "['information', 29069, 0.021559597985328288]\t\n",
    "['on', 29069, 0.021559597985328288]\t\n",
    "['incorrect', 29133, 0.021607064849377997]\t\n",
    "['report', 34903, 0.025886499311359636]\t\n",
    "['escrow', 36767, 0.02726897172680743]\t\n",
    "['servicing', 36767, 0.02726897172680743]\t\n",
    "['payments', 39993, 0.02966159834281311]\t\n",
    "['or', 40508, 0.030043558264463116]\t\n",
    "['credit', 55251, 0.040977995400164204]\t\n",
    "['account', 57448, 0.04260744384262065]\t\n",
    "['foreclosure', 70487, 0.05227807572299822]\t\n",
    "['modification', 70487, 0.05227807572299822]\t\n",
    "['collection', 72394, 0.05369243993772941]\t\n",
    "['loan', 119630, 0.08872595228541826]\t"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**HW3.3. Shopping Cart Analysis\n",
    "Product Recommendations: The action or practice of selling additional products or services \n",
    "to existing customers is called cross-selling. Giving product recommendation is \n",
    "one of the examples of cross-selling that are frequently used by online retailers. \n",
    "One simple method to give product recommendations is to recommend products that are frequently\n",
    "browsed together by the customers.**\n",
    "\n",
    "**Do some exploratory data analysis of this dataset. How many unique items are available from this supplier? Using a single reducer: Report your findings such as number of unique products; largest basket; report the top 50 most frequently purchased items,  their frequency,  and their relative frequency (break ties by sorting the products alphabetical order) etc. using Hadoop Map-Reduce.** "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting mapper.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile mapper.py\n",
    "#!/usr/bin/python\n",
    "## mapper.py\n",
    "## Author: Jing Xu\n",
    "## Description: mapper code for HW3.3.1\n",
    "\n",
    "import sys\n",
    "\n",
    "# read input from STDIN (standard input)\n",
    "for line in sys.stdin:\n",
    "    # split the line into words\n",
    "    words = line.split()\n",
    "    for word in words:\n",
    "        print '%s\\t%d' % (word, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting reducer.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile reducer.py\n",
    "#!/usr/bin/python\n",
    "## reducer.py\n",
    "## Author: Jing Xu\n",
    "## Description: combiner code for HW3.3.1\n",
    "\n",
    "import sys\n",
    "\n",
    "current_word = None\n",
    "current_count = 0\n",
    "word = None\n",
    "\n",
    "# input comes from STDIN\n",
    "for line in sys.stdin:\n",
    "    # parse the input we got from mapper.py\n",
    "    word, count = line.split('\\t', 1)\n",
    "    # convert count (currently a string) to int\n",
    "    try:\n",
    "        count = int(count)\n",
    "    except ValueError:\n",
    "        # count was not a number, so silently\n",
    "        # ignore/discard this line\n",
    "        continue\n",
    "    if current_word == word:\n",
    "        current_count += count\n",
    "    else:\n",
    "        if current_word:\n",
    "            # write result to STDOUT\n",
    "            print '%s\\t%d' % (current_word, current_count)\n",
    "            sys.stderr.write(\"reporter:counter:UniqueWordsCounter,Total,1\\n\")\n",
    "        current_count = count\n",
    "        current_word = word\n",
    "\n",
    "# do not forget to output the last word if needed!\n",
    "if current_word == word:\n",
    "    print '%s\\t%d' % (current_word, current_count)\n",
    "    sys.stderr.write(\"reporter:counter:UniqueWordsCounter,Total,1\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting mapreduce.sh\n"
     ]
    }
   ],
   "source": [
    "%%writefile mapreduce.sh\n",
    "## mapreduce.sh\n",
    "## Author: Jing Xu\n",
    "## Description: mapreduce bash script for HW3.3.1\n",
    "\n",
    "hdfs dfs -mkdir /user # create hdfs folder\n",
    "wait\n",
    "hdfs dfs -mkdir /user/jing # create hdfs folder\n",
    "wait\n",
    "hadoop fs -put ProductPurchaseData.txt /user/jing # upload local file to hdfs folder\n",
    "wait\n",
    "\n",
    "# hadoop command to run streaming mapreduce job\n",
    "hadoop jar /Users/JingXu/Documents/hadoop-2.6.3/share/hadoop/tools/lib/hadoop-streaming-2.6.3.jar \\\n",
    "-D mapred.job.name=\"Count Job via Streaming\" \\\n",
    "-D mapred.map.tasks=1 \\\n",
    "-D mapred.reduce.tasks=1 \\\n",
    "-file mapper.py    -mapper mapper.py \\\n",
    "-file reducer.py   -reducer reducer.py \\\n",
    "-input /user/jing/* -output /user/jing/test-output1/\n",
    "\n",
    "wait\n",
    "\n",
    "hdfs dfs -rmr /user/jing # remove hdfs folder\n",
    "hdfs dfs -rmr /user # remove hdfs folder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "16/02/01 16:31:35 WARN util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "16/02/01 16:31:36 WARN util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "16/02/01 16:31:38 WARN util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "16/02/01 16:31:40 WARN streaming.StreamJob: -file option is deprecated, please use generic option -files instead.\n",
      "16/02/01 16:31:40 WARN util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "packageJobJar: [mapper.py, reducer.py] [] /var/folders/zs/k144hqks281fbt0x68c_zj9m0000gp/T/streamjob746043006379144654.jar tmpDir=null\n",
      "16/02/01 16:31:40 INFO Configuration.deprecation: session.id is deprecated. Instead, use dfs.metrics.session-id\n",
      "16/02/01 16:31:40 INFO jvm.JvmMetrics: Initializing JVM Metrics with processName=JobTracker, sessionId=\n",
      "16/02/01 16:31:40 INFO jvm.JvmMetrics: Cannot initialize JVM Metrics with processName=JobTracker, sessionId= - already initialized\n",
      "16/02/01 16:31:41 INFO mapred.FileInputFormat: Total input paths to process : 1\n",
      "16/02/01 16:31:41 INFO mapreduce.JobSubmitter: number of splits:1\n",
      "16/02/01 16:31:41 INFO Configuration.deprecation: mapred.reduce.tasks is deprecated. Instead, use mapreduce.job.reduces\n",
      "16/02/01 16:31:41 INFO Configuration.deprecation: mapred.job.name is deprecated. Instead, use mapreduce.job.name\n",
      "16/02/01 16:31:41 INFO Configuration.deprecation: mapred.map.tasks is deprecated. Instead, use mapreduce.job.maps\n",
      "16/02/01 16:31:41 INFO mapreduce.JobSubmitter: Submitting tokens for job: job_local634522462_0001\n",
      "16/02/01 16:31:41 INFO mapred.LocalDistributedCacheManager: Localized file:/Users/JingXu/Dropbox/DataScience/W261/W261/HW3/mapper.py as file:/tmp/hadoop-JingXu/mapred/local/1454373101548/mapper.py\n",
      "16/02/01 16:31:41 INFO mapred.LocalDistributedCacheManager: Localized file:/Users/JingXu/Dropbox/DataScience/W261/W261/HW3/reducer.py as file:/tmp/hadoop-JingXu/mapred/local/1454373101549/reducer.py\n",
      "16/02/01 16:31:41 INFO mapreduce.Job: The url to track the job: http://localhost:8080/\n",
      "16/02/01 16:31:41 INFO mapred.LocalJobRunner: OutputCommitter set in config null\n",
      "16/02/01 16:31:41 INFO mapreduce.Job: Running job: job_local634522462_0001\n",
      "16/02/01 16:31:41 INFO mapred.LocalJobRunner: OutputCommitter is org.apache.hadoop.mapred.FileOutputCommitter\n",
      "16/02/01 16:31:41 INFO mapred.LocalJobRunner: Waiting for map tasks\n",
      "16/02/01 16:31:41 INFO mapred.LocalJobRunner: Starting task: attempt_local634522462_0001_m_000000_0\n",
      "16/02/01 16:31:41 INFO util.ProcfsBasedProcessTree: ProcfsBasedProcessTree currently is supported only on Linux.\n",
      "16/02/01 16:31:41 INFO mapred.Task:  Using ResourceCalculatorProcessTree : null\n",
      "16/02/01 16:31:41 INFO mapred.MapTask: Processing split: hdfs://localhost:9000/user/jing/ProductPurchaseData.txt:0+3458517\n",
      "16/02/01 16:31:41 INFO mapred.MapTask: numReduceTasks: 1\n",
      "16/02/01 16:31:41 INFO mapred.MapTask: (EQUATOR) 0 kvi 26214396(104857584)\n",
      "16/02/01 16:31:41 INFO mapred.MapTask: mapreduce.task.io.sort.mb: 100\n",
      "16/02/01 16:31:41 INFO mapred.MapTask: soft limit at 83886080\n",
      "16/02/01 16:31:41 INFO mapred.MapTask: bufstart = 0; bufvoid = 104857600\n",
      "16/02/01 16:31:41 INFO mapred.MapTask: kvstart = 26214396; length = 6553600\n",
      "16/02/01 16:31:41 INFO mapred.MapTask: Map output collector class = org.apache.hadoop.mapred.MapTask$MapOutputBuffer\n",
      "16/02/01 16:31:41 INFO streaming.PipeMapRed: PipeMapRed exec [/Users/JingXu/Dropbox/DataScience/W261/W261/HW3/./mapper.py]\n",
      "16/02/01 16:31:41 INFO Configuration.deprecation: mapred.task.id is deprecated. Instead, use mapreduce.task.attempt.id\n",
      "16/02/01 16:31:41 INFO Configuration.deprecation: user.name is deprecated. Instead, use mapreduce.job.user.name\n",
      "16/02/01 16:31:41 INFO Configuration.deprecation: map.input.start is deprecated. Instead, use mapreduce.map.input.start\n",
      "16/02/01 16:31:41 INFO Configuration.deprecation: mapred.task.is.map is deprecated. Instead, use mapreduce.task.ismap\n",
      "16/02/01 16:31:41 INFO Configuration.deprecation: mapred.tip.id is deprecated. Instead, use mapreduce.task.id\n",
      "16/02/01 16:31:41 INFO Configuration.deprecation: mapred.skip.on is deprecated. Instead, use mapreduce.job.skiprecords\n",
      "16/02/01 16:31:41 INFO Configuration.deprecation: mapred.task.partition is deprecated. Instead, use mapreduce.task.partition\n",
      "16/02/01 16:31:41 INFO Configuration.deprecation: map.input.length is deprecated. Instead, use mapreduce.map.input.length\n",
      "16/02/01 16:31:41 INFO Configuration.deprecation: mapred.local.dir is deprecated. Instead, use mapreduce.cluster.local.dir\n",
      "16/02/01 16:31:41 INFO Configuration.deprecation: mapred.work.output.dir is deprecated. Instead, use mapreduce.task.output.dir\n",
      "16/02/01 16:31:41 INFO Configuration.deprecation: map.input.file is deprecated. Instead, use mapreduce.map.input.file\n",
      "16/02/01 16:31:41 INFO Configuration.deprecation: mapred.job.id is deprecated. Instead, use mapreduce.job.id\n",
      "16/02/01 16:31:41 INFO streaming.PipeMapRed: R/W/S=1/0/0 in:NA [rec/s] out:NA [rec/s]\n",
      "16/02/01 16:31:41 INFO streaming.PipeMapRed: R/W/S=10/0/0 in:NA [rec/s] out:NA [rec/s]\n",
      "16/02/01 16:31:41 INFO streaming.PipeMapRed: R/W/S=100/0/0 in:NA [rec/s] out:NA [rec/s]\n",
      "16/02/01 16:31:42 INFO streaming.PipeMapRed: R/W/S=1000/0/0 in:NA [rec/s] out:NA [rec/s]\n",
      "16/02/01 16:31:42 INFO streaming.PipeMapRed: Records R/W=1216/1\n",
      "16/02/01 16:31:42 INFO streaming.PipeMapRed: R/W/S=10000/120645/0 in:NA [rec/s] out:NA [rec/s]\n",
      "16/02/01 16:31:42 INFO streaming.PipeMapRed: MRErrorThread done\n",
      "16/02/01 16:31:42 INFO streaming.PipeMapRed: mapRedFinished\n",
      "16/02/01 16:31:42 INFO mapred.LocalJobRunner: \n",
      "16/02/01 16:31:42 INFO mapred.MapTask: Starting flush of map output\n",
      "16/02/01 16:31:42 INFO mapred.MapTask: Spilling map output\n",
      "16/02/01 16:31:42 INFO mapred.MapTask: bufstart = 0; bufend = 4189064; bufvoid = 104857600\n",
      "16/02/01 16:31:42 INFO mapred.MapTask: kvstart = 26214396(104857584); kvend = 24691104(98764416); length = 1523293/6553600\n",
      "16/02/01 16:31:42 INFO mapreduce.Job: Job job_local634522462_0001 running in uber mode : false\n",
      "16/02/01 16:31:42 INFO mapreduce.Job:  map 0% reduce 0%\n",
      "16/02/01 16:31:43 INFO mapred.MapTask: Finished spill 0\n",
      "16/02/01 16:31:43 INFO mapred.Task: Task:attempt_local634522462_0001_m_000000_0 is done. And is in the process of committing\n",
      "16/02/01 16:31:43 INFO mapred.LocalJobRunner: Records R/W=1216/1\n",
      "16/02/01 16:31:43 INFO mapred.Task: Task 'attempt_local634522462_0001_m_000000_0' done.\n",
      "16/02/01 16:31:43 INFO mapred.LocalJobRunner: Finishing task: attempt_local634522462_0001_m_000000_0\n",
      "16/02/01 16:31:43 INFO mapred.LocalJobRunner: map task executor complete.\n",
      "16/02/01 16:31:43 INFO mapred.LocalJobRunner: Waiting for reduce tasks\n",
      "16/02/01 16:31:43 INFO mapred.LocalJobRunner: Starting task: attempt_local634522462_0001_r_000000_0\n",
      "16/02/01 16:31:43 INFO util.ProcfsBasedProcessTree: ProcfsBasedProcessTree currently is supported only on Linux.\n",
      "16/02/01 16:31:43 INFO mapred.Task:  Using ResourceCalculatorProcessTree : null\n",
      "16/02/01 16:31:43 INFO mapred.ReduceTask: Using ShuffleConsumerPlugin: org.apache.hadoop.mapreduce.task.reduce.Shuffle@3d62a997\n",
      "16/02/01 16:31:43 INFO reduce.MergeManagerImpl: MergerManager: memoryLimit=333971456, maxSingleShuffleLimit=83492864, mergeThreshold=220421168, ioSortFactor=10, memToMemMergeOutputsThreshold=10\n",
      "16/02/01 16:31:43 INFO reduce.EventFetcher: attempt_local634522462_0001_r_000000_0 Thread started: EventFetcher for fetching Map Completion Events\n",
      "16/02/01 16:31:43 INFO reduce.LocalFetcher: localfetcher#1 about to shuffle output of map attempt_local634522462_0001_m_000000_0 decomp: 4950714 len: 4950718 to MEMORY\n",
      "16/02/01 16:31:43 INFO reduce.InMemoryMapOutput: Read 4950714 bytes from map-output for attempt_local634522462_0001_m_000000_0\n",
      "16/02/01 16:31:43 INFO reduce.MergeManagerImpl: closeInMemoryFile -> map-output of size: 4950714, inMemoryMapOutputs.size() -> 1, commitMemory -> 0, usedMemory ->4950714\n",
      "16/02/01 16:31:43 INFO reduce.EventFetcher: EventFetcher is interrupted.. Returning\n",
      "16/02/01 16:31:43 INFO mapred.LocalJobRunner: 1 / 1 copied.\n",
      "16/02/01 16:31:43 INFO reduce.MergeManagerImpl: finalMerge called with 1 in-memory map-outputs and 0 on-disk map-outputs\n",
      "16/02/01 16:31:43 INFO mapred.Merger: Merging 1 sorted segments\n",
      "16/02/01 16:31:43 INFO mapred.Merger: Down to the last merge-pass, with 1 segments left of total size: 4950703 bytes\n",
      "16/02/01 16:31:43 INFO mapreduce.Job:  map 100% reduce 0%\n",
      "16/02/01 16:31:43 INFO reduce.MergeManagerImpl: Merged 1 segments, 4950714 bytes to disk to satisfy reduce memory limit\n",
      "16/02/01 16:31:43 INFO reduce.MergeManagerImpl: Merging 1 files, 4950718 bytes from disk\n",
      "16/02/01 16:31:43 INFO reduce.MergeManagerImpl: Merging 0 segments, 0 bytes from memory into reduce\n",
      "16/02/01 16:31:43 INFO mapred.Merger: Merging 1 sorted segments\n",
      "16/02/01 16:31:43 INFO mapred.Merger: Down to the last merge-pass, with 1 segments left of total size: 4950703 bytes\n",
      "16/02/01 16:31:43 INFO mapred.LocalJobRunner: 1 / 1 copied.\n",
      "16/02/01 16:31:43 INFO streaming.PipeMapRed: PipeMapRed exec [/Users/JingXu/Dropbox/DataScience/W261/W261/HW3/./reducer.py]\n",
      "16/02/01 16:31:43 INFO Configuration.deprecation: mapred.job.tracker is deprecated. Instead, use mapreduce.jobtracker.address\n",
      "16/02/01 16:31:43 INFO Configuration.deprecation: mapred.map.tasks is deprecated. Instead, use mapreduce.job.maps\n",
      "16/02/01 16:31:43 INFO streaming.PipeMapRed: R/W/S=1/0/0 in:NA [rec/s] out:NA [rec/s]\n",
      "16/02/01 16:31:43 INFO streaming.PipeMapRed: R/W/S=10/0/0 in:NA [rec/s] out:NA [rec/s]\n",
      "16/02/01 16:31:43 INFO streaming.PipeMapRed: R/W/S=100/0/0 in:NA [rec/s] out:NA [rec/s]\n",
      "16/02/01 16:31:43 INFO streaming.PipeMapRed: R/W/S=1000/0/0 in:NA [rec/s] out:NA [rec/s]\n",
      "16/02/01 16:31:43 INFO streaming.PipeMapRed: R/W/S=10000/0/0 in:NA [rec/s] out:NA [rec/s]\n",
      "16/02/01 16:31:44 INFO streaming.PipeMapRed: Records R/W=50013/1\n",
      "16/02/01 16:31:44 INFO streaming.PipeMapRed: R/W/S=100000/1444/0 in:NA [rec/s] out:NA [rec/s]\n",
      "16/02/01 16:31:44 INFO streaming.PipeMapRed: R/W/S=200000/5780/0 in:NA [rec/s] out:NA [rec/s]\n",
      "16/02/01 16:31:44 INFO streaming.PipeMapRed: R/W/S=300000/8675/0 in:NA [rec/s] out:NA [rec/s]\n",
      "16/02/01 16:31:44 INFO streaming.PipeMapRed: MRErrorThread done\n",
      "16/02/01 16:31:44 INFO streaming.PipeMapRed: mapRedFinished\n",
      "16/02/01 16:31:44 INFO mapred.Task: Task:attempt_local634522462_0001_r_000000_0 is done. And is in the process of committing\n",
      "16/02/01 16:31:44 INFO mapred.LocalJobRunner: 1 / 1 copied.\n",
      "16/02/01 16:31:44 INFO mapred.Task: Task attempt_local634522462_0001_r_000000_0 is allowed to commit now\n",
      "16/02/01 16:31:44 INFO output.FileOutputCommitter: Saved output of task 'attempt_local634522462_0001_r_000000_0' to hdfs://localhost:9000/user/jing/test-output1/_temporary/0/task_local634522462_0001_r_000000\n",
      "16/02/01 16:31:44 INFO mapred.LocalJobRunner: Records R/W=50013/1 > reduce\n",
      "16/02/01 16:31:44 INFO mapred.Task: Task 'attempt_local634522462_0001_r_000000_0' done.\n",
      "16/02/01 16:31:44 INFO mapred.LocalJobRunner: Finishing task: attempt_local634522462_0001_r_000000_0\n",
      "16/02/01 16:31:44 INFO mapred.LocalJobRunner: reduce task executor complete.\n",
      "16/02/01 16:31:44 INFO mapreduce.Job:  map 100% reduce 100%\n",
      "16/02/01 16:31:44 INFO mapreduce.Job: Job job_local634522462_0001 completed successfully\n",
      "16/02/01 16:31:44 INFO mapreduce.Job: Counters: 36\n",
      "\tFile System Counters\n",
      "\t\tFILE: Number of bytes read=9906194\n",
      "\t\tFILE: Number of bytes written=15375422\n",
      "\t\tFILE: Number of read operations=0\n",
      "\t\tFILE: Number of large read operations=0\n",
      "\t\tFILE: Number of write operations=0\n",
      "\t\tHDFS: Number of bytes read=6917034\n",
      "\t\tHDFS: Number of bytes written=142658\n",
      "\t\tHDFS: Number of read operations=15\n",
      "\t\tHDFS: Number of large read operations=0\n",
      "\t\tHDFS: Number of write operations=4\n",
      "\tMap-Reduce Framework\n",
      "\t\tMap input records=31101\n",
      "\t\tMap output records=380824\n",
      "\t\tMap output bytes=4189064\n",
      "\t\tMap output materialized bytes=4950718\n",
      "\t\tInput split bytes=107\n",
      "\t\tCombine input records=0\n",
      "\t\tCombine output records=0\n",
      "\t\tReduce input groups=12592\n",
      "\t\tReduce shuffle bytes=4950718\n",
      "\t\tReduce input records=380824\n",
      "\t\tReduce output records=12592\n",
      "\t\tSpilled Records=761648\n",
      "\t\tShuffled Maps =1\n",
      "\t\tFailed Shuffles=0\n",
      "\t\tMerged Map outputs=1\n",
      "\t\tGC time elapsed (ms)=0\n",
      "\t\tTotal committed heap usage (bytes)=582483968\n",
      "\tShuffle Errors\n",
      "\t\tBAD_ID=0\n",
      "\t\tCONNECTION=0\n",
      "\t\tIO_ERROR=0\n",
      "\t\tWRONG_LENGTH=0\n",
      "\t\tWRONG_MAP=0\n",
      "\t\tWRONG_REDUCE=0\n",
      "\tUniqueWordsCounter\n",
      "\t\tTotal=12592\n",
      "\tFile Input Format Counters \n",
      "\t\tBytes Read=3458517\n",
      "\tFile Output Format Counters \n",
      "\t\tBytes Written=142658\n",
      "16/02/01 16:31:44 INFO streaming.StreamJob: Output directory: /user/jing/test-output1/\n",
      "rmr: DEPRECATED: Please use 'rm -r' instead.\n",
      "16/02/01 16:31:45 WARN util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "16/02/01 16:31:46 INFO fs.TrashPolicyDefault: Namenode trash configuration: Deletion interval = 0 minutes, Emptier interval = 0 minutes.\n",
      "Deleted /user/jing\n",
      "rmr: DEPRECATED: Please use 'rm -r' instead.\n",
      "16/02/01 16:31:47 WARN util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "16/02/01 16:31:48 INFO fs.TrashPolicyDefault: Namenode trash configuration: Deletion interval = 0 minutes, Emptier interval = 0 minutes.\n",
      "Deleted /user\n"
     ]
    }
   ],
   "source": [
    "!./mapreduce.sh"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "UniqueWordsCounter\n",
    "    \n",
    "    Total=12592"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting mapper.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile mapper.py\n",
    "#!/usr/bin/python\n",
    "## mapper.py\n",
    "## Author: Jing Xu\n",
    "## Description: mapper code for HW3.3.2\n",
    "\n",
    "import sys\n",
    "\n",
    "max_basket_count = 0\n",
    "max_basket_items = []\n",
    "\n",
    "# read input from STDIN (standard input)\n",
    "for line in sys.stdin:\n",
    "    # split the line into words\n",
    "    basket_items = []\n",
    "    basket_count = 0\n",
    "    words = line.split()\n",
    "    for word in words:\n",
    "        basket_items.append(word)\n",
    "        basket_count+=1\n",
    "    if basket_count >= max_basket_count: \n",
    "        max_basket_count = basket_count\n",
    "        max_basket_items = basket_items\n",
    "\n",
    "print '%s\\t%d\\t%s' % ('max_count', max_basket_count, max_basket_items)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting reducer.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile reducer.py\n",
    "#!/usr/bin/python\n",
    "## reducer.py\n",
    "## Author: Jing Xu\n",
    "## Description: combiner code for HW3.3.2\n",
    "\n",
    "import sys\n",
    "\n",
    "# input comes from STDIN\n",
    "for line in sys.stdin:\n",
    "    line = line.strip('\\n')\n",
    "    print line"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting mapreduce.sh\n"
     ]
    }
   ],
   "source": [
    "%%writefile mapreduce.sh\n",
    "## mapreduce.sh\n",
    "## Author: Jing Xu\n",
    "## Description: mapreduce bash script for HW3.3.2\n",
    "\n",
    "hdfs dfs -mkdir /user # create hdfs folder\n",
    "wait\n",
    "hdfs dfs -mkdir /user/jing # create hdfs folder\n",
    "wait\n",
    "hadoop fs -put ProductPurchaseData.txt /user/jing # upload local file to hdfs folder\n",
    "wait\n",
    "\n",
    "# hadoop command to run streaming mapreduce job\n",
    "hadoop jar /Users/JingXu/Documents/hadoop-2.6.3/share/hadoop/tools/lib/hadoop-streaming-2.6.3.jar \\\n",
    "-D mapred.job.name=\"Count Job via Streaming\" \\\n",
    "-D mapred.map.tasks=1 \\\n",
    "-D mapred.reduce.tasks=1 \\\n",
    "-file mapper.py    -mapper mapper.py \\\n",
    "-file reducer.py   -reducer reducer.py \\\n",
    "-input /user/jing/* -output /user/jing/test-output1/\n",
    "\n",
    "wait\n",
    "\n",
    "hadoop dfs -cat /user/jing/test-output1/part-00000 #examine hdfs file output\n",
    "wait\n",
    "\n",
    "hdfs dfs -rmr /user/jing # remove hdfs folder\n",
    "hdfs dfs -rmr /user # remove hdfs folder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "16/02/01 16:31:56 WARN util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "16/02/01 16:31:57 WARN util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "16/02/01 16:31:59 WARN util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "16/02/01 16:32:01 WARN streaming.StreamJob: -file option is deprecated, please use generic option -files instead.\n",
      "16/02/01 16:32:01 WARN util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "packageJobJar: [mapper.py, reducer.py] [] /var/folders/zs/k144hqks281fbt0x68c_zj9m0000gp/T/streamjob2458430377133595769.jar tmpDir=null\n",
      "16/02/01 16:32:02 INFO Configuration.deprecation: session.id is deprecated. Instead, use dfs.metrics.session-id\n",
      "16/02/01 16:32:02 INFO jvm.JvmMetrics: Initializing JVM Metrics with processName=JobTracker, sessionId=\n",
      "16/02/01 16:32:02 INFO jvm.JvmMetrics: Cannot initialize JVM Metrics with processName=JobTracker, sessionId= - already initialized\n",
      "16/02/01 16:32:02 INFO mapred.FileInputFormat: Total input paths to process : 1\n",
      "16/02/01 16:32:02 INFO mapreduce.JobSubmitter: number of splits:1\n",
      "16/02/01 16:32:02 INFO Configuration.deprecation: mapred.reduce.tasks is deprecated. Instead, use mapreduce.job.reduces\n",
      "16/02/01 16:32:02 INFO Configuration.deprecation: mapred.job.name is deprecated. Instead, use mapreduce.job.name\n",
      "16/02/01 16:32:02 INFO Configuration.deprecation: mapred.map.tasks is deprecated. Instead, use mapreduce.job.maps\n",
      "16/02/01 16:32:02 INFO mapreduce.JobSubmitter: Submitting tokens for job: job_local438755493_0001\n",
      "16/02/01 16:32:02 INFO mapred.LocalDistributedCacheManager: Localized file:/Users/JingXu/Dropbox/DataScience/W261/W261/HW3/mapper.py as file:/tmp/hadoop-JingXu/mapred/local/1454373122686/mapper.py\n",
      "16/02/01 16:32:02 INFO mapred.LocalDistributedCacheManager: Localized file:/Users/JingXu/Dropbox/DataScience/W261/W261/HW3/reducer.py as file:/tmp/hadoop-JingXu/mapred/local/1454373122687/reducer.py\n",
      "16/02/01 16:32:02 INFO mapreduce.Job: The url to track the job: http://localhost:8080/\n",
      "16/02/01 16:32:02 INFO mapred.LocalJobRunner: OutputCommitter set in config null\n",
      "16/02/01 16:32:02 INFO mapreduce.Job: Running job: job_local438755493_0001\n",
      "16/02/01 16:32:02 INFO mapred.LocalJobRunner: OutputCommitter is org.apache.hadoop.mapred.FileOutputCommitter\n",
      "16/02/01 16:32:02 INFO mapred.LocalJobRunner: Waiting for map tasks\n",
      "16/02/01 16:32:02 INFO mapred.LocalJobRunner: Starting task: attempt_local438755493_0001_m_000000_0\n",
      "16/02/01 16:32:03 INFO util.ProcfsBasedProcessTree: ProcfsBasedProcessTree currently is supported only on Linux.\n",
      "16/02/01 16:32:03 INFO mapred.Task:  Using ResourceCalculatorProcessTree : null\n",
      "16/02/01 16:32:03 INFO mapred.MapTask: Processing split: hdfs://localhost:9000/user/jing/ProductPurchaseData.txt:0+3458517\n",
      "16/02/01 16:32:03 INFO mapred.MapTask: numReduceTasks: 1\n",
      "16/02/01 16:32:03 INFO mapred.MapTask: (EQUATOR) 0 kvi 26214396(104857584)\n",
      "16/02/01 16:32:03 INFO mapred.MapTask: mapreduce.task.io.sort.mb: 100\n",
      "16/02/01 16:32:03 INFO mapred.MapTask: soft limit at 83886080\n",
      "16/02/01 16:32:03 INFO mapred.MapTask: bufstart = 0; bufvoid = 104857600\n",
      "16/02/01 16:32:03 INFO mapred.MapTask: kvstart = 26214396; length = 6553600\n",
      "16/02/01 16:32:03 INFO mapred.MapTask: Map output collector class = org.apache.hadoop.mapred.MapTask$MapOutputBuffer\n",
      "16/02/01 16:32:03 INFO streaming.PipeMapRed: PipeMapRed exec [/Users/JingXu/Dropbox/DataScience/W261/W261/HW3/./mapper.py]\n",
      "16/02/01 16:32:03 INFO Configuration.deprecation: mapred.task.id is deprecated. Instead, use mapreduce.task.attempt.id\n",
      "16/02/01 16:32:03 INFO Configuration.deprecation: user.name is deprecated. Instead, use mapreduce.job.user.name\n",
      "16/02/01 16:32:03 INFO Configuration.deprecation: map.input.start is deprecated. Instead, use mapreduce.map.input.start\n",
      "16/02/01 16:32:03 INFO Configuration.deprecation: mapred.task.is.map is deprecated. Instead, use mapreduce.task.ismap\n",
      "16/02/01 16:32:03 INFO Configuration.deprecation: mapred.tip.id is deprecated. Instead, use mapreduce.task.id\n",
      "16/02/01 16:32:03 INFO Configuration.deprecation: mapred.skip.on is deprecated. Instead, use mapreduce.job.skiprecords\n",
      "16/02/01 16:32:03 INFO Configuration.deprecation: mapred.task.partition is deprecated. Instead, use mapreduce.task.partition\n",
      "16/02/01 16:32:03 INFO Configuration.deprecation: map.input.length is deprecated. Instead, use mapreduce.map.input.length\n",
      "16/02/01 16:32:03 INFO Configuration.deprecation: mapred.local.dir is deprecated. Instead, use mapreduce.cluster.local.dir\n",
      "16/02/01 16:32:03 INFO Configuration.deprecation: mapred.work.output.dir is deprecated. Instead, use mapreduce.task.output.dir\n",
      "16/02/01 16:32:03 INFO Configuration.deprecation: map.input.file is deprecated. Instead, use mapreduce.map.input.file\n",
      "16/02/01 16:32:03 INFO Configuration.deprecation: mapred.job.id is deprecated. Instead, use mapreduce.job.id\n",
      "16/02/01 16:32:03 INFO streaming.PipeMapRed: R/W/S=1/0/0 in:NA [rec/s] out:NA [rec/s]\n",
      "16/02/01 16:32:03 INFO streaming.PipeMapRed: R/W/S=10/0/0 in:NA [rec/s] out:NA [rec/s]\n",
      "16/02/01 16:32:03 INFO streaming.PipeMapRed: R/W/S=100/0/0 in:NA [rec/s] out:NA [rec/s]\n",
      "16/02/01 16:32:03 INFO streaming.PipeMapRed: R/W/S=1000/0/0 in:NA [rec/s] out:NA [rec/s]\n",
      "16/02/01 16:32:03 INFO streaming.PipeMapRed: R/W/S=10000/0/0 in:NA [rec/s] out:NA [rec/s]\n",
      "16/02/01 16:32:03 INFO streaming.PipeMapRed: MRErrorThread done\n",
      "16/02/01 16:32:03 INFO streaming.PipeMapRed: Records R/W=31101/1\n",
      "16/02/01 16:32:03 INFO streaming.PipeMapRed: mapRedFinished\n",
      "16/02/01 16:32:03 INFO mapred.LocalJobRunner: \n",
      "16/02/01 16:32:03 INFO mapred.MapTask: Starting flush of map output\n",
      "16/02/01 16:32:03 INFO mapred.MapTask: Spilling map output\n",
      "16/02/01 16:32:03 INFO mapred.MapTask: bufstart = 0; bufend = 460; bufvoid = 104857600\n",
      "16/02/01 16:32:03 INFO mapred.MapTask: kvstart = 26214396(104857584); kvend = 26214396(104857584); length = 1/6553600\n",
      "16/02/01 16:32:03 INFO mapred.MapTask: Finished spill 0\n",
      "16/02/01 16:32:03 INFO mapred.Task: Task:attempt_local438755493_0001_m_000000_0 is done. And is in the process of committing\n",
      "16/02/01 16:32:03 INFO mapred.LocalJobRunner: Records R/W=31101/1\n",
      "16/02/01 16:32:03 INFO mapred.Task: Task 'attempt_local438755493_0001_m_000000_0' done.\n",
      "16/02/01 16:32:03 INFO mapred.LocalJobRunner: Finishing task: attempt_local438755493_0001_m_000000_0\n",
      "16/02/01 16:32:03 INFO mapred.LocalJobRunner: map task executor complete.\n",
      "16/02/01 16:32:03 INFO mapred.LocalJobRunner: Waiting for reduce tasks\n",
      "16/02/01 16:32:03 INFO mapred.LocalJobRunner: Starting task: attempt_local438755493_0001_r_000000_0\n",
      "16/02/01 16:32:03 INFO util.ProcfsBasedProcessTree: ProcfsBasedProcessTree currently is supported only on Linux.\n",
      "16/02/01 16:32:03 INFO mapred.Task:  Using ResourceCalculatorProcessTree : null\n",
      "16/02/01 16:32:03 INFO mapred.ReduceTask: Using ShuffleConsumerPlugin: org.apache.hadoop.mapreduce.task.reduce.Shuffle@6491b172\n",
      "16/02/01 16:32:03 INFO reduce.MergeManagerImpl: MergerManager: memoryLimit=333971456, maxSingleShuffleLimit=83492864, mergeThreshold=220421168, ioSortFactor=10, memToMemMergeOutputsThreshold=10\n",
      "16/02/01 16:32:03 INFO reduce.EventFetcher: attempt_local438755493_0001_r_000000_0 Thread started: EventFetcher for fetching Map Completion Events\n",
      "16/02/01 16:32:03 INFO reduce.LocalFetcher: localfetcher#1 about to shuffle output of map attempt_local438755493_0001_m_000000_0 decomp: 466 len: 470 to MEMORY\n",
      "16/02/01 16:32:03 INFO reduce.InMemoryMapOutput: Read 466 bytes from map-output for attempt_local438755493_0001_m_000000_0\n",
      "16/02/01 16:32:03 INFO reduce.MergeManagerImpl: closeInMemoryFile -> map-output of size: 466, inMemoryMapOutputs.size() -> 1, commitMemory -> 0, usedMemory ->466\n",
      "16/02/01 16:32:03 INFO reduce.EventFetcher: EventFetcher is interrupted.. Returning\n",
      "16/02/01 16:32:03 INFO mapred.LocalJobRunner: 1 / 1 copied.\n",
      "16/02/01 16:32:03 INFO reduce.MergeManagerImpl: finalMerge called with 1 in-memory map-outputs and 0 on-disk map-outputs\n",
      "16/02/01 16:32:03 INFO mapred.Merger: Merging 1 sorted segments\n",
      "16/02/01 16:32:03 INFO mapred.Merger: Down to the last merge-pass, with 1 segments left of total size: 452 bytes\n",
      "16/02/01 16:32:03 INFO reduce.MergeManagerImpl: Merged 1 segments, 466 bytes to disk to satisfy reduce memory limit\n",
      "16/02/01 16:32:03 INFO reduce.MergeManagerImpl: Merging 1 files, 470 bytes from disk\n",
      "16/02/01 16:32:03 INFO reduce.MergeManagerImpl: Merging 0 segments, 0 bytes from memory into reduce\n",
      "16/02/01 16:32:03 INFO mapred.Merger: Merging 1 sorted segments\n",
      "16/02/01 16:32:03 INFO mapred.Merger: Down to the last merge-pass, with 1 segments left of total size: 452 bytes\n",
      "16/02/01 16:32:03 INFO mapred.LocalJobRunner: 1 / 1 copied.\n",
      "16/02/01 16:32:03 INFO streaming.PipeMapRed: PipeMapRed exec [/Users/JingXu/Dropbox/DataScience/W261/W261/HW3/./reducer.py]\n",
      "16/02/01 16:32:03 INFO Configuration.deprecation: mapred.job.tracker is deprecated. Instead, use mapreduce.jobtracker.address\n",
      "16/02/01 16:32:03 INFO Configuration.deprecation: mapred.map.tasks is deprecated. Instead, use mapreduce.job.maps\n",
      "16/02/01 16:32:03 INFO streaming.PipeMapRed: R/W/S=1/0/0 in:NA [rec/s] out:NA [rec/s]\n",
      "16/02/01 16:32:03 INFO streaming.PipeMapRed: Records R/W=1/1\n",
      "16/02/01 16:32:03 INFO streaming.PipeMapRed: MRErrorThread done\n",
      "16/02/01 16:32:03 INFO streaming.PipeMapRed: mapRedFinished\n",
      "16/02/01 16:32:03 INFO mapred.Task: Task:attempt_local438755493_0001_r_000000_0 is done. And is in the process of committing\n",
      "16/02/01 16:32:03 INFO mapred.LocalJobRunner: 1 / 1 copied.\n",
      "16/02/01 16:32:03 INFO mapred.Task: Task attempt_local438755493_0001_r_000000_0 is allowed to commit now\n",
      "16/02/01 16:32:03 INFO output.FileOutputCommitter: Saved output of task 'attempt_local438755493_0001_r_000000_0' to hdfs://localhost:9000/user/jing/test-output1/_temporary/0/task_local438755493_0001_r_000000\n",
      "16/02/01 16:32:03 INFO mapred.LocalJobRunner: Records R/W=1/1 > reduce\n",
      "16/02/01 16:32:03 INFO mapred.Task: Task 'attempt_local438755493_0001_r_000000_0' done.\n",
      "16/02/01 16:32:03 INFO mapred.LocalJobRunner: Finishing task: attempt_local438755493_0001_r_000000_0\n",
      "16/02/01 16:32:03 INFO mapred.LocalJobRunner: reduce task executor complete.\n",
      "16/02/01 16:32:03 INFO mapreduce.Job: Job job_local438755493_0001 running in uber mode : false\n",
      "16/02/01 16:32:03 INFO mapreduce.Job:  map 100% reduce 100%\n",
      "16/02/01 16:32:03 INFO mapreduce.Job: Job job_local438755493_0001 completed successfully\n",
      "16/02/01 16:32:03 INFO mapreduce.Job: Counters: 35\n",
      "\tFile System Counters\n",
      "\t\tFILE: Number of bytes read=4232\n",
      "\t\tFILE: Number of bytes written=523208\n",
      "\t\tFILE: Number of read operations=0\n",
      "\t\tFILE: Number of large read operations=0\n",
      "\t\tFILE: Number of write operations=0\n",
      "\t\tHDFS: Number of bytes read=6917034\n",
      "\t\tHDFS: Number of bytes written=458\n",
      "\t\tHDFS: Number of read operations=15\n",
      "\t\tHDFS: Number of large read operations=0\n",
      "\t\tHDFS: Number of write operations=4\n",
      "\tMap-Reduce Framework\n",
      "\t\tMap input records=31101\n",
      "\t\tMap output records=1\n",
      "\t\tMap output bytes=460\n",
      "\t\tMap output materialized bytes=470\n",
      "\t\tInput split bytes=107\n",
      "\t\tCombine input records=0\n",
      "\t\tCombine output records=0\n",
      "\t\tReduce input groups=1\n",
      "\t\tReduce shuffle bytes=470\n",
      "\t\tReduce input records=1\n",
      "\t\tReduce output records=1\n",
      "\t\tSpilled Records=2\n",
      "\t\tShuffled Maps =1\n",
      "\t\tFailed Shuffles=0\n",
      "\t\tMerged Map outputs=1\n",
      "\t\tGC time elapsed (ms)=8\n",
      "\t\tTotal committed heap usage (bytes)=582483968\n",
      "\tShuffle Errors\n",
      "\t\tBAD_ID=0\n",
      "\t\tCONNECTION=0\n",
      "\t\tIO_ERROR=0\n",
      "\t\tWRONG_LENGTH=0\n",
      "\t\tWRONG_MAP=0\n",
      "\t\tWRONG_REDUCE=0\n",
      "\tFile Input Format Counters \n",
      "\t\tBytes Read=3458517\n",
      "\tFile Output Format Counters \n",
      "\t\tBytes Written=458\n",
      "16/02/01 16:32:03 INFO streaming.StreamJob: Output directory: /user/jing/test-output1/\n",
      "DEPRECATED: Use of this script to execute hdfs command is deprecated.\n",
      "Instead use the hdfs command for it.\n",
      "\n",
      "16/02/01 16:32:05 WARN util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "max_count\t37\t['GRO21487', 'FRO85978', 'DAI89320', 'SNA53220', 'SNA55762', 'GRO46854', 'ELE38511', 'SNA66583', 'FRO79579', 'FRO92469', 'FRO40251', 'GRO97448', 'DAI35347', 'FRO31317', 'FRO87622', 'SNA42518', 'ELE53126', 'ELE17451', 'GRO32086', 'ELE30327', 'DAI58206', 'DAI38969', 'ELE16038', 'DAI75645', 'DAI55148', 'GRO94173', 'ELE43952', 'FRO69613', 'GRO81647', 'GRO73461', 'FRO24098', 'ELE96667', 'GRO88324', 'GRO82670', 'GRO12815', 'SNA37475', 'ELE24369']\n",
      "rmr: DEPRECATED: Please use 'rm -r' instead.\n",
      "16/02/01 16:32:06 WARN util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "16/02/01 16:32:07 INFO fs.TrashPolicyDefault: Namenode trash configuration: Deletion interval = 0 minutes, Emptier interval = 0 minutes.\n",
      "Deleted /user/jing\n",
      "rmr: DEPRECATED: Please use 'rm -r' instead.\n",
      "16/02/01 16:32:08 WARN util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "16/02/01 16:32:09 INFO fs.TrashPolicyDefault: Namenode trash configuration: Deletion interval = 0 minutes, Emptier interval = 0 minutes.\n",
      "Deleted /user\n"
     ]
    }
   ],
   "source": [
    "!./mapreduce.sh"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "max_count: 37\t\n",
    "basket: ['GRO21487', 'FRO85978', 'DAI89320', 'SNA53220', 'SNA55762', 'GRO46854', 'ELE38511', 'SNA66583', 'FRO79579', 'FRO92469', 'FRO40251', 'GRO97448', 'DAI35347', 'FRO31317', 'FRO87622', 'SNA42518', 'ELE53126', 'ELE17451', 'GRO32086', 'ELE30327', 'DAI58206', 'DAI38969', 'ELE16038', 'DAI75645', 'DAI55148', 'GRO94173', 'ELE43952', 'FRO69613', 'GRO81647', 'GRO73461', 'FRO24098', 'ELE96667', 'GRO88324', 'GRO82670', 'GRO12815', 'SNA37475', 'ELE24369']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting mapper.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile mapper.py\n",
    "#!/usr/bin/python\n",
    "## mapper.py\n",
    "## Author: Jing Xu\n",
    "## Description: mapper code for HW3.3.3\n",
    "\n",
    "import sys\n",
    "\n",
    "# read input from STDIN (standard input)\n",
    "for line in sys.stdin:\n",
    "    # split the line into words\n",
    "    words = line.split()\n",
    "    for word in words:\n",
    "        print '%s\\t%d' % ('*', 1)\n",
    "        print '%s\\t%d' % (word, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting mapper2.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile mapper2.py\n",
    "#!/usr/bin/python\n",
    "## mapper2.py\n",
    "## Author: Jing Xu\n",
    "## Description: mapper2 code for HW3.2.4\n",
    "\n",
    "import sys\n",
    "\n",
    "# read input from STDIN (standard input)\n",
    "for line in sys.stdin:\n",
    "    # split the line into words\n",
    "    line = line.strip('\\n')\n",
    "    word, count = line.split('\\t', 1)\n",
    "    count = int(count)\n",
    "    print '%s\\t%d' % (word, count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting reducer.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile reducer.py\n",
    "#!/usr/bin/python\n",
    "## combiner.py\n",
    "## Author: Jing Xu\n",
    "## Description: combiner code for HW3.2.4\n",
    "\n",
    "import sys\n",
    "\n",
    "current_word = None\n",
    "current_count = 0\n",
    "word = None\n",
    "\n",
    "# input comes from STDIN\n",
    "for line in sys.stdin:\n",
    "    # parse the input we got from mapper.py\n",
    "    word, count = line.split('\\t', 1)\n",
    "    # convert count (currently a string) to int\n",
    "    try:\n",
    "        count = int(count)\n",
    "    except ValueError:\n",
    "        # count was not a number, so silently\n",
    "        # ignore/discard this line\n",
    "        continue\n",
    "    if current_word == word:\n",
    "        current_count += count\n",
    "    else:\n",
    "        if current_word:\n",
    "            # write result to STDOUT\n",
    "            print '%s\\t%d' % (current_word, current_count)\n",
    "        current_count = count\n",
    "        current_word = word\n",
    "\n",
    "# do not forget to output the last word if needed!\n",
    "if current_word == word:\n",
    "    print '%s\\t%d' % (current_word, current_count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting reducer2.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile reducer2.py\n",
    "#!/usr/bin/python\n",
    "## reducer2.py\n",
    "## Author: Jing Xu\n",
    "## Description: reducer2 code for HW3.2.4\n",
    "\n",
    "import sys, Queue\n",
    "\n",
    "n_max = 50 #want top 50 largest numbers and 10 smallest numbers\n",
    "q_max = Queue.Queue(n_max) #queue for largest values\n",
    "total_words = 0 #total words used to calculate relative frequency\n",
    "\n",
    "for line in sys.stdin:\n",
    "    line = line.strip('\\n') #clean line\n",
    "    rec = line.split('\\t', 1) #split line\n",
    "    if rec[0] == '*': #if key is *, indicates it is the total count value\n",
    "        rec[1] = rec[1].strip('\\t')\n",
    "        total_words = int(rec[1]) #assign count as total_words\n",
    "    else:\n",
    "        rec[1] = rec[1].strip('\\t')\n",
    "        rec[1] = int(rec[1])\n",
    "        # whatever left is the biggest\n",
    "        if q_max.full():\n",
    "            q_max.get()\n",
    "        q_max.put([rec[0], rec[1]])\n",
    "\n",
    "print '\\n%d biggest records:' %n_max\n",
    "for i in range(n_max):\n",
    "    value = q_max.get() \n",
    "    freq = float(value[1])/float(total_words) #calculate relative frequency\n",
    "    value.append(freq) #append relative frequency to output \n",
    "    print value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting mapreduce.sh\n"
     ]
    }
   ],
   "source": [
    "%%writefile mapreduce.sh\n",
    "## mapreduce.sh\n",
    "## Author: Jing Xu\n",
    "## Description: mapreduce bash script for HW3.2.4\n",
    "\n",
    "hdfs dfs -mkdir /user # create hdfs folder\n",
    "wait\n",
    "hdfs dfs -mkdir /user/jing # create hdfs folder\n",
    "wait\n",
    "hadoop fs -put ProductPurchaseData.txt /user/jing # upload local file to hdfs folder\n",
    "wait\n",
    "\n",
    "# hadoop command to run streaming mapreduce job\n",
    "hadoop jar /Users/JingXu/Documents/hadoop-2.6.3/share/hadoop/tools/lib/hadoop-streaming-2.6.3.jar \\\n",
    "-D mapred.job.name=\"Count Job via Streaming1\" \\\n",
    "-D mapred.map.tasks=1 \\\n",
    "-D mapred.reduce.tasks=1 \\\n",
    "-file mapper.py    -mapper mapper.py \\\n",
    "-file reducer.py   -reducer reducer.py \\\n",
    "-input /user/jing/* -output /user/jing/test-output1/\n",
    "\n",
    "wait\n",
    "\n",
    "# second hadoop command to run streaming mapreduce job\n",
    "# sort primarly by numeric descending in 2nd key, then alphabetically on 1st key\n",
    "hadoop jar /Users/JingXu/Documents/hadoop-2.6.3/share/hadoop/tools/lib/hadoop-streaming-2.6.3.jar \\\n",
    "-D mapred.job.name=\"Count Job via Streaming2\" \\\n",
    "-D mapred.map.tasks=1 \\\n",
    "-D mapred.reduce.tasks=1 \\\n",
    "-D stream.num.map.output.key.fields=2 \\\n",
    "-D mapred.output.key.comparator.class=org.apache.hadoop.mapred.lib.KeyFieldBasedComparator \\\n",
    "-D mapred.text.key.comparator.options=\"-k2n -k1\" \\\n",
    "-file mapper2.py    -mapper mapper2.py \\\n",
    "-file reducer2.py   -reducer reducer2.py \\\n",
    "-input /user/jing/test-output1/part-00000 -output /user/jing/test-output2/\n",
    "\n",
    "wait\n",
    "\n",
    "hadoop dfs -cat /user/jing/test-output2/part-00000 #examine hdfs file output\n",
    "wait\n",
    "\n",
    "hdfs dfs -rmr /user/jing # remove hdfs folder\n",
    "hdfs dfs -rmr /user # remove hdfs folder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "16/02/01 16:32:17 WARN util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "16/02/01 16:32:19 WARN util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "16/02/01 16:32:21 WARN util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "16/02/01 16:32:22 WARN streaming.StreamJob: -file option is deprecated, please use generic option -files instead.\n",
      "16/02/01 16:32:22 WARN util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "packageJobJar: [mapper.py, reducer.py] [] /var/folders/zs/k144hqks281fbt0x68c_zj9m0000gp/T/streamjob4443816464631775356.jar tmpDir=null\n",
      "16/02/01 16:32:23 INFO Configuration.deprecation: session.id is deprecated. Instead, use dfs.metrics.session-id\n",
      "16/02/01 16:32:23 INFO jvm.JvmMetrics: Initializing JVM Metrics with processName=JobTracker, sessionId=\n",
      "16/02/01 16:32:23 INFO jvm.JvmMetrics: Cannot initialize JVM Metrics with processName=JobTracker, sessionId= - already initialized\n",
      "16/02/01 16:32:23 INFO mapred.FileInputFormat: Total input paths to process : 1\n",
      "16/02/01 16:32:23 INFO mapreduce.JobSubmitter: number of splits:1\n",
      "16/02/01 16:32:23 INFO Configuration.deprecation: mapred.reduce.tasks is deprecated. Instead, use mapreduce.job.reduces\n",
      "16/02/01 16:32:23 INFO Configuration.deprecation: mapred.job.name is deprecated. Instead, use mapreduce.job.name\n",
      "16/02/01 16:32:23 INFO Configuration.deprecation: mapred.map.tasks is deprecated. Instead, use mapreduce.job.maps\n",
      "16/02/01 16:32:24 INFO mapreduce.JobSubmitter: Submitting tokens for job: job_local2145735169_0001\n",
      "16/02/01 16:32:24 INFO mapred.LocalDistributedCacheManager: Localized file:/Users/JingXu/Dropbox/DataScience/W261/W261/HW3/mapper.py as file:/tmp/hadoop-JingXu/mapred/local/1454373144231/mapper.py\n",
      "16/02/01 16:32:24 INFO mapred.LocalDistributedCacheManager: Localized file:/Users/JingXu/Dropbox/DataScience/W261/W261/HW3/reducer.py as file:/tmp/hadoop-JingXu/mapred/local/1454373144232/reducer.py\n",
      "16/02/01 16:32:24 INFO mapreduce.Job: The url to track the job: http://localhost:8080/\n",
      "16/02/01 16:32:24 INFO mapred.LocalJobRunner: OutputCommitter set in config null\n",
      "16/02/01 16:32:24 INFO mapreduce.Job: Running job: job_local2145735169_0001\n",
      "16/02/01 16:32:24 INFO mapred.LocalJobRunner: OutputCommitter is org.apache.hadoop.mapred.FileOutputCommitter\n",
      "16/02/01 16:32:24 INFO mapred.LocalJobRunner: Waiting for map tasks\n",
      "16/02/01 16:32:24 INFO mapred.LocalJobRunner: Starting task: attempt_local2145735169_0001_m_000000_0\n",
      "16/02/01 16:32:24 INFO util.ProcfsBasedProcessTree: ProcfsBasedProcessTree currently is supported only on Linux.\n",
      "16/02/01 16:32:24 INFO mapred.Task:  Using ResourceCalculatorProcessTree : null\n",
      "16/02/01 16:32:24 INFO mapred.MapTask: Processing split: hdfs://localhost:9000/user/jing/ProductPurchaseData.txt:0+3458517\n",
      "16/02/01 16:32:24 INFO mapred.MapTask: numReduceTasks: 1\n",
      "16/02/01 16:32:24 INFO mapred.MapTask: (EQUATOR) 0 kvi 26214396(104857584)\n",
      "16/02/01 16:32:24 INFO mapred.MapTask: mapreduce.task.io.sort.mb: 100\n",
      "16/02/01 16:32:24 INFO mapred.MapTask: soft limit at 83886080\n",
      "16/02/01 16:32:24 INFO mapred.MapTask: bufstart = 0; bufvoid = 104857600\n",
      "16/02/01 16:32:24 INFO mapred.MapTask: kvstart = 26214396; length = 6553600\n",
      "16/02/01 16:32:24 INFO mapred.MapTask: Map output collector class = org.apache.hadoop.mapred.MapTask$MapOutputBuffer\n",
      "16/02/01 16:32:24 INFO streaming.PipeMapRed: PipeMapRed exec [/Users/JingXu/Dropbox/DataScience/W261/W261/HW3/./mapper.py]\n",
      "16/02/01 16:32:24 INFO Configuration.deprecation: mapred.task.id is deprecated. Instead, use mapreduce.task.attempt.id\n",
      "16/02/01 16:32:24 INFO Configuration.deprecation: user.name is deprecated. Instead, use mapreduce.job.user.name\n",
      "16/02/01 16:32:24 INFO Configuration.deprecation: map.input.start is deprecated. Instead, use mapreduce.map.input.start\n",
      "16/02/01 16:32:24 INFO Configuration.deprecation: mapred.task.is.map is deprecated. Instead, use mapreduce.task.ismap\n",
      "16/02/01 16:32:24 INFO Configuration.deprecation: mapred.tip.id is deprecated. Instead, use mapreduce.task.id\n",
      "16/02/01 16:32:24 INFO Configuration.deprecation: mapred.skip.on is deprecated. Instead, use mapreduce.job.skiprecords\n",
      "16/02/01 16:32:24 INFO Configuration.deprecation: mapred.task.partition is deprecated. Instead, use mapreduce.task.partition\n",
      "16/02/01 16:32:24 INFO Configuration.deprecation: map.input.length is deprecated. Instead, use mapreduce.map.input.length\n",
      "16/02/01 16:32:24 INFO Configuration.deprecation: mapred.local.dir is deprecated. Instead, use mapreduce.cluster.local.dir\n",
      "16/02/01 16:32:24 INFO Configuration.deprecation: mapred.work.output.dir is deprecated. Instead, use mapreduce.task.output.dir\n",
      "16/02/01 16:32:24 INFO Configuration.deprecation: map.input.file is deprecated. Instead, use mapreduce.map.input.file\n",
      "16/02/01 16:32:24 INFO Configuration.deprecation: mapred.job.id is deprecated. Instead, use mapreduce.job.id\n",
      "16/02/01 16:32:24 INFO streaming.PipeMapRed: R/W/S=1/0/0 in:NA [rec/s] out:NA [rec/s]\n",
      "16/02/01 16:32:24 INFO streaming.PipeMapRed: R/W/S=10/0/0 in:NA [rec/s] out:NA [rec/s]\n",
      "16/02/01 16:32:24 INFO streaming.PipeMapRed: R/W/S=100/0/0 in:NA [rec/s] out:NA [rec/s]\n",
      "16/02/01 16:32:24 INFO streaming.PipeMapRed: R/W/S=1000/0/0 in:NA [rec/s] out:NA [rec/s]\n",
      "16/02/01 16:32:24 INFO streaming.PipeMapRed: Records R/W=1216/1\n",
      "16/02/01 16:32:25 INFO streaming.PipeMapRed: R/W/S=10000/240299/0 in:NA [rec/s] out:NA [rec/s]\n",
      "16/02/01 16:32:25 INFO mapreduce.Job: Job job_local2145735169_0001 running in uber mode : false\n",
      "16/02/01 16:32:25 INFO mapreduce.Job:  map 0% reduce 0%\n",
      "16/02/01 16:32:25 INFO streaming.PipeMapRed: MRErrorThread done\n",
      "16/02/01 16:32:25 INFO streaming.PipeMapRed: mapRedFinished\n",
      "16/02/01 16:32:25 INFO mapred.LocalJobRunner: \n",
      "16/02/01 16:32:25 INFO mapred.MapTask: Starting flush of map output\n",
      "16/02/01 16:32:25 INFO mapred.MapTask: Spilling map output\n",
      "16/02/01 16:32:25 INFO mapred.MapTask: bufstart = 0; bufend = 5712360; bufvoid = 104857600\n",
      "16/02/01 16:32:25 INFO mapred.MapTask: kvstart = 26214396(104857584); kvend = 23167808(92671232); length = 3046589/6553600\n",
      "16/02/01 16:32:26 INFO mapred.MapTask: Finished spill 0\n",
      "16/02/01 16:32:26 INFO mapred.Task: Task:attempt_local2145735169_0001_m_000000_0 is done. And is in the process of committing\n",
      "16/02/01 16:32:26 INFO mapred.LocalJobRunner: Records R/W=1216/1\n",
      "16/02/01 16:32:26 INFO mapred.Task: Task 'attempt_local2145735169_0001_m_000000_0' done.\n",
      "16/02/01 16:32:26 INFO mapred.LocalJobRunner: Finishing task: attempt_local2145735169_0001_m_000000_0\n",
      "16/02/01 16:32:26 INFO mapred.LocalJobRunner: map task executor complete.\n",
      "16/02/01 16:32:26 INFO mapred.LocalJobRunner: Waiting for reduce tasks\n",
      "16/02/01 16:32:26 INFO mapred.LocalJobRunner: Starting task: attempt_local2145735169_0001_r_000000_0\n",
      "16/02/01 16:32:26 INFO util.ProcfsBasedProcessTree: ProcfsBasedProcessTree currently is supported only on Linux.\n",
      "16/02/01 16:32:26 INFO mapred.Task:  Using ResourceCalculatorProcessTree : null\n",
      "16/02/01 16:32:26 INFO mapred.ReduceTask: Using ShuffleConsumerPlugin: org.apache.hadoop.mapreduce.task.reduce.Shuffle@56beb399\n",
      "16/02/01 16:32:26 INFO reduce.MergeManagerImpl: MergerManager: memoryLimit=333971456, maxSingleShuffleLimit=83492864, mergeThreshold=220421168, ioSortFactor=10, memToMemMergeOutputsThreshold=10\n",
      "16/02/01 16:32:26 INFO reduce.EventFetcher: attempt_local2145735169_0001_r_000000_0 Thread started: EventFetcher for fetching Map Completion Events\n",
      "16/02/01 16:32:26 INFO reduce.LocalFetcher: localfetcher#1 about to shuffle output of map attempt_local2145735169_0001_m_000000_0 decomp: 7235658 len: 7235662 to MEMORY\n",
      "16/02/01 16:32:26 INFO reduce.InMemoryMapOutput: Read 7235658 bytes from map-output for attempt_local2145735169_0001_m_000000_0\n",
      "16/02/01 16:32:26 INFO reduce.MergeManagerImpl: closeInMemoryFile -> map-output of size: 7235658, inMemoryMapOutputs.size() -> 1, commitMemory -> 0, usedMemory ->7235658\n",
      "16/02/01 16:32:26 INFO reduce.EventFetcher: EventFetcher is interrupted.. Returning\n",
      "16/02/01 16:32:26 INFO mapred.LocalJobRunner: 1 / 1 copied.\n",
      "16/02/01 16:32:26 INFO reduce.MergeManagerImpl: finalMerge called with 1 in-memory map-outputs and 0 on-disk map-outputs\n",
      "16/02/01 16:32:26 INFO mapred.Merger: Merging 1 sorted segments\n",
      "16/02/01 16:32:26 INFO mapred.Merger: Down to the last merge-pass, with 1 segments left of total size: 7235654 bytes\n",
      "16/02/01 16:32:27 INFO reduce.MergeManagerImpl: Merged 1 segments, 7235658 bytes to disk to satisfy reduce memory limit\n",
      "16/02/01 16:32:27 INFO reduce.MergeManagerImpl: Merging 1 files, 7235662 bytes from disk\n",
      "16/02/01 16:32:27 INFO reduce.MergeManagerImpl: Merging 0 segments, 0 bytes from memory into reduce\n",
      "16/02/01 16:32:27 INFO mapred.Merger: Merging 1 sorted segments\n",
      "16/02/01 16:32:27 INFO mapred.Merger: Down to the last merge-pass, with 1 segments left of total size: 7235654 bytes\n",
      "16/02/01 16:32:27 INFO mapred.LocalJobRunner: 1 / 1 copied.\n",
      "16/02/01 16:32:27 INFO streaming.PipeMapRed: PipeMapRed exec [/Users/JingXu/Dropbox/DataScience/W261/W261/HW3/./reducer.py]\n",
      "16/02/01 16:32:27 INFO mapreduce.Job:  map 100% reduce 0%\n",
      "16/02/01 16:32:27 INFO Configuration.deprecation: mapred.job.tracker is deprecated. Instead, use mapreduce.jobtracker.address\n",
      "16/02/01 16:32:27 INFO Configuration.deprecation: mapred.map.tasks is deprecated. Instead, use mapreduce.job.maps\n",
      "16/02/01 16:32:27 INFO streaming.PipeMapRed: R/W/S=1/0/0 in:NA [rec/s] out:NA [rec/s]\n",
      "16/02/01 16:32:27 INFO streaming.PipeMapRed: R/W/S=10/0/0 in:NA [rec/s] out:NA [rec/s]\n",
      "16/02/01 16:32:27 INFO streaming.PipeMapRed: R/W/S=100/0/0 in:NA [rec/s] out:NA [rec/s]\n",
      "16/02/01 16:32:27 INFO streaming.PipeMapRed: R/W/S=1000/0/0 in:NA [rec/s] out:NA [rec/s]\n",
      "16/02/01 16:32:27 INFO streaming.PipeMapRed: R/W/S=10000/0/0 in:NA [rec/s] out:NA [rec/s]\n",
      "16/02/01 16:32:27 INFO streaming.PipeMapRed: R/W/S=100000/0/0 in:NA [rec/s] out:NA [rec/s]\n",
      "16/02/01 16:32:27 INFO streaming.PipeMapRed: R/W/S=200000/0/0 in:NA [rec/s] out:NA [rec/s]\n",
      "16/02/01 16:32:28 INFO streaming.PipeMapRed: R/W/S=300000/0/0 in:NA [rec/s] out:NA [rec/s]\n",
      "16/02/01 16:32:28 INFO streaming.PipeMapRed: R/W/S=400000/0/0 in:NA [rec/s] out:NA [rec/s]\n",
      "16/02/01 16:32:28 INFO streaming.PipeMapRed: Records R/W=432991/1\n",
      "16/02/01 16:32:28 INFO streaming.PipeMapRed: R/W/S=500000/2889/0 in:NA [rec/s] out:NA [rec/s]\n",
      "16/02/01 16:32:28 INFO streaming.PipeMapRed: R/W/S=600000/5780/0 in:600000=600000/1 [rec/s] out:5780=5780/1 [rec/s]\n",
      "16/02/01 16:32:28 INFO streaming.PipeMapRed: R/W/S=700000/8675/0 in:700000=700000/1 [rec/s] out:8675=8675/1 [rec/s]\n",
      "16/02/01 16:32:28 INFO streaming.PipeMapRed: MRErrorThread done\n",
      "16/02/01 16:32:28 INFO streaming.PipeMapRed: mapRedFinished\n",
      "16/02/01 16:32:28 INFO mapred.Task: Task:attempt_local2145735169_0001_r_000000_0 is done. And is in the process of committing\n",
      "16/02/01 16:32:28 INFO mapred.LocalJobRunner: 1 / 1 copied.\n",
      "16/02/01 16:32:28 INFO mapred.Task: Task attempt_local2145735169_0001_r_000000_0 is allowed to commit now\n",
      "16/02/01 16:32:28 INFO output.FileOutputCommitter: Saved output of task 'attempt_local2145735169_0001_r_000000_0' to hdfs://localhost:9000/user/jing/test-output1/_temporary/0/task_local2145735169_0001_r_000000\n",
      "16/02/01 16:32:28 INFO mapred.LocalJobRunner: Records R/W=432991/1 > reduce\n",
      "16/02/01 16:32:28 INFO mapred.Task: Task 'attempt_local2145735169_0001_r_000000_0' done.\n",
      "16/02/01 16:32:28 INFO mapred.LocalJobRunner: Finishing task: attempt_local2145735169_0001_r_000000_0\n",
      "16/02/01 16:32:28 INFO mapred.LocalJobRunner: reduce task executor complete.\n",
      "16/02/01 16:32:29 INFO mapreduce.Job:  map 100% reduce 100%\n",
      "16/02/01 16:32:29 INFO mapreduce.Job: Job job_local2145735169_0001 completed successfully\n",
      "16/02/01 16:32:29 INFO mapreduce.Job: Counters: 35\n",
      "\tFile System Counters\n",
      "\t\tFILE: Number of bytes read=14475772\n",
      "\t\tFILE: Number of bytes written=22232722\n",
      "\t\tFILE: Number of read operations=0\n",
      "\t\tFILE: Number of large read operations=0\n",
      "\t\tFILE: Number of write operations=0\n",
      "\t\tHDFS: Number of bytes read=6917034\n",
      "\t\tHDFS: Number of bytes written=142667\n",
      "\t\tHDFS: Number of read operations=15\n",
      "\t\tHDFS: Number of large read operations=0\n",
      "\t\tHDFS: Number of write operations=4\n",
      "\tMap-Reduce Framework\n",
      "\t\tMap input records=31101\n",
      "\t\tMap output records=761648\n",
      "\t\tMap output bytes=5712360\n",
      "\t\tMap output materialized bytes=7235662\n",
      "\t\tInput split bytes=107\n",
      "\t\tCombine input records=0\n",
      "\t\tCombine output records=0\n",
      "\t\tReduce input groups=12593\n",
      "\t\tReduce shuffle bytes=7235662\n",
      "\t\tReduce input records=761648\n",
      "\t\tReduce output records=12593\n",
      "\t\tSpilled Records=1523296\n",
      "\t\tShuffled Maps =1\n",
      "\t\tFailed Shuffles=0\n",
      "\t\tMerged Map outputs=1\n",
      "\t\tGC time elapsed (ms)=10\n",
      "\t\tTotal committed heap usage (bytes)=511705088\n",
      "\tShuffle Errors\n",
      "\t\tBAD_ID=0\n",
      "\t\tCONNECTION=0\n",
      "\t\tIO_ERROR=0\n",
      "\t\tWRONG_LENGTH=0\n",
      "\t\tWRONG_MAP=0\n",
      "\t\tWRONG_REDUCE=0\n",
      "\tFile Input Format Counters \n",
      "\t\tBytes Read=3458517\n",
      "\tFile Output Format Counters \n",
      "\t\tBytes Written=142667\n",
      "16/02/01 16:32:29 INFO streaming.StreamJob: Output directory: /user/jing/test-output1/\n",
      "16/02/01 16:32:30 WARN streaming.StreamJob: -file option is deprecated, please use generic option -files instead.\n",
      "16/02/01 16:32:30 WARN util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "packageJobJar: [mapper2.py, reducer2.py] [] /var/folders/zs/k144hqks281fbt0x68c_zj9m0000gp/T/streamjob6851351755219612695.jar tmpDir=null\n",
      "16/02/01 16:32:31 INFO Configuration.deprecation: session.id is deprecated. Instead, use dfs.metrics.session-id\n",
      "16/02/01 16:32:31 INFO jvm.JvmMetrics: Initializing JVM Metrics with processName=JobTracker, sessionId=\n",
      "16/02/01 16:32:31 INFO jvm.JvmMetrics: Cannot initialize JVM Metrics with processName=JobTracker, sessionId= - already initialized\n",
      "16/02/01 16:32:31 INFO mapred.FileInputFormat: Total input paths to process : 1\n",
      "16/02/01 16:32:31 INFO mapreduce.JobSubmitter: number of splits:1\n",
      "16/02/01 16:32:31 INFO Configuration.deprecation: mapred.reduce.tasks is deprecated. Instead, use mapreduce.job.reduces\n",
      "16/02/01 16:32:31 INFO Configuration.deprecation: mapred.output.key.comparator.class is deprecated. Instead, use mapreduce.job.output.key.comparator.class\n",
      "16/02/01 16:32:31 INFO Configuration.deprecation: mapred.text.key.comparator.options is deprecated. Instead, use mapreduce.partition.keycomparator.options\n",
      "16/02/01 16:32:31 INFO Configuration.deprecation: mapred.job.name is deprecated. Instead, use mapreduce.job.name\n",
      "16/02/01 16:32:31 INFO Configuration.deprecation: mapred.map.tasks is deprecated. Instead, use mapreduce.job.maps\n",
      "16/02/01 16:32:31 INFO mapreduce.JobSubmitter: Submitting tokens for job: job_local295030928_0001\n",
      "16/02/01 16:32:31 INFO mapred.LocalDistributedCacheManager: Localized file:/Users/JingXu/Dropbox/DataScience/W261/W261/HW3/mapper2.py as file:/tmp/hadoop-JingXu/mapred/local/1454373151741/mapper2.py\n",
      "16/02/01 16:32:31 INFO mapred.LocalDistributedCacheManager: Localized file:/Users/JingXu/Dropbox/DataScience/W261/W261/HW3/reducer2.py as file:/tmp/hadoop-JingXu/mapred/local/1454373151742/reducer2.py\n",
      "16/02/01 16:32:31 INFO mapreduce.Job: The url to track the job: http://localhost:8080/\n",
      "16/02/01 16:32:31 INFO mapred.LocalJobRunner: OutputCommitter set in config null\n",
      "16/02/01 16:32:31 INFO mapreduce.Job: Running job: job_local295030928_0001\n",
      "16/02/01 16:32:31 INFO mapred.LocalJobRunner: OutputCommitter is org.apache.hadoop.mapred.FileOutputCommitter\n",
      "16/02/01 16:32:31 INFO mapred.LocalJobRunner: Waiting for map tasks\n",
      "16/02/01 16:32:31 INFO mapred.LocalJobRunner: Starting task: attempt_local295030928_0001_m_000000_0\n",
      "16/02/01 16:32:32 INFO util.ProcfsBasedProcessTree: ProcfsBasedProcessTree currently is supported only on Linux.\n",
      "16/02/01 16:32:32 INFO mapred.Task:  Using ResourceCalculatorProcessTree : null\n",
      "16/02/01 16:32:32 INFO mapred.MapTask: Processing split: hdfs://localhost:9000/user/jing/test-output1/part-00000:0+142667\n",
      "16/02/01 16:32:32 INFO mapred.MapTask: numReduceTasks: 1\n",
      "16/02/01 16:32:32 INFO mapred.MapTask: (EQUATOR) 0 kvi 26214396(104857584)\n",
      "16/02/01 16:32:32 INFO mapred.MapTask: mapreduce.task.io.sort.mb: 100\n",
      "16/02/01 16:32:32 INFO mapred.MapTask: soft limit at 83886080\n",
      "16/02/01 16:32:32 INFO mapred.MapTask: bufstart = 0; bufvoid = 104857600\n",
      "16/02/01 16:32:32 INFO mapred.MapTask: kvstart = 26214396; length = 6553600\n",
      "16/02/01 16:32:32 INFO mapred.MapTask: Map output collector class = org.apache.hadoop.mapred.MapTask$MapOutputBuffer\n",
      "16/02/01 16:32:32 INFO streaming.PipeMapRed: PipeMapRed exec [/Users/JingXu/Dropbox/DataScience/W261/W261/HW3/./mapper2.py]\n",
      "16/02/01 16:32:32 INFO Configuration.deprecation: mapred.task.id is deprecated. Instead, use mapreduce.task.attempt.id\n",
      "16/02/01 16:32:32 INFO Configuration.deprecation: user.name is deprecated. Instead, use mapreduce.job.user.name\n",
      "16/02/01 16:32:32 INFO Configuration.deprecation: map.input.start is deprecated. Instead, use mapreduce.map.input.start\n",
      "16/02/01 16:32:32 INFO Configuration.deprecation: mapred.task.is.map is deprecated. Instead, use mapreduce.task.ismap\n",
      "16/02/01 16:32:32 INFO Configuration.deprecation: mapred.tip.id is deprecated. Instead, use mapreduce.task.id\n",
      "16/02/01 16:32:32 INFO Configuration.deprecation: mapred.skip.on is deprecated. Instead, use mapreduce.job.skiprecords\n",
      "16/02/01 16:32:32 INFO Configuration.deprecation: mapred.task.partition is deprecated. Instead, use mapreduce.task.partition\n",
      "16/02/01 16:32:32 INFO Configuration.deprecation: map.input.length is deprecated. Instead, use mapreduce.map.input.length\n",
      "16/02/01 16:32:32 INFO Configuration.deprecation: mapred.local.dir is deprecated. Instead, use mapreduce.cluster.local.dir\n",
      "16/02/01 16:32:32 INFO Configuration.deprecation: mapred.work.output.dir is deprecated. Instead, use mapreduce.task.output.dir\n",
      "16/02/01 16:32:32 INFO Configuration.deprecation: map.input.file is deprecated. Instead, use mapreduce.map.input.file\n",
      "16/02/01 16:32:32 INFO Configuration.deprecation: mapred.job.id is deprecated. Instead, use mapreduce.job.id\n",
      "16/02/01 16:32:32 INFO streaming.PipeMapRed: R/W/S=1/0/0 in:NA [rec/s] out:NA [rec/s]\n",
      "16/02/01 16:32:32 INFO streaming.PipeMapRed: R/W/S=10/0/0 in:NA [rec/s] out:NA [rec/s]\n",
      "16/02/01 16:32:32 INFO streaming.PipeMapRed: R/W/S=100/0/0 in:NA [rec/s] out:NA [rec/s]\n",
      "16/02/01 16:32:32 INFO streaming.PipeMapRed: R/W/S=1000/0/0 in:NA [rec/s] out:NA [rec/s]\n",
      "16/02/01 16:32:32 INFO streaming.PipeMapRed: R/W/S=10000/0/0 in:NA [rec/s] out:NA [rec/s]\n",
      "16/02/01 16:32:32 INFO streaming.PipeMapRed: Records R/W=11568/1\n",
      "16/02/01 16:32:32 INFO streaming.PipeMapRed: MRErrorThread done\n",
      "16/02/01 16:32:32 INFO streaming.PipeMapRed: mapRedFinished\n",
      "16/02/01 16:32:32 INFO mapred.LocalJobRunner: \n",
      "16/02/01 16:32:32 INFO mapred.MapTask: Starting flush of map output\n",
      "16/02/01 16:32:32 INFO mapred.MapTask: Spilling map output\n",
      "16/02/01 16:32:32 INFO mapred.MapTask: bufstart = 0; bufend = 155260; bufvoid = 104857600\n",
      "16/02/01 16:32:32 INFO mapred.MapTask: kvstart = 26214396(104857584); kvend = 26164028(104656112); length = 50369/6553600\n",
      "16/02/01 16:32:32 INFO mapred.MapTask: Finished spill 0\n",
      "16/02/01 16:32:32 INFO mapred.Task: Task:attempt_local295030928_0001_m_000000_0 is done. And is in the process of committing\n",
      "16/02/01 16:32:32 INFO mapred.LocalJobRunner: Records R/W=11568/1\n",
      "16/02/01 16:32:32 INFO mapred.Task: Task 'attempt_local295030928_0001_m_000000_0' done.\n",
      "16/02/01 16:32:32 INFO mapred.LocalJobRunner: Finishing task: attempt_local295030928_0001_m_000000_0\n",
      "16/02/01 16:32:32 INFO mapred.LocalJobRunner: map task executor complete.\n",
      "16/02/01 16:32:32 INFO mapred.LocalJobRunner: Waiting for reduce tasks\n",
      "16/02/01 16:32:32 INFO mapred.LocalJobRunner: Starting task: attempt_local295030928_0001_r_000000_0\n",
      "16/02/01 16:32:32 INFO util.ProcfsBasedProcessTree: ProcfsBasedProcessTree currently is supported only on Linux.\n",
      "16/02/01 16:32:32 INFO mapred.Task:  Using ResourceCalculatorProcessTree : null\n",
      "16/02/01 16:32:32 INFO mapred.ReduceTask: Using ShuffleConsumerPlugin: org.apache.hadoop.mapreduce.task.reduce.Shuffle@599ef69a\n",
      "16/02/01 16:32:32 INFO reduce.MergeManagerImpl: MergerManager: memoryLimit=333971456, maxSingleShuffleLimit=83492864, mergeThreshold=220421168, ioSortFactor=10, memToMemMergeOutputsThreshold=10\n",
      "16/02/01 16:32:32 INFO reduce.EventFetcher: attempt_local295030928_0001_r_000000_0 Thread started: EventFetcher for fetching Map Completion Events\n",
      "16/02/01 16:32:32 INFO reduce.LocalFetcher: localfetcher#1 about to shuffle output of map attempt_local295030928_0001_m_000000_0 decomp: 180448 len: 180452 to MEMORY\n",
      "16/02/01 16:32:32 INFO reduce.InMemoryMapOutput: Read 180448 bytes from map-output for attempt_local295030928_0001_m_000000_0\n",
      "16/02/01 16:32:32 INFO reduce.MergeManagerImpl: closeInMemoryFile -> map-output of size: 180448, inMemoryMapOutputs.size() -> 1, commitMemory -> 0, usedMemory ->180448\n",
      "16/02/01 16:32:32 INFO reduce.EventFetcher: EventFetcher is interrupted.. Returning\n",
      "16/02/01 16:32:32 INFO mapred.LocalJobRunner: 1 / 1 copied.\n",
      "16/02/01 16:32:32 INFO reduce.MergeManagerImpl: finalMerge called with 1 in-memory map-outputs and 0 on-disk map-outputs\n",
      "16/02/01 16:32:32 INFO mapred.Merger: Merging 1 sorted segments\n",
      "16/02/01 16:32:32 INFO mapred.Merger: Down to the last merge-pass, with 1 segments left of total size: 180435 bytes\n",
      "16/02/01 16:32:32 INFO reduce.MergeManagerImpl: Merged 1 segments, 180448 bytes to disk to satisfy reduce memory limit\n",
      "16/02/01 16:32:32 INFO reduce.MergeManagerImpl: Merging 1 files, 180452 bytes from disk\n",
      "16/02/01 16:32:32 INFO reduce.MergeManagerImpl: Merging 0 segments, 0 bytes from memory into reduce\n",
      "16/02/01 16:32:32 INFO mapred.Merger: Merging 1 sorted segments\n",
      "16/02/01 16:32:32 INFO mapred.Merger: Down to the last merge-pass, with 1 segments left of total size: 180435 bytes\n",
      "16/02/01 16:32:32 INFO mapred.LocalJobRunner: 1 / 1 copied.\n",
      "16/02/01 16:32:32 INFO streaming.PipeMapRed: PipeMapRed exec [/Users/JingXu/Dropbox/DataScience/W261/W261/HW3/./reducer2.py]\n",
      "16/02/01 16:32:32 INFO Configuration.deprecation: mapred.job.tracker is deprecated. Instead, use mapreduce.jobtracker.address\n",
      "16/02/01 16:32:32 INFO Configuration.deprecation: mapred.map.tasks is deprecated. Instead, use mapreduce.job.maps\n",
      "16/02/01 16:32:32 INFO streaming.PipeMapRed: R/W/S=1/0/0 in:NA [rec/s] out:NA [rec/s]\n",
      "16/02/01 16:32:32 INFO streaming.PipeMapRed: R/W/S=10/0/0 in:NA [rec/s] out:NA [rec/s]\n",
      "16/02/01 16:32:32 INFO streaming.PipeMapRed: R/W/S=100/0/0 in:NA [rec/s] out:NA [rec/s]\n",
      "16/02/01 16:32:32 INFO streaming.PipeMapRed: R/W/S=1000/0/0 in:NA [rec/s] out:NA [rec/s]\n",
      "16/02/01 16:32:32 INFO streaming.PipeMapRed: R/W/S=10000/0/0 in:NA [rec/s] out:NA [rec/s]\n",
      "16/02/01 16:32:32 INFO mapreduce.Job: Job job_local295030928_0001 running in uber mode : false\n",
      "16/02/01 16:32:32 INFO mapreduce.Job:  map 100% reduce 0%\n",
      "16/02/01 16:32:33 INFO streaming.PipeMapRed: Records R/W=12593/1\n",
      "16/02/01 16:32:33 INFO streaming.PipeMapRed: MRErrorThread done\n",
      "16/02/01 16:32:33 INFO streaming.PipeMapRed: mapRedFinished\n",
      "16/02/01 16:32:33 INFO mapred.Task: Task:attempt_local295030928_0001_r_000000_0 is done. And is in the process of committing\n",
      "16/02/01 16:32:33 INFO mapred.LocalJobRunner: 1 / 1 copied.\n",
      "16/02/01 16:32:33 INFO mapred.Task: Task attempt_local295030928_0001_r_000000_0 is allowed to commit now\n",
      "16/02/01 16:32:33 INFO output.FileOutputCommitter: Saved output of task 'attempt_local295030928_0001_r_000000_0' to hdfs://localhost:9000/user/jing/test-output2/_temporary/0/task_local295030928_0001_r_000000\n",
      "16/02/01 16:32:33 INFO mapred.LocalJobRunner: Records R/W=12593/1 > reduce\n",
      "16/02/01 16:32:33 INFO mapred.Task: Task 'attempt_local295030928_0001_r_000000_0' done.\n",
      "16/02/01 16:32:33 INFO mapred.LocalJobRunner: Finishing task: attempt_local295030928_0001_r_000000_0\n",
      "16/02/01 16:32:33 INFO mapred.LocalJobRunner: reduce task executor complete.\n",
      "16/02/01 16:32:33 INFO mapreduce.Job:  map 100% reduce 100%\n",
      "16/02/01 16:32:33 INFO mapreduce.Job: Job job_local295030928_0001 completed successfully\n",
      "16/02/01 16:32:33 INFO mapreduce.Job: Counters: 35\n",
      "\tFile System Counters\n",
      "\t\tFILE: Number of bytes read=365850\n",
      "\t\tFILE: Number of bytes written=1067726\n",
      "\t\tFILE: Number of read operations=0\n",
      "\t\tFILE: Number of large read operations=0\n",
      "\t\tFILE: Number of write operations=0\n",
      "\t\tHDFS: Number of bytes read=285334\n",
      "\t\tHDFS: Number of bytes written=2135\n",
      "\t\tHDFS: Number of read operations=13\n",
      "\t\tHDFS: Number of large read operations=0\n",
      "\t\tHDFS: Number of write operations=4\n",
      "\tMap-Reduce Framework\n",
      "\t\tMap input records=12593\n",
      "\t\tMap output records=12593\n",
      "\t\tMap output bytes=155260\n",
      "\t\tMap output materialized bytes=180452\n",
      "\t\tInput split bytes=107\n",
      "\t\tCombine input records=0\n",
      "\t\tCombine output records=0\n",
      "\t\tReduce input groups=12593\n",
      "\t\tReduce shuffle bytes=180452\n",
      "\t\tReduce input records=12593\n",
      "\t\tReduce output records=52\n",
      "\t\tSpilled Records=25186\n",
      "\t\tShuffled Maps =1\n",
      "\t\tFailed Shuffles=0\n",
      "\t\tMerged Map outputs=1\n",
      "\t\tGC time elapsed (ms)=12\n",
      "\t\tTotal committed heap usage (bytes)=650117120\n",
      "\tShuffle Errors\n",
      "\t\tBAD_ID=0\n",
      "\t\tCONNECTION=0\n",
      "\t\tIO_ERROR=0\n",
      "\t\tWRONG_LENGTH=0\n",
      "\t\tWRONG_MAP=0\n",
      "\t\tWRONG_REDUCE=0\n",
      "\tFile Input Format Counters \n",
      "\t\tBytes Read=142667\n",
      "\tFile Output Format Counters \n",
      "\t\tBytes Written=2135\n",
      "16/02/01 16:32:33 INFO streaming.StreamJob: Output directory: /user/jing/test-output2/\n",
      "DEPRECATED: Use of this script to execute hdfs command is deprecated.\n",
      "Instead use the hdfs command for it.\n",
      "\n",
      "16/02/01 16:32:35 WARN util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "\t\n",
      "50 biggest records:\t\n",
      "['GRO85051', 1214, 0.0031878242967880175]\t\n",
      "['DAI22896', 1219, 0.0032009537214041134]\t\n",
      "['GRO81087', 1220, 0.0032035796063273323]\t\n",
      "['DAI31081', 1261, 0.0033112408881793166]\t\n",
      "['GRO15017', 1275, 0.003348003277104384]\t\n",
      "['ELE91337', 1289, 0.0033847656660294517]\t\n",
      "['DAI43223', 1290, 0.003387391550952671]\t\n",
      "['SNA96271', 1295, 0.0034005209755687666]\t\n",
      "['ELE59935', 1311, 0.003442535134340273]\t\n",
      "['DAI88807', 1316, 0.0034556645589563684]\t\n",
      "['ELE74482', 1316, 0.0034556645589563684]\t\n",
      "['GRO61133', 1321, 0.003468793983572464]\t\n",
      "['ELE56788', 1345, 0.003531815221729723]\t\n",
      "['GRO38814', 1352, 0.0035501964161922567]\t\n",
      "['SNA90094', 1390, 0.0036499800432745837]\t\n",
      "['SNA93860', 1407, 0.0036946200869693085]\t\n",
      "['FRO53271', 1420, 0.003728756590971157]\t\n",
      "['FRO35904', 1436, 0.0037707707497426635]\t\n",
      "['ELE34057', 1489, 0.003909942650673277]\t\n",
      "['GRO94758', 1489, 0.003909942650673277]\t\n",
      "['ELE99737', 1516, 0.003980841543600193]\t\n",
      "['FRO78087', 1531, 0.00402022981744848]\t\n",
      "['DAI22177', 1627, 0.004272314770077516]\t\n",
      "['SNA55762', 1646, 0.00432220658361868]\t\n",
      "['ELE66810', 1697, 0.0044561267147028545]\t\n",
      "['FRO32293', 1702, 0.004469256139318951]\t\n",
      "['DAI83733', 1712, 0.004495514988551142]\t\n",
      "['ELE66600', 1713, 0.004498140873474361]\t\n",
      "['GRO46854', 1756, 0.004611053925172783]\t\n",
      "['DAI63921', 1773, 0.004655693968867509]\t\n",
      "['GRO56726', 1784, 0.0046845787030229185]\t\n",
      "['ELE74009', 1816, 0.004768607020565931]\t\n",
      "['GRO30386', 1840, 0.00483162825872319]\t\n",
      "['FRO85978', 1918, 0.005036447282734282]\t\n",
      "['GRO71621', 1920, 0.0050416990525807195]\t\n",
      "['GRO59710', 2004, 0.005262273386131126]\t\n",
      "['SNA99873', 2083, 0.005469718295065437]\t\n",
      "['GRO21487', 2115, 0.005553746612608449]\t\n",
      "['FRO80039', 2233, 0.005863601033548306]\t\n",
      "['ELE26917', 2292, 0.006018528244018234]\t\n",
      "['DAI85309', 2293, 0.006021154128941453]\t\n",
      "['FRO31317', 2330, 0.006118311871100561]\t\n",
      "['SNA45677', 2455, 0.006446547486502951]\t\n",
      "['DAI75645', 2736, 0.007184421149927526]\t\n",
      "['ELE32164', 2851, 0.007486397916097725]\t\n",
      "['SNA80324', 3044, 0.007993193706279015]\t\n",
      "['GRO73461', 3602, 0.009458437493435288]\t\n",
      "['ELE17451', 3875, 0.010175304077474108]\t\n",
      "['FRO40251', 3881, 0.010191059387013424]\t\n",
      "['DAI62779', 6667, 0.017506774783101905]\t\n",
      "rmr: DEPRECATED: Please use 'rm -r' instead.\n",
      "16/02/01 16:32:36 WARN util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "16/02/01 16:32:37 INFO fs.TrashPolicyDefault: Namenode trash configuration: Deletion interval = 0 minutes, Emptier interval = 0 minutes.\n",
      "Deleted /user/jing\n",
      "rmr: DEPRECATED: Please use 'rm -r' instead.\n",
      "16/02/01 16:32:38 WARN util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "16/02/01 16:32:39 INFO fs.TrashPolicyDefault: Namenode trash configuration: Deletion interval = 0 minutes, Emptier interval = 0 minutes.\n",
      "Deleted /user\n"
     ]
    }
   ],
   "source": [
    "!./mapreduce.sh"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**50 biggest records:**\n",
    "\n",
    "['GRO85051', 1214, 0.0031878242967880175]\t\n",
    "['DAI22896', 1219, 0.0032009537214041134]\t\n",
    "['GRO81087', 1220, 0.0032035796063273323]\t\n",
    "['DAI31081', 1261, 0.0033112408881793166]\t\n",
    "['GRO15017', 1275, 0.003348003277104384]\t\n",
    "['ELE91337', 1289, 0.0033847656660294517]\t\n",
    "['DAI43223', 1290, 0.003387391550952671]\t\n",
    "['SNA96271', 1295, 0.0034005209755687666]\t\n",
    "['ELE59935', 1311, 0.003442535134340273]\t\n",
    "['DAI88807', 1316, 0.0034556645589563684]\t\n",
    "['ELE74482', 1316, 0.0034556645589563684]\t\n",
    "['GRO61133', 1321, 0.003468793983572464]\t\n",
    "['ELE56788', 1345, 0.003531815221729723]\t\n",
    "['GRO38814', 1352, 0.0035501964161922567]\t\n",
    "['SNA90094', 1390, 0.0036499800432745837]\t\n",
    "['SNA93860', 1407, 0.0036946200869693085]\t\n",
    "['FRO53271', 1420, 0.003728756590971157]\t\n",
    "['FRO35904', 1436, 0.0037707707497426635]\t\n",
    "['ELE34057', 1489, 0.003909942650673277]\t\n",
    "['GRO94758', 1489, 0.003909942650673277]\t\n",
    "['ELE99737', 1516, 0.003980841543600193]\t\n",
    "['FRO78087', 1531, 0.00402022981744848]\t\n",
    "['DAI22177', 1627, 0.004272314770077516]\t\n",
    "['SNA55762', 1646, 0.00432220658361868]\t\n",
    "['ELE66810', 1697, 0.0044561267147028545]\t\n",
    "['FRO32293', 1702, 0.004469256139318951]\t\n",
    "['DAI83733', 1712, 0.004495514988551142]\t\n",
    "['ELE66600', 1713, 0.004498140873474361]\t\n",
    "['GRO46854', 1756, 0.004611053925172783]\t\n",
    "['DAI63921', 1773, 0.004655693968867509]\t\n",
    "['GRO56726', 1784, 0.0046845787030229185]\t\n",
    "['ELE74009', 1816, 0.004768607020565931]\t\n",
    "['GRO30386', 1840, 0.00483162825872319]\t\n",
    "['FRO85978', 1918, 0.005036447282734282]\t\n",
    "['GRO71621', 1920, 0.0050416990525807195]\t\n",
    "['GRO59710', 2004, 0.005262273386131126]\t\n",
    "['SNA99873', 2083, 0.005469718295065437]\t\n",
    "['GRO21487', 2115, 0.005553746612608449]\t\n",
    "['FRO80039', 2233, 0.005863601033548306]\t\n",
    "['ELE26917', 2292, 0.006018528244018234]\t\n",
    "['DAI85309', 2293, 0.006021154128941453]\t\n",
    "['FRO31317', 2330, 0.006118311871100561]\t\n",
    "['SNA45677', 2455, 0.006446547486502951]\t\n",
    "['DAI75645', 2736, 0.007184421149927526]\t\n",
    "['ELE32164', 2851, 0.007486397916097725]\t\n",
    "['SNA80324', 3044, 0.007993193706279015]\t\n",
    "['GRO73461', 3602, 0.009458437493435288]\t\n",
    "['ELE17451', 3875, 0.010175304077474108]\t\n",
    "['FRO40251', 3881, 0.010191059387013424]\t\n",
    "['DAI62779', 6667, 0.017506774783101905]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**HW3.4. (Computationally prohibitive but then again Hadoop can handle this) Pairs**\n",
    "\n",
    "**Suppose we want to recommend new products to the customer based on the products they\n",
    "have already browsed on the online website. Write a map-reduce program \n",
    "to find products which are frequently browsed together. Fix the support count (cooccurence count) to s = 100 \n",
    "(i.e. product pairs need to occur together at least 100 times to be considered frequent) \n",
    "and find pairs of items (sometimes referred to itemsets of size 2 in association rule mining) that have a support count of 100 or more.**\n",
    "\n",
    "**List the top 50 product pairs with corresponding support count (aka frequency), and relative frequency or support (number of records where they coccur, the number of records where they coccur/the number of baskets in the dataset)  in decreasing order of support  for frequent (100>count) itemsets of size 2.**\n",
    "\n",
    "**Use the Pairs pattern (lecture 3)  to  extract these frequent itemsets of size 2. Free free to use combiners if they bring value. Instrument your code with counters for count the number of times your mapper, combiner and reducers are called.**\n",
    "\n",
    "**Please output records of the following form for the top 50 pairs (itemsets of size 2):** \n",
    "\n",
    "**      item1, item2, support count, support**\n",
    "\n",
    "\n",
    "\n",
    "**Fix the ordering of the pairs lexicographically (left to right), \n",
    "and break ties in support (between pairs, if any exist) \n",
    "by taking the first ones in lexicographically increasing order.** \n",
    "\n",
    "**Report the compute time for the Pairs job. Describe the computational setup used (E.g., single computer; dual core; linux, number of mappers, number of reducers)\n",
    "Instrument your mapper, combiner, and reducer to count how many times each is called using Counters and report these counts.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting mapper.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile mapper.py\n",
    "#!/usr/bin/python\n",
    "## mapper.py\n",
    "## Author: Jing Xu\n",
    "## Description: mapper code for HW3.4.1\n",
    "\n",
    "import sys\n",
    "from itertools import combinations\n",
    "\n",
    "sys.stderr.write(\"reporter:counter:Mapper1Counter,Instance,1\\n\")\n",
    "\n",
    "# read input from STDIN (standard input)\n",
    "for line in sys.stdin:\n",
    "    # split the line into words\n",
    "    items = line.split()\n",
    "    items = set(items) #create unique set of items\n",
    "    items = list(items) #convert to list\n",
    "    pairs = [\",\".join(map(str,comb)) for comb in combinations(items, 2)] #list of every unique pair from items list\n",
    "    for pair in pairs:\n",
    "        pair = pair.split(',')\n",
    "        pair = sorted(pair, key=str.lower) #sort pair alphabetically for future matching\n",
    "        pair = pair[0]+','+pair[1]\n",
    "        print '%s\\t%d' % (pair, 1)\n",
    "        sys.stderr.write(\"reporter:counter:Mapper1Counter,Pairs,1\\n\")\n",
    "    print '%s\\t%d' % ('*,*', 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting mapper2.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile mapper2.py\n",
    "#!/usr/bin/python\n",
    "## mapper2.py\n",
    "## Author: Jing Xu\n",
    "## Description: mapper2 code for HW3.4.1\n",
    "\n",
    "import sys\n",
    "\n",
    "# read input from STDIN (standard input)\n",
    "for line in sys.stdin:\n",
    "    # split the line into words\n",
    "    line = line.strip('\\n')\n",
    "    pair, count = line.split('\\t', 1)\n",
    "    count = int(count)\n",
    "    print '%s\\t%d' % (pair, count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting reducer.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile reducer.py\n",
    "#!/usr/bin/python\n",
    "## combiner.py\n",
    "## Author: Jing Xu\n",
    "## Description: combiner code for HW3.4.1\n",
    "\n",
    "import sys\n",
    "\n",
    "current_pair = None\n",
    "current_count = 0\n",
    "word = None\n",
    "\n",
    "# input comes from STDIN\n",
    "for line in sys.stdin:\n",
    "    # parse the input we got from mapper.py\n",
    "    pair, count = line.split('\\t', 1)\n",
    "    # convert count (currently a string) to int\n",
    "    try:\n",
    "        count = int(count)\n",
    "    except ValueError:\n",
    "        # count was not a number, so silently\n",
    "        # ignore/discard this line\n",
    "        continue\n",
    "    if current_pair == pair:\n",
    "        current_count += count\n",
    "    else:\n",
    "        if current_pair and current_count>=100: #filter out all pairs that have cooccurence < 100\n",
    "            # write result to STDOUT\n",
    "            print '%s\\t%d' % (current_pair, current_count)\n",
    "            sys.stderr.write(\"reporter:counter:Reducer1Counter,Pairs,1\\n\")\n",
    "        current_count = count\n",
    "        current_pair = pair\n",
    "    \n",
    "# do not forget to output the last word if needed!\n",
    "if current_pair == pair and current_count>=100:\n",
    "    print '%s\\t%d' % (current_pair, current_count)\n",
    "    sys.stderr.write(\"reporter:counter:Reducer1Counter,Pairs,1\\n\")\n",
    "\n",
    "sys.stderr.write(\"reporter:counter:Reducer1Counter,Instances,1\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting reducer2.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile reducer2.py\n",
    "#!/usr/bin/python\n",
    "## reducer2.py\n",
    "## Author: Jing Xu\n",
    "## Description: reducer2 code for HW3.2.4\n",
    "\n",
    "import sys, Queue\n",
    "\n",
    "n_max = 50 #want top 50 largest numbers and 10 smallest numbers\n",
    "q_max = Queue.Queue(n_max) #queue for largest values\n",
    "total_buckets = 0 #total words used to calculate relative frequency\n",
    "\n",
    "for line in sys.stdin:\n",
    "    line = line.strip('\\n') #clean line\n",
    "    rec = line.split('\\t', 1) #split line\n",
    "    if rec[0] == '*,*': #if key is *,*, indicates it is the total buckets value\n",
    "        rec[1] = rec[1].strip('\\t')\n",
    "        total_buckets = int(rec[1]) #assign count as total_words\n",
    "    else:\n",
    "        first, second = rec[0].split(',')\n",
    "        rec[1] = rec[1].strip('\\t')\n",
    "        rec[1] = int(rec[1])\n",
    "        # whatever left is the biggest\n",
    "        if q_max.full():\n",
    "            q_max.get()\n",
    "        q_max.put([first, second, rec[1]])\n",
    "\n",
    "top_list = []\n",
    "        \n",
    "for i in range(n_max):\n",
    "    value = q_max.get() \n",
    "    freq = float(value[2])/float(total_buckets) #calculate relative frequency\n",
    "    value.append(freq) #append relative frequency to output \n",
    "    top_list.append(value)\n",
    "\n",
    "top_list.reverse() #reverse list so highest value is first\n",
    "    \n",
    "print '\\nTop %d product pairs:' %n_max\n",
    "for value in top_list:\n",
    "    print value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting mapreduce.sh\n"
     ]
    }
   ],
   "source": [
    "%%writefile mapreduce.sh\n",
    "## mapreduce.sh\n",
    "## Author: Jing Xu\n",
    "## Description: mapreduce bash script for HW3.4.1\n",
    "\n",
    "hdfs dfs -mkdir /user # create hdfs folder\n",
    "wait\n",
    "hdfs dfs -mkdir /user/jing # create hdfs folder\n",
    "wait\n",
    "hadoop fs -put ProductPurchaseData.txt /user/jing # upload local file to hdfs folder\n",
    "wait\n",
    "\n",
    "STARTTIME=$(date +%s)\n",
    "\n",
    "# hadoop command to run streaming mapreduce job\n",
    "hadoop jar /Users/JingXu/Documents/hadoop-2.6.3/share/hadoop/tools/lib/hadoop-streaming-2.6.3.jar \\\n",
    "-D mapred.job.name=\"Count Job via Streaming1\" \\\n",
    "-D mapred.map.tasks=1 \\\n",
    "-D mapred.reduce.tasks=1 \\\n",
    "-file mapper.py    -mapper mapper.py \\\n",
    "-file reducer.py   -reducer reducer.py \\\n",
    "-input /user/jing/* -output /user/jing/test-output1/\n",
    "\n",
    "wait\n",
    "\n",
    "# second hadoop command to run streaming mapreduce job\n",
    "# sort primarly by numeric descending in 2nd key, then alphabetically on 1st key\n",
    "hadoop jar /Users/JingXu/Documents/hadoop-2.6.3/share/hadoop/tools/lib/hadoop-streaming-2.6.3.jar \\\n",
    "-D mapred.job.name=\"Count Job via Streaming2\" \\\n",
    "-D mapred.map.tasks=1 \\\n",
    "-D mapred.reduce.tasks=1 \\\n",
    "-D stream.num.map.output.key.fields=2 \\\n",
    "-D mapred.output.key.comparator.class=org.apache.hadoop.mapred.lib.KeyFieldBasedComparator \\\n",
    "-D mapred.text.key.comparator.options=\"-k2n -k1\" \\\n",
    "-file mapper2.py    -mapper mapper2.py \\\n",
    "-file reducer2.py   -reducer reducer2.py \\\n",
    "-input /user/jing/test-output1/part-00000 -output /user/jing/test-output2/\n",
    "\n",
    "wait\n",
    "\n",
    "#calculate compute time for Pairs job\n",
    "ENDTIME=$(date +%s)\n",
    "echo \"It takes $(($ENDTIME - $STARTTIME)) seconds to complete this task\"\n",
    "wait\n",
    "\n",
    "hadoop dfs -cat /user/jing/test-output2/part-00000 #examine hdfs file output\n",
    "wait\n",
    "\n",
    "hdfs dfs -rmr /user/jing # remove hdfs folder\n",
    "hdfs dfs -rmr /user # remove hdfs folder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "16/02/01 16:32:52 WARN util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "16/02/01 16:32:54 WARN util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "16/02/01 16:32:55 WARN util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "16/02/01 16:32:57 WARN streaming.StreamJob: -file option is deprecated, please use generic option -files instead.\n",
      "16/02/01 16:32:57 WARN util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "packageJobJar: [mapper.py, reducer.py] [] /var/folders/zs/k144hqks281fbt0x68c_zj9m0000gp/T/streamjob8771942968046436571.jar tmpDir=null\n",
      "16/02/01 16:32:58 INFO Configuration.deprecation: session.id is deprecated. Instead, use dfs.metrics.session-id\n",
      "16/02/01 16:32:58 INFO jvm.JvmMetrics: Initializing JVM Metrics with processName=JobTracker, sessionId=\n",
      "16/02/01 16:32:58 INFO jvm.JvmMetrics: Cannot initialize JVM Metrics with processName=JobTracker, sessionId= - already initialized\n",
      "16/02/01 16:32:58 INFO mapred.FileInputFormat: Total input paths to process : 1\n",
      "16/02/01 16:32:58 INFO mapreduce.JobSubmitter: number of splits:1\n",
      "16/02/01 16:32:58 INFO Configuration.deprecation: mapred.reduce.tasks is deprecated. Instead, use mapreduce.job.reduces\n",
      "16/02/01 16:32:58 INFO Configuration.deprecation: mapred.job.name is deprecated. Instead, use mapreduce.job.name\n",
      "16/02/01 16:32:58 INFO Configuration.deprecation: mapred.map.tasks is deprecated. Instead, use mapreduce.job.maps\n",
      "16/02/01 16:32:58 INFO mapreduce.JobSubmitter: Submitting tokens for job: job_local403900502_0001\n",
      "16/02/01 16:32:59 INFO mapred.LocalDistributedCacheManager: Localized file:/Users/JingXu/Dropbox/DataScience/W261/W261/HW3/mapper.py as file:/tmp/hadoop-JingXu/mapred/local/1454373178975/mapper.py\n",
      "16/02/01 16:32:59 INFO mapred.LocalDistributedCacheManager: Localized file:/Users/JingXu/Dropbox/DataScience/W261/W261/HW3/reducer.py as file:/tmp/hadoop-JingXu/mapred/local/1454373178976/reducer.py\n",
      "16/02/01 16:32:59 INFO mapreduce.Job: The url to track the job: http://localhost:8080/\n",
      "16/02/01 16:32:59 INFO mapred.LocalJobRunner: OutputCommitter set in config null\n",
      "16/02/01 16:32:59 INFO mapreduce.Job: Running job: job_local403900502_0001\n",
      "16/02/01 16:32:59 INFO mapred.LocalJobRunner: OutputCommitter is org.apache.hadoop.mapred.FileOutputCommitter\n",
      "16/02/01 16:32:59 INFO mapred.LocalJobRunner: Waiting for map tasks\n",
      "16/02/01 16:32:59 INFO mapred.LocalJobRunner: Starting task: attempt_local403900502_0001_m_000000_0\n",
      "16/02/01 16:32:59 INFO util.ProcfsBasedProcessTree: ProcfsBasedProcessTree currently is supported only on Linux.\n",
      "16/02/01 16:32:59 INFO mapred.Task:  Using ResourceCalculatorProcessTree : null\n",
      "16/02/01 16:32:59 INFO mapred.MapTask: Processing split: hdfs://localhost:9000/user/jing/ProductPurchaseData.txt:0+3458517\n",
      "16/02/01 16:32:59 INFO mapred.MapTask: numReduceTasks: 1\n",
      "16/02/01 16:32:59 INFO mapred.MapTask: (EQUATOR) 0 kvi 26214396(104857584)\n",
      "16/02/01 16:32:59 INFO mapred.MapTask: mapreduce.task.io.sort.mb: 100\n",
      "16/02/01 16:32:59 INFO mapred.MapTask: soft limit at 83886080\n",
      "16/02/01 16:32:59 INFO mapred.MapTask: bufstart = 0; bufvoid = 104857600\n",
      "16/02/01 16:32:59 INFO mapred.MapTask: kvstart = 26214396; length = 6553600\n",
      "16/02/01 16:32:59 INFO mapred.MapTask: Map output collector class = org.apache.hadoop.mapred.MapTask$MapOutputBuffer\n",
      "16/02/01 16:32:59 INFO streaming.PipeMapRed: PipeMapRed exec [/Users/JingXu/Dropbox/DataScience/W261/W261/HW3/./mapper.py]\n",
      "16/02/01 16:32:59 INFO Configuration.deprecation: mapred.task.id is deprecated. Instead, use mapreduce.task.attempt.id\n",
      "16/02/01 16:32:59 INFO Configuration.deprecation: user.name is deprecated. Instead, use mapreduce.job.user.name\n",
      "16/02/01 16:32:59 INFO Configuration.deprecation: map.input.start is deprecated. Instead, use mapreduce.map.input.start\n",
      "16/02/01 16:32:59 INFO Configuration.deprecation: mapred.task.is.map is deprecated. Instead, use mapreduce.task.ismap\n",
      "16/02/01 16:32:59 INFO Configuration.deprecation: mapred.tip.id is deprecated. Instead, use mapreduce.task.id\n",
      "16/02/01 16:32:59 INFO Configuration.deprecation: mapred.skip.on is deprecated. Instead, use mapreduce.job.skiprecords\n",
      "16/02/01 16:32:59 INFO Configuration.deprecation: mapred.task.partition is deprecated. Instead, use mapreduce.task.partition\n",
      "16/02/01 16:32:59 INFO Configuration.deprecation: map.input.length is deprecated. Instead, use mapreduce.map.input.length\n",
      "16/02/01 16:32:59 INFO Configuration.deprecation: mapred.local.dir is deprecated. Instead, use mapreduce.cluster.local.dir\n",
      "16/02/01 16:32:59 INFO Configuration.deprecation: mapred.work.output.dir is deprecated. Instead, use mapreduce.task.output.dir\n",
      "16/02/01 16:32:59 INFO Configuration.deprecation: map.input.file is deprecated. Instead, use mapreduce.map.input.file\n",
      "16/02/01 16:32:59 INFO Configuration.deprecation: mapred.job.id is deprecated. Instead, use mapreduce.job.id\n",
      "16/02/01 16:32:59 INFO streaming.PipeMapRed: R/W/S=1/0/0 in:NA [rec/s] out:NA [rec/s]\n",
      "16/02/01 16:32:59 INFO streaming.PipeMapRed: R/W/S=10/0/0 in:NA [rec/s] out:NA [rec/s]\n",
      "16/02/01 16:32:59 INFO streaming.PipeMapRed: R/W/S=100/0/0 in:NA [rec/s] out:NA [rec/s]\n",
      "16/02/01 16:32:59 INFO streaming.PipeMapRed: R/W/S=1000/0/0 in:NA [rec/s] out:NA [rec/s]\n",
      "16/02/01 16:32:59 INFO streaming.PipeMapRed: Records R/W=1216/1\n",
      "16/02/01 16:33:00 INFO mapreduce.Job: Job job_local403900502_0001 running in uber mode : false\n",
      "16/02/01 16:33:00 INFO mapreduce.Job:  map 0% reduce 0%\n",
      "16/02/01 16:33:05 INFO mapred.LocalJobRunner: Records R/W=1216/1 > map\n",
      "16/02/01 16:33:06 INFO mapreduce.Job:  map 18% reduce 0%\n",
      "16/02/01 16:33:07 INFO streaming.PipeMapRed: R/W/S=10000/863246/0 in:1250=10000/8 [rec/s] out:107905=863246/8 [rec/s]\n",
      "16/02/01 16:33:08 INFO mapred.LocalJobRunner: Records R/W=1216/1 > map\n",
      "16/02/01 16:33:09 INFO mapreduce.Job:  map 25% reduce 0%\n",
      "16/02/01 16:33:09 INFO streaming.PipeMapRed: Records R/W=13103/1116545\n",
      "16/02/01 16:33:11 INFO mapred.LocalJobRunner: Records R/W=13103/1116545 > map\n",
      "16/02/01 16:33:12 INFO mapreduce.Job:  map 35% reduce 0%\n",
      "16/02/01 16:33:14 INFO mapred.LocalJobRunner: Records R/W=13103/1116545 > map\n",
      "16/02/01 16:33:15 INFO mapreduce.Job:  map 45% reduce 0%\n",
      "16/02/01 16:33:17 INFO mapred.LocalJobRunner: Records R/W=13103/1116545 > map\n",
      "16/02/01 16:33:18 INFO mapreduce.Job:  map 56% reduce 0%\n",
      "16/02/01 16:33:19 INFO streaming.PipeMapRed: Records R/W=29427/2318016\n",
      "16/02/01 16:33:19 INFO mapred.MapTask: Spilling map output\n",
      "16/02/01 16:33:19 INFO mapred.MapTask: bufstart = 0; bufend = 46429944; bufvoid = 104857600\n",
      "16/02/01 16:33:19 INFO mapred.MapTask: kvstart = 26214396(104857584); kvend = 16850364(67401456); length = 9364033/6553600\n",
      "16/02/01 16:33:19 INFO mapred.MapTask: (EQUATOR) 56016904 kvi 14004220(56016880)\n",
      "16/02/01 16:33:20 INFO mapred.LocalJobRunner: Records R/W=29427/2318016 > map\n",
      "16/02/01 16:33:21 INFO mapreduce.Job:  map 66% reduce 0%\n",
      "16/02/01 16:33:21 INFO streaming.PipeMapRed: MRErrorThread done\n",
      "16/02/01 16:33:21 INFO streaming.PipeMapRed: mapRedFinished\n",
      "16/02/01 16:33:21 INFO mapred.LocalJobRunner: Records R/W=29427/2318016 > map\n",
      "16/02/01 16:33:21 INFO mapred.MapTask: Starting flush of map output\n",
      "16/02/01 16:33:23 INFO mapred.LocalJobRunner: Records R/W=29427/2318016 > sort\n",
      "16/02/01 16:33:24 INFO mapreduce.Job:  map 67% reduce 0%\n",
      "16/02/01 16:33:24 INFO mapred.MapTask: Finished spill 0\n",
      "16/02/01 16:33:24 INFO mapred.MapTask: (RESET) equator 56016904 kv 14004220(56016880) kvi 13107796(52431184)\n",
      "16/02/01 16:33:24 INFO mapred.MapTask: Spilling map output\n",
      "16/02/01 16:33:24 INFO mapred.MapTask: bufstart = 56016904; bufend = 60453846; bufvoid = 104857600\n",
      "16/02/01 16:33:24 INFO mapred.MapTask: kvstart = 14004220(56016880); kvend = 13107800(52431200); length = 896421/6553600\n",
      "16/02/01 16:33:24 INFO mapred.MapTask: Finished spill 1\n",
      "16/02/01 16:33:24 INFO mapred.Merger: Merging 2 sorted segments\n",
      "16/02/01 16:33:24 INFO mapred.Merger: Down to the last merge-pass, with 2 segments left of total size: 55997116 bytes\n",
      "16/02/01 16:33:26 INFO mapred.LocalJobRunner: Records R/W=29427/2318016 > sort > \n",
      "16/02/01 16:33:26 INFO mapreduce.Job:  map 82% reduce 0%\n",
      "16/02/01 16:33:28 INFO mapred.Task: Task:attempt_local403900502_0001_m_000000_0 is done. And is in the process of committing\n",
      "16/02/01 16:33:28 INFO mapred.LocalJobRunner: Records R/W=29427/2318016 > sort\n",
      "16/02/01 16:33:28 INFO mapred.Task: Task 'attempt_local403900502_0001_m_000000_0' done.\n",
      "16/02/01 16:33:28 INFO mapred.LocalJobRunner: Finishing task: attempt_local403900502_0001_m_000000_0\n",
      "16/02/01 16:33:28 INFO mapred.LocalJobRunner: map task executor complete.\n",
      "16/02/01 16:33:28 INFO mapred.LocalJobRunner: Waiting for reduce tasks\n",
      "16/02/01 16:33:28 INFO mapred.LocalJobRunner: Starting task: attempt_local403900502_0001_r_000000_0\n",
      "16/02/01 16:33:28 INFO util.ProcfsBasedProcessTree: ProcfsBasedProcessTree currently is supported only on Linux.\n",
      "16/02/01 16:33:28 INFO mapred.Task:  Using ResourceCalculatorProcessTree : null\n",
      "16/02/01 16:33:28 INFO mapred.ReduceTask: Using ShuffleConsumerPlugin: org.apache.hadoop.mapreduce.task.reduce.Shuffle@1e6bcecc\n",
      "16/02/01 16:33:28 INFO reduce.MergeManagerImpl: MergerManager: memoryLimit=333971456, maxSingleShuffleLimit=83492864, mergeThreshold=220421168, ioSortFactor=10, memToMemMergeOutputsThreshold=10\n",
      "16/02/01 16:33:28 INFO reduce.EventFetcher: attempt_local403900502_0001_r_000000_0 Thread started: EventFetcher for fetching Map Completion Events\n",
      "16/02/01 16:33:28 INFO reduce.LocalFetcher: localfetcher#1 about to shuffle output of map attempt_local403900502_0001_m_000000_0 decomp: 55997118 len: 55997122 to MEMORY\n",
      "16/02/01 16:33:28 INFO reduce.InMemoryMapOutput: Read 55997118 bytes from map-output for attempt_local403900502_0001_m_000000_0\n",
      "16/02/01 16:33:28 INFO reduce.MergeManagerImpl: closeInMemoryFile -> map-output of size: 55997118, inMemoryMapOutputs.size() -> 1, commitMemory -> 0, usedMemory ->55997118\n",
      "16/02/01 16:33:28 INFO reduce.EventFetcher: EventFetcher is interrupted.. Returning\n",
      "16/02/01 16:33:28 INFO mapred.LocalJobRunner: 1 / 1 copied.\n",
      "16/02/01 16:33:28 INFO reduce.MergeManagerImpl: finalMerge called with 1 in-memory map-outputs and 0 on-disk map-outputs\n",
      "16/02/01 16:33:28 INFO mapred.Merger: Merging 1 sorted segments\n",
      "16/02/01 16:33:28 INFO mapred.Merger: Down to the last merge-pass, with 1 segments left of total size: 55997112 bytes\n",
      "16/02/01 16:33:28 INFO mapreduce.Job:  map 100% reduce 0%\n",
      "16/02/01 16:33:29 INFO reduce.MergeManagerImpl: Merged 1 segments, 55997118 bytes to disk to satisfy reduce memory limit\n",
      "16/02/01 16:33:29 INFO reduce.MergeManagerImpl: Merging 1 files, 55997122 bytes from disk\n",
      "16/02/01 16:33:29 INFO reduce.MergeManagerImpl: Merging 0 segments, 0 bytes from memory into reduce\n",
      "16/02/01 16:33:29 INFO mapred.Merger: Merging 1 sorted segments\n",
      "16/02/01 16:33:29 INFO mapred.Merger: Down to the last merge-pass, with 1 segments left of total size: 55997112 bytes\n",
      "16/02/01 16:33:29 INFO mapred.LocalJobRunner: 1 / 1 copied.\n",
      "16/02/01 16:33:29 INFO streaming.PipeMapRed: PipeMapRed exec [/Users/JingXu/Dropbox/DataScience/W261/W261/HW3/./reducer.py]\n",
      "16/02/01 16:33:29 INFO Configuration.deprecation: mapred.job.tracker is deprecated. Instead, use mapreduce.jobtracker.address\n",
      "16/02/01 16:33:29 INFO Configuration.deprecation: mapred.map.tasks is deprecated. Instead, use mapreduce.job.maps\n",
      "16/02/01 16:33:29 INFO streaming.PipeMapRed: R/W/S=1/0/0 in:NA [rec/s] out:NA [rec/s]\n",
      "16/02/01 16:33:29 INFO streaming.PipeMapRed: R/W/S=10/0/0 in:NA [rec/s] out:NA [rec/s]\n",
      "16/02/01 16:33:29 INFO streaming.PipeMapRed: R/W/S=100/0/0 in:NA [rec/s] out:NA [rec/s]\n",
      "16/02/01 16:33:29 INFO streaming.PipeMapRed: R/W/S=1000/0/0 in:NA [rec/s] out:NA [rec/s]\n",
      "16/02/01 16:33:30 INFO streaming.PipeMapRed: R/W/S=10000/0/0 in:NA [rec/s] out:NA [rec/s]\n",
      "16/02/01 16:33:30 INFO streaming.PipeMapRed: R/W/S=100000/0/0 in:NA [rec/s] out:NA [rec/s]\n",
      "16/02/01 16:33:30 INFO streaming.PipeMapRed: R/W/S=200000/0/0 in:NA [rec/s] out:NA [rec/s]\n",
      "16/02/01 16:33:30 INFO streaming.PipeMapRed: R/W/S=300000/0/0 in:NA [rec/s] out:NA [rec/s]\n",
      "16/02/01 16:33:30 INFO streaming.PipeMapRed: R/W/S=400000/0/0 in:NA [rec/s] out:NA [rec/s]\n",
      "16/02/01 16:33:30 INFO streaming.PipeMapRed: R/W/S=500000/0/0 in:NA [rec/s] out:NA [rec/s]\n",
      "16/02/01 16:33:31 INFO streaming.PipeMapRed: R/W/S=600000/0/0 in:600000=600000/1 [rec/s] out:0=0/1 [rec/s]\n",
      "16/02/01 16:33:31 INFO streaming.PipeMapRed: R/W/S=700000/0/0 in:700000=700000/1 [rec/s] out:0=0/1 [rec/s]\n",
      "16/02/01 16:33:31 INFO streaming.PipeMapRed: R/W/S=800000/0/0 in:800000=800000/1 [rec/s] out:0=0/1 [rec/s]\n",
      "16/02/01 16:33:31 INFO streaming.PipeMapRed: R/W/S=900000/0/0 in:900000=900000/1 [rec/s] out:0=0/1 [rec/s]\n",
      "16/02/01 16:33:31 INFO streaming.PipeMapRed: R/W/S=1000000/0/0 in:1000000=1000000/1 [rec/s] out:0=0/1 [rec/s]\n",
      "16/02/01 16:33:31 INFO streaming.PipeMapRed: R/W/S=1100000/0/0 in:1100000=1100000/1 [rec/s] out:0=0/1 [rec/s]\n",
      "16/02/01 16:33:32 INFO streaming.PipeMapRed: R/W/S=1200000/0/0 in:600000=1200000/2 [rec/s] out:0=0/2 [rec/s]\n",
      "16/02/01 16:33:32 INFO streaming.PipeMapRed: Records R/W=1273395/1\n",
      "16/02/01 16:33:32 INFO streaming.PipeMapRed: R/W/S=1300000/745/0 in:650000=1300000/2 [rec/s] out:372=745/2 [rec/s]\n",
      "16/02/01 16:33:32 INFO streaming.PipeMapRed: R/W/S=1400000/745/0 in:700000=1400000/2 [rec/s] out:372=745/2 [rec/s]\n",
      "16/02/01 16:33:32 INFO streaming.PipeMapRed: R/W/S=1500000/745/0 in:750000=1500000/2 [rec/s] out:372=745/2 [rec/s]\n",
      "16/02/01 16:33:32 INFO streaming.PipeMapRed: R/W/S=1600000/745/0 in:800000=1600000/2 [rec/s] out:372=745/2 [rec/s]\n",
      "16/02/01 16:33:32 INFO streaming.PipeMapRed: R/W/S=1700000/745/0 in:850000=1700000/2 [rec/s] out:372=745/2 [rec/s]\n",
      "16/02/01 16:33:33 INFO streaming.PipeMapRed: R/W/S=1800000/745/0 in:600000=1800000/3 [rec/s] out:248=745/3 [rec/s]\n",
      "16/02/01 16:33:33 INFO streaming.PipeMapRed: R/W/S=1900000/745/0 in:633333=1900000/3 [rec/s] out:248=745/3 [rec/s]\n",
      "16/02/01 16:33:33 INFO streaming.PipeMapRed: R/W/S=2000000/745/0 in:666666=2000000/3 [rec/s] out:248=745/3 [rec/s]\n",
      "16/02/01 16:33:33 INFO streaming.PipeMapRed: R/W/S=2100000/745/0 in:700000=2100000/3 [rec/s] out:248=745/3 [rec/s]\n",
      "16/02/01 16:33:33 INFO streaming.PipeMapRed: R/W/S=2200000/745/0 in:733333=2200000/3 [rec/s] out:248=745/3 [rec/s]\n",
      "16/02/01 16:33:33 INFO streaming.PipeMapRed: R/W/S=2300000/745/0 in:575000=2300000/4 [rec/s] out:186=745/4 [rec/s]\n",
      "16/02/01 16:33:34 INFO mapred.LocalJobRunner: Records R/W=1273395/1 > reduce\n",
      "16/02/01 16:33:34 INFO streaming.PipeMapRed: R/W/S=2400000/745/0 in:600000=2400000/4 [rec/s] out:186=745/4 [rec/s]\n",
      "16/02/01 16:33:34 INFO streaming.PipeMapRed: R/W/S=2500000/745/0 in:625000=2500000/4 [rec/s] out:186=745/4 [rec/s]\n",
      "16/02/01 16:33:34 INFO mapreduce.Job:  map 100% reduce 97%\n",
      "16/02/01 16:33:34 INFO streaming.PipeMapRed: MRErrorThread done\n",
      "16/02/01 16:33:34 INFO streaming.PipeMapRed: mapRedFinished\n",
      "16/02/01 16:33:34 INFO mapred.Task: Task:attempt_local403900502_0001_r_000000_0 is done. And is in the process of committing\n",
      "16/02/01 16:33:34 INFO mapred.LocalJobRunner: Records R/W=1273395/1 > reduce\n",
      "16/02/01 16:33:34 INFO mapred.Task: Task attempt_local403900502_0001_r_000000_0 is allowed to commit now\n",
      "16/02/01 16:33:34 INFO output.FileOutputCommitter: Saved output of task 'attempt_local403900502_0001_r_000000_0' to hdfs://localhost:9000/user/jing/test-output1/_temporary/0/task_local403900502_0001_r_000000\n",
      "16/02/01 16:33:34 INFO mapred.LocalJobRunner: Records R/W=1273395/1 > reduce\n",
      "16/02/01 16:33:34 INFO mapred.Task: Task 'attempt_local403900502_0001_r_000000_0' done.\n",
      "16/02/01 16:33:34 INFO mapred.LocalJobRunner: Finishing task: attempt_local403900502_0001_r_000000_0\n",
      "16/02/01 16:33:34 INFO mapred.LocalJobRunner: reduce task executor complete.\n",
      "16/02/01 16:33:35 INFO mapreduce.Job:  map 100% reduce 100%\n",
      "16/02/01 16:33:35 INFO mapreduce.Job: Job job_local403900502_0001 completed successfully\n",
      "16/02/01 16:33:35 INFO mapreduce.Job: Counters: 39\n",
      "\tFile System Counters\n",
      "\t\tFILE: Number of bytes read=223995298\n",
      "\t\tFILE: Number of bytes written=280510960\n",
      "\t\tFILE: Number of read operations=0\n",
      "\t\tFILE: Number of large read operations=0\n",
      "\t\tFILE: Number of write operations=0\n",
      "\t\tHDFS: Number of bytes read=6917034\n",
      "\t\tHDFS: Number of bytes written=29365\n",
      "\t\tHDFS: Number of read operations=15\n",
      "\t\tHDFS: Number of large read operations=0\n",
      "\t\tHDFS: Number of write operations=4\n",
      "\tMap-Reduce Framework\n",
      "\t\tMap input records=31101\n",
      "\t\tMap output records=2565115\n",
      "\t\tMap output bytes=50866886\n",
      "\t\tMap output materialized bytes=55997122\n",
      "\t\tInput split bytes=107\n",
      "\t\tCombine input records=0\n",
      "\t\tCombine output records=0\n",
      "\t\tReduce input groups=877096\n",
      "\t\tReduce shuffle bytes=55997122\n",
      "\t\tReduce input records=2565115\n",
      "\t\tReduce output records=1335\n",
      "\t\tSpilled Records=7695345\n",
      "\t\tShuffled Maps =1\n",
      "\t\tFailed Shuffles=0\n",
      "\t\tMerged Map outputs=1\n",
      "\t\tGC time elapsed (ms)=76\n",
      "\t\tTotal committed heap usage (bytes)=576192512\n",
      "\tMapper1Counter\n",
      "\t\tInstance=1\n",
      "\t\tPairs=2534014\n",
      "\tReducer1Counter\n",
      "\t\tInstances=1\n",
      "\t\tPairs=1335\n",
      "\tShuffle Errors\n",
      "\t\tBAD_ID=0\n",
      "\t\tCONNECTION=0\n",
      "\t\tIO_ERROR=0\n",
      "\t\tWRONG_LENGTH=0\n",
      "\t\tWRONG_MAP=0\n",
      "\t\tWRONG_REDUCE=0\n",
      "\tFile Input Format Counters \n",
      "\t\tBytes Read=3458517\n",
      "\tFile Output Format Counters \n",
      "\t\tBytes Written=29365\n",
      "16/02/01 16:33:35 INFO streaming.StreamJob: Output directory: /user/jing/test-output1/\n",
      "16/02/01 16:33:36 WARN streaming.StreamJob: -file option is deprecated, please use generic option -files instead.\n",
      "16/02/01 16:33:36 WARN util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "packageJobJar: [mapper2.py, reducer2.py] [] /var/folders/zs/k144hqks281fbt0x68c_zj9m0000gp/T/streamjob1133257175310895547.jar tmpDir=null\n",
      "16/02/01 16:33:36 INFO Configuration.deprecation: session.id is deprecated. Instead, use dfs.metrics.session-id\n",
      "16/02/01 16:33:36 INFO jvm.JvmMetrics: Initializing JVM Metrics with processName=JobTracker, sessionId=\n",
      "16/02/01 16:33:36 INFO jvm.JvmMetrics: Cannot initialize JVM Metrics with processName=JobTracker, sessionId= - already initialized\n",
      "16/02/01 16:33:37 INFO mapred.FileInputFormat: Total input paths to process : 1\n",
      "16/02/01 16:33:37 INFO mapreduce.JobSubmitter: number of splits:1\n",
      "16/02/01 16:33:37 INFO Configuration.deprecation: mapred.reduce.tasks is deprecated. Instead, use mapreduce.job.reduces\n",
      "16/02/01 16:33:37 INFO Configuration.deprecation: mapred.output.key.comparator.class is deprecated. Instead, use mapreduce.job.output.key.comparator.class\n",
      "16/02/01 16:33:37 INFO Configuration.deprecation: mapred.text.key.comparator.options is deprecated. Instead, use mapreduce.partition.keycomparator.options\n",
      "16/02/01 16:33:37 INFO Configuration.deprecation: mapred.job.name is deprecated. Instead, use mapreduce.job.name\n",
      "16/02/01 16:33:37 INFO Configuration.deprecation: mapred.map.tasks is deprecated. Instead, use mapreduce.job.maps\n",
      "16/02/01 16:33:37 INFO mapreduce.JobSubmitter: Submitting tokens for job: job_local1968726408_0001\n",
      "16/02/01 16:33:37 INFO mapred.LocalDistributedCacheManager: Localized file:/Users/JingXu/Dropbox/DataScience/W261/W261/HW3/mapper2.py as file:/tmp/hadoop-JingXu/mapred/local/1454373217590/mapper2.py\n",
      "16/02/01 16:33:37 INFO mapred.LocalDistributedCacheManager: Localized file:/Users/JingXu/Dropbox/DataScience/W261/W261/HW3/reducer2.py as file:/tmp/hadoop-JingXu/mapred/local/1454373217591/reducer2.py\n",
      "16/02/01 16:33:37 INFO mapreduce.Job: The url to track the job: http://localhost:8080/\n",
      "16/02/01 16:33:37 INFO mapred.LocalJobRunner: OutputCommitter set in config null\n",
      "16/02/01 16:33:37 INFO mapreduce.Job: Running job: job_local1968726408_0001\n",
      "16/02/01 16:33:37 INFO mapred.LocalJobRunner: OutputCommitter is org.apache.hadoop.mapred.FileOutputCommitter\n",
      "16/02/01 16:33:37 INFO mapred.LocalJobRunner: Waiting for map tasks\n",
      "16/02/01 16:33:37 INFO mapred.LocalJobRunner: Starting task: attempt_local1968726408_0001_m_000000_0\n",
      "16/02/01 16:33:37 INFO util.ProcfsBasedProcessTree: ProcfsBasedProcessTree currently is supported only on Linux.\n",
      "16/02/01 16:33:37 INFO mapred.Task:  Using ResourceCalculatorProcessTree : null\n",
      "16/02/01 16:33:37 INFO mapred.MapTask: Processing split: hdfs://localhost:9000/user/jing/test-output1/part-00000:0+29365\n",
      "16/02/01 16:33:37 INFO mapred.MapTask: numReduceTasks: 1\n",
      "16/02/01 16:33:37 INFO mapred.MapTask: (EQUATOR) 0 kvi 26214396(104857584)\n",
      "16/02/01 16:33:37 INFO mapred.MapTask: mapreduce.task.io.sort.mb: 100\n",
      "16/02/01 16:33:37 INFO mapred.MapTask: soft limit at 83886080\n",
      "16/02/01 16:33:37 INFO mapred.MapTask: bufstart = 0; bufvoid = 104857600\n",
      "16/02/01 16:33:37 INFO mapred.MapTask: kvstart = 26214396; length = 6553600\n",
      "16/02/01 16:33:37 INFO mapred.MapTask: Map output collector class = org.apache.hadoop.mapred.MapTask$MapOutputBuffer\n",
      "16/02/01 16:33:37 INFO streaming.PipeMapRed: PipeMapRed exec [/Users/JingXu/Dropbox/DataScience/W261/W261/HW3/./mapper2.py]\n",
      "16/02/01 16:33:37 INFO Configuration.deprecation: mapred.task.id is deprecated. Instead, use mapreduce.task.attempt.id\n",
      "16/02/01 16:33:37 INFO Configuration.deprecation: user.name is deprecated. Instead, use mapreduce.job.user.name\n",
      "16/02/01 16:33:37 INFO Configuration.deprecation: map.input.start is deprecated. Instead, use mapreduce.map.input.start\n",
      "16/02/01 16:33:37 INFO Configuration.deprecation: mapred.task.is.map is deprecated. Instead, use mapreduce.task.ismap\n",
      "16/02/01 16:33:37 INFO Configuration.deprecation: mapred.tip.id is deprecated. Instead, use mapreduce.task.id\n",
      "16/02/01 16:33:37 INFO Configuration.deprecation: mapred.skip.on is deprecated. Instead, use mapreduce.job.skiprecords\n",
      "16/02/01 16:33:37 INFO Configuration.deprecation: mapred.task.partition is deprecated. Instead, use mapreduce.task.partition\n",
      "16/02/01 16:33:37 INFO Configuration.deprecation: map.input.length is deprecated. Instead, use mapreduce.map.input.length\n",
      "16/02/01 16:33:37 INFO Configuration.deprecation: mapred.local.dir is deprecated. Instead, use mapreduce.cluster.local.dir\n",
      "16/02/01 16:33:37 INFO Configuration.deprecation: mapred.work.output.dir is deprecated. Instead, use mapreduce.task.output.dir\n",
      "16/02/01 16:33:37 INFO Configuration.deprecation: map.input.file is deprecated. Instead, use mapreduce.map.input.file\n",
      "16/02/01 16:33:37 INFO Configuration.deprecation: mapred.job.id is deprecated. Instead, use mapreduce.job.id\n",
      "16/02/01 16:33:38 INFO streaming.PipeMapRed: R/W/S=1/0/0 in:NA [rec/s] out:NA [rec/s]\n",
      "16/02/01 16:33:38 INFO streaming.PipeMapRed: R/W/S=10/0/0 in:NA [rec/s] out:NA [rec/s]\n",
      "16/02/01 16:33:38 INFO streaming.PipeMapRed: R/W/S=100/0/0 in:NA [rec/s] out:NA [rec/s]\n",
      "16/02/01 16:33:38 INFO streaming.PipeMapRed: R/W/S=1000/0/0 in:NA [rec/s] out:NA [rec/s]\n",
      "16/02/01 16:33:38 INFO streaming.PipeMapRed: Records R/W=1335/1\n",
      "16/02/01 16:33:38 INFO streaming.PipeMapRed: MRErrorThread done\n",
      "16/02/01 16:33:38 INFO streaming.PipeMapRed: mapRedFinished\n",
      "16/02/01 16:33:38 INFO mapred.LocalJobRunner: \n",
      "16/02/01 16:33:38 INFO mapred.MapTask: Starting flush of map output\n",
      "16/02/01 16:33:38 INFO mapred.MapTask: Spilling map output\n",
      "16/02/01 16:33:38 INFO mapred.MapTask: bufstart = 0; bufend = 30700; bufvoid = 104857600\n",
      "16/02/01 16:33:38 INFO mapred.MapTask: kvstart = 26214396(104857584); kvend = 26209060(104836240); length = 5337/6553600\n",
      "16/02/01 16:33:38 INFO mapred.MapTask: Finished spill 0\n",
      "16/02/01 16:33:38 INFO mapred.Task: Task:attempt_local1968726408_0001_m_000000_0 is done. And is in the process of committing\n",
      "16/02/01 16:33:38 INFO mapred.LocalJobRunner: Records R/W=1335/1\n",
      "16/02/01 16:33:38 INFO mapred.Task: Task 'attempt_local1968726408_0001_m_000000_0' done.\n",
      "16/02/01 16:33:38 INFO mapred.LocalJobRunner: Finishing task: attempt_local1968726408_0001_m_000000_0\n",
      "16/02/01 16:33:38 INFO mapred.LocalJobRunner: map task executor complete.\n",
      "16/02/01 16:33:38 INFO mapred.LocalJobRunner: Waiting for reduce tasks\n",
      "16/02/01 16:33:38 INFO mapred.LocalJobRunner: Starting task: attempt_local1968726408_0001_r_000000_0\n",
      "16/02/01 16:33:38 INFO util.ProcfsBasedProcessTree: ProcfsBasedProcessTree currently is supported only on Linux.\n",
      "16/02/01 16:33:38 INFO mapred.Task:  Using ResourceCalculatorProcessTree : null\n",
      "16/02/01 16:33:38 INFO mapred.ReduceTask: Using ShuffleConsumerPlugin: org.apache.hadoop.mapreduce.task.reduce.Shuffle@60d98f23\n",
      "16/02/01 16:33:38 INFO reduce.MergeManagerImpl: MergerManager: memoryLimit=333971456, maxSingleShuffleLimit=83492864, mergeThreshold=220421168, ioSortFactor=10, memToMemMergeOutputsThreshold=10\n",
      "16/02/01 16:33:38 INFO reduce.EventFetcher: attempt_local1968726408_0001_r_000000_0 Thread started: EventFetcher for fetching Map Completion Events\n",
      "16/02/01 16:33:38 INFO reduce.LocalFetcher: localfetcher#1 about to shuffle output of map attempt_local1968726408_0001_m_000000_0 decomp: 33372 len: 33376 to MEMORY\n",
      "16/02/01 16:33:38 INFO reduce.InMemoryMapOutput: Read 33372 bytes from map-output for attempt_local1968726408_0001_m_000000_0\n",
      "16/02/01 16:33:38 INFO reduce.MergeManagerImpl: closeInMemoryFile -> map-output of size: 33372, inMemoryMapOutputs.size() -> 1, commitMemory -> 0, usedMemory ->33372\n",
      "16/02/01 16:33:38 INFO reduce.EventFetcher: EventFetcher is interrupted.. Returning\n",
      "16/02/01 16:33:38 INFO mapred.LocalJobRunner: 1 / 1 copied.\n",
      "16/02/01 16:33:38 INFO reduce.MergeManagerImpl: finalMerge called with 1 in-memory map-outputs and 0 on-disk map-outputs\n",
      "16/02/01 16:33:38 INFO mapred.Merger: Merging 1 sorted segments\n",
      "16/02/01 16:33:38 INFO mapred.Merger: Down to the last merge-pass, with 1 segments left of total size: 33348 bytes\n",
      "16/02/01 16:33:38 INFO reduce.MergeManagerImpl: Merged 1 segments, 33372 bytes to disk to satisfy reduce memory limit\n",
      "16/02/01 16:33:38 INFO reduce.MergeManagerImpl: Merging 1 files, 33376 bytes from disk\n",
      "16/02/01 16:33:38 INFO reduce.MergeManagerImpl: Merging 0 segments, 0 bytes from memory into reduce\n",
      "16/02/01 16:33:38 INFO mapred.Merger: Merging 1 sorted segments\n",
      "16/02/01 16:33:38 INFO mapred.Merger: Down to the last merge-pass, with 1 segments left of total size: 33348 bytes\n",
      "16/02/01 16:33:38 INFO mapred.LocalJobRunner: 1 / 1 copied.\n",
      "16/02/01 16:33:38 INFO streaming.PipeMapRed: PipeMapRed exec [/Users/JingXu/Dropbox/DataScience/W261/W261/HW3/./reducer2.py]\n",
      "16/02/01 16:33:38 INFO Configuration.deprecation: mapred.job.tracker is deprecated. Instead, use mapreduce.jobtracker.address\n",
      "16/02/01 16:33:38 INFO Configuration.deprecation: mapred.map.tasks is deprecated. Instead, use mapreduce.job.maps\n",
      "16/02/01 16:33:38 INFO streaming.PipeMapRed: R/W/S=1/0/0 in:NA [rec/s] out:NA [rec/s]\n",
      "16/02/01 16:33:38 INFO streaming.PipeMapRed: R/W/S=10/0/0 in:NA [rec/s] out:NA [rec/s]\n",
      "16/02/01 16:33:38 INFO streaming.PipeMapRed: R/W/S=100/0/0 in:NA [rec/s] out:NA [rec/s]\n",
      "16/02/01 16:33:38 INFO streaming.PipeMapRed: R/W/S=1000/0/0 in:NA [rec/s] out:NA [rec/s]\n",
      "16/02/01 16:33:38 INFO streaming.PipeMapRed: MRErrorThread done\n",
      "16/02/01 16:33:38 INFO streaming.PipeMapRed: Records R/W=1335/1\n",
      "16/02/01 16:33:38 INFO streaming.PipeMapRed: mapRedFinished\n",
      "16/02/01 16:33:38 INFO mapred.Task: Task:attempt_local1968726408_0001_r_000000_0 is done. And is in the process of committing\n",
      "16/02/01 16:33:38 INFO mapred.LocalJobRunner: 1 / 1 copied.\n",
      "16/02/01 16:33:38 INFO mapred.Task: Task attempt_local1968726408_0001_r_000000_0 is allowed to commit now\n",
      "16/02/01 16:33:38 INFO output.FileOutputCommitter: Saved output of task 'attempt_local1968726408_0001_r_000000_0' to hdfs://localhost:9000/user/jing/test-output2/_temporary/0/task_local1968726408_0001_r_000000\n",
      "16/02/01 16:33:38 INFO mapred.LocalJobRunner: Records R/W=1335/1 > reduce\n",
      "16/02/01 16:33:38 INFO mapred.Task: Task 'attempt_local1968726408_0001_r_000000_0' done.\n",
      "16/02/01 16:33:38 INFO mapred.LocalJobRunner: Finishing task: attempt_local1968726408_0001_r_000000_0\n",
      "16/02/01 16:33:38 INFO mapred.LocalJobRunner: reduce task executor complete.\n",
      "16/02/01 16:33:38 INFO mapreduce.Job: Job job_local1968726408_0001 running in uber mode : false\n",
      "16/02/01 16:33:38 INFO mapreduce.Job:  map 100% reduce 100%\n",
      "16/02/01 16:33:38 INFO mapreduce.Job: Job job_local1968726408_0001 completed successfully\n",
      "16/02/01 16:33:38 INFO mapreduce.Job: Counters: 35\n",
      "\tFile System Counters\n",
      "\t\tFILE: Number of bytes read=72282\n",
      "\t\tFILE: Number of bytes written=629894\n",
      "\t\tFILE: Number of read operations=0\n",
      "\t\tFILE: Number of large read operations=0\n",
      "\t\tFILE: Number of write operations=0\n",
      "\t\tHDFS: Number of bytes read=58730\n",
      "\t\tHDFS: Number of bytes written=2660\n",
      "\t\tHDFS: Number of read operations=13\n",
      "\t\tHDFS: Number of large read operations=0\n",
      "\t\tHDFS: Number of write operations=4\n",
      "\tMap-Reduce Framework\n",
      "\t\tMap input records=1335\n",
      "\t\tMap output records=1335\n",
      "\t\tMap output bytes=30700\n",
      "\t\tMap output materialized bytes=33376\n",
      "\t\tInput split bytes=107\n",
      "\t\tCombine input records=0\n",
      "\t\tCombine output records=0\n",
      "\t\tReduce input groups=1335\n",
      "\t\tReduce shuffle bytes=33376\n",
      "\t\tReduce input records=1335\n",
      "\t\tReduce output records=52\n",
      "\t\tSpilled Records=2670\n",
      "\t\tShuffled Maps =1\n",
      "\t\tFailed Shuffles=0\n",
      "\t\tMerged Map outputs=1\n",
      "\t\tGC time elapsed (ms)=10\n",
      "\t\tTotal committed heap usage (bytes)=582483968\n",
      "\tShuffle Errors\n",
      "\t\tBAD_ID=0\n",
      "\t\tCONNECTION=0\n",
      "\t\tIO_ERROR=0\n",
      "\t\tWRONG_LENGTH=0\n",
      "\t\tWRONG_MAP=0\n",
      "\t\tWRONG_REDUCE=0\n",
      "\tFile Input Format Counters \n",
      "\t\tBytes Read=29365\n",
      "\tFile Output Format Counters \n",
      "\t\tBytes Written=2660\n",
      "16/02/01 16:33:38 INFO streaming.StreamJob: Output directory: /user/jing/test-output2/\n",
      "It takes 42 seconds to complete this task\n",
      "DEPRECATED: Use of this script to execute hdfs command is deprecated.\n",
      "Instead use the hdfs command for it.\n",
      "\n",
      "16/02/01 16:33:39 WARN util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "\t\n",
      "Top 50 product pairs:\t\n",
      "['DAI62779', 'ELE17451', 1592, 0.05118806469245362]\t\n",
      "['FRO40251', 'SNA80324', 1412, 0.04540046943828173]\t\n",
      "['DAI75645', 'FRO40251', 1254, 0.04032024693739751]\t\n",
      "['FRO40251', 'GRO85051', 1213, 0.039001961351725026]\t\n",
      "['DAI62779', 'GRO73461', 1139, 0.03662261663612103]\t\n",
      "['DAI75645', 'SNA80324', 1130, 0.03633323687341243]\t\n",
      "['DAI62779', 'FRO40251', 1070, 0.03440403845535513]\t\n",
      "['DAI62779', 'SNA80324', 923, 0.029677502331114755]\t\n",
      "['DAI62779', 'DAI85309', 918, 0.029516735796276648]\t\n",
      "['ELE32164', 'GRO59710', 911, 0.029291662647503297]\t\n",
      "['FRO40251', 'GRO73461', 882, 0.02835921674544227]\t\n",
      "['DAI62779', 'DAI75645', 882, 0.02835921674544227]\t\n",
      "['DAI62779', 'ELE92920', 877, 0.02819845021060416]\t\n",
      "['FRO40251', 'FRO92469', 835, 0.026848011317964052]\t\n",
      "['DAI62779', 'ELE32164', 832, 0.026751551397061188]\t\n",
      "['DAI75645', 'GRO73461', 712, 0.022893154560946594]\t\n",
      "['DAI43223', 'ELE32164', 711, 0.022861001253978972]\t\n",
      "['DAI62779', 'GRO30386', 709, 0.02279669464004373]\t\n",
      "['ELE17451', 'FRO40251', 697, 0.022410854956432268]\t\n",
      "['DAI85309', 'ELE99737', 659, 0.021189029291662647]\t\n",
      "['DAI62779', 'ELE26917', 650, 0.020899649528954053]\t\n",
      "['GRO21487', 'GRO73461', 631, 0.02028873669656924]\t\n",
      "['DAI62779', 'SNA45677', 604, 0.019420597408443457]\t\n",
      "['ELE17451', 'SNA80324', 597, 0.019195524259670107]\t\n",
      "['DAI62779', 'GRO71621', 595, 0.019131217645734864]\t\n",
      "['DAI62779', 'SNA55762', 593, 0.01906691103179962]\t\n",
      "['DAI62779', 'DAI83733', 586, 0.01884183788302627]\t\n",
      "['ELE17451', 'GRO73461', 580, 0.018648918041220538]\t\n",
      "['GRO73461', 'SNA80324', 562, 0.01807015851580335]\t\n",
      "['DAI62779', 'GRO59710', 561, 0.01803800520883573]\t\n",
      "['DAI62779', 'FRO80039', 550, 0.01768431883219189]\t\n",
      "['DAI75645', 'ELE17451', 547, 0.017587858911289025]\t\n",
      "['DAI62779', 'SNA93860', 537, 0.01726632584161281]\t\n",
      "['DAI55148', 'DAI62779', 526, 0.016912639464968973]\t\n",
      "['DAI43223', 'GRO59710', 512, 0.01646249316742227]\t\n",
      "['ELE17451', 'ELE32164', 511, 0.016430339860454647]\t\n",
      "['DAI62779', 'SNA18336', 506, 0.01626957332561654]\t\n",
      "['ELE32164', 'GRO73461', 486, 0.015626507186264106]\t\n",
      "['DAI85309', 'ELE17451', 482, 0.01549789395839362]\t\n",
      "['DAI62779', 'FRO78087', 482, 0.01549789395839362]\t\n",
      "['DAI62779', 'GRO94758', 479, 0.015401434037490756]\t\n",
      "['GRO85051', 'SNA80324', 471, 0.015144207581749784]\t\n",
      "['DAI62779', 'GRO21487', 471, 0.015144207581749784]\t\n",
      "['ELE17451', 'GRO30386', 468, 0.015047747660846917]\t\n",
      "['FRO85978', 'SNA95666', 463, 0.01488698112600881]\t\n",
      "['DAI62779', 'FRO19221', 462, 0.014854827819041188]\t\n",
      "['DAI62779', 'GRO46854', 461, 0.014822674512073567]\t\n",
      "['DAI43223', 'DAI62779', 459, 0.014758367898138324]\t\n",
      "['ELE92920', 'SNA18336', 455, 0.014629754670267838]\t\n",
      "['DAI88079', 'FRO40251', 446, 0.014340374907559243]\t\n",
      "rmr: DEPRECATED: Please use 'rm -r' instead.\n",
      "16/02/01 16:33:41 WARN util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "16/02/01 16:33:42 INFO fs.TrashPolicyDefault: Namenode trash configuration: Deletion interval = 0 minutes, Emptier interval = 0 minutes.\n",
      "Deleted /user/jing\n",
      "rmr: DEPRECATED: Please use 'rm -r' instead.\n",
      "16/02/01 16:33:43 WARN util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "16/02/01 16:33:43 INFO fs.TrashPolicyDefault: Namenode trash configuration: Deletion interval = 0 minutes, Emptier interval = 0 minutes.\n",
      "Deleted /user\n"
     ]
    }
   ],
   "source": [
    "!./mapreduce.sh"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Single computer dual core, 1 mapper, 1 reducer**\n",
    "\n",
    "**It takes 42 seconds to complete this task**\n",
    "\n",
    "**Mapper1Counter**\n",
    "\n",
    "    Instance=1\n",
    "\tPairs=2534014\n",
    "\n",
    "**Reducer1Counter**\n",
    "\n",
    "\tInstances=1\n",
    "\tPairs=1335\n",
    "\n",
    "**Top 50 product pairs:**\n",
    "\n",
    "['DAI62779', 'ELE17451', 1592, 0.05118806469245362]\t\n",
    "['FRO40251', 'SNA80324', 1412, 0.04540046943828173]\t\n",
    "['DAI75645', 'FRO40251', 1254, 0.04032024693739751]\t\n",
    "['FRO40251', 'GRO85051', 1213, 0.039001961351725026]\t\n",
    "['DAI62779', 'GRO73461', 1139, 0.03662261663612103]\t\n",
    "['DAI75645', 'SNA80324', 1130, 0.03633323687341243]\t\n",
    "['DAI62779', 'FRO40251', 1070, 0.03440403845535513]\t\n",
    "['DAI62779', 'SNA80324', 923, 0.029677502331114755]\t\n",
    "['DAI62779', 'DAI85309', 918, 0.029516735796276648]\t\n",
    "['ELE32164', 'GRO59710', 911, 0.029291662647503297]\t\n",
    "['FRO40251', 'GRO73461', 882, 0.02835921674544227]\t\n",
    "['DAI62779', 'DAI75645', 882, 0.02835921674544227]\t\n",
    "['DAI62779', 'ELE92920', 877, 0.02819845021060416]\t\n",
    "['FRO40251', 'FRO92469', 835, 0.026848011317964052]\t\n",
    "['DAI62779', 'ELE32164', 832, 0.026751551397061188]\t\n",
    "['DAI75645', 'GRO73461', 712, 0.022893154560946594]\t\n",
    "['DAI43223', 'ELE32164', 711, 0.022861001253978972]\t\n",
    "['DAI62779', 'GRO30386', 709, 0.02279669464004373]\t\n",
    "['ELE17451', 'FRO40251', 697, 0.022410854956432268]\t\n",
    "['DAI85309', 'ELE99737', 659, 0.021189029291662647]\t\n",
    "['DAI62779', 'ELE26917', 650, 0.020899649528954053]\t\n",
    "['GRO21487', 'GRO73461', 631, 0.02028873669656924]\t\n",
    "['DAI62779', 'SNA45677', 604, 0.019420597408443457]\t\n",
    "['ELE17451', 'SNA80324', 597, 0.019195524259670107]\t\n",
    "['DAI62779', 'GRO71621', 595, 0.019131217645734864]\t\n",
    "['DAI62779', 'SNA55762', 593, 0.01906691103179962]\t\n",
    "['DAI62779', 'DAI83733', 586, 0.01884183788302627]\t\n",
    "['ELE17451', 'GRO73461', 580, 0.018648918041220538]\t\n",
    "['GRO73461', 'SNA80324', 562, 0.01807015851580335]\t\n",
    "['DAI62779', 'GRO59710', 561, 0.01803800520883573]\t\n",
    "['DAI62779', 'FRO80039', 550, 0.01768431883219189]\t\n",
    "['DAI75645', 'ELE17451', 547, 0.017587858911289025]\t\n",
    "['DAI62779', 'SNA93860', 537, 0.01726632584161281]\t\n",
    "['DAI55148', 'DAI62779', 526, 0.016912639464968973]\t\n",
    "['DAI43223', 'GRO59710', 512, 0.01646249316742227]\t\n",
    "['ELE17451', 'ELE32164', 511, 0.016430339860454647]\t\n",
    "['DAI62779', 'SNA18336', 506, 0.01626957332561654]\t\n",
    "['ELE32164', 'GRO73461', 486, 0.015626507186264106]\t\n",
    "['DAI85309', 'ELE17451', 482, 0.01549789395839362]\t\n",
    "['DAI62779', 'FRO78087', 482, 0.01549789395839362]\t\n",
    "['DAI62779', 'GRO94758', 479, 0.015401434037490756]\t\n",
    "['GRO85051', 'SNA80324', 471, 0.015144207581749784]\t\n",
    "['DAI62779', 'GRO21487', 471, 0.015144207581749784]\t\n",
    "['ELE17451', 'GRO30386', 468, 0.015047747660846917]\t\n",
    "['FRO85978', 'SNA95666', 463, 0.01488698112600881]\t\n",
    "['DAI62779', 'FRO19221', 462, 0.014854827819041188]\t\n",
    "['DAI62779', 'GRO46854', 461, 0.014822674512073567]\t\n",
    "['DAI43223', 'DAI62779', 459, 0.014758367898138324]\t\n",
    "['ELE92920', 'SNA18336', 455, 0.014629754670267838]\t\n",
    "['DAI88079', 'FRO40251', 446, 0.014340374907559243]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**HW3.5: Stripes\n",
    "Repeat 3.4 using the stripes design pattern for finding cooccuring pairs.**\n",
    "\n",
    "**Report the compute times for stripes job versus the Pairs job. Describe the computational setup used (E.g., single computer; dual core; linux, number of mappers, number of reducers)**\n",
    "\n",
    "**Instrument your mapper, combiner, and reducer to count how many times each is called using Counters and report these counts. Discuss the differences in these counts between the Pairs and Stripes jobs.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting mapper.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile mapper.py\n",
    "#!/usr/bin/python\n",
    "## mapper.py\n",
    "## Author: Jing Xu\n",
    "## Description: mapper code for HW3.5\n",
    "\n",
    "import sys\n",
    "\n",
    "sys.stderr.write(\"reporter:counter:Mapper1Counter,Instance,1\\n\")\n",
    "\n",
    "# read input from STDIN (standard input)\n",
    "for line in sys.stdin:\n",
    "    # split the line into words\n",
    "    items = line.split()\n",
    "    items = set(items) #create unique set of items\n",
    "    items = list(items) #convert to list\n",
    "    items = sorted(items, key=str.lower) #sort list of unique items\n",
    "    while len(items) != 1:\n",
    "        first = items[0] #first item in unique list is key\n",
    "        items = items[1:] #remove first item from unique list \n",
    "        string = ''\n",
    "        for item in items: #replicate dictionary structure\n",
    "            string+=\"'%s': 1, \"%item        \n",
    "        print '%s\\t{%s}' % (first, string)\n",
    "        sys.stderr.write(\"reporter:counter:Mapper1Counter,Stripes,1\\n\")\n",
    "    print \"*\\t{'*': 1}\" #counter for total baskets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting mapper2.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile mapper2.py\n",
    "#!/usr/bin/python\n",
    "## mapper2.py\n",
    "## Author: Jing Xu\n",
    "## Description: mapper2 code for HW3.5\n",
    "\n",
    "import sys\n",
    "import ast\n",
    "\n",
    "# read input from STDIN (standard input)\n",
    "for line in sys.stdin:\n",
    "    # split the line into words\n",
    "    line = line.strip('\\n')\n",
    "    key, stripe = line.split('\\t', 1)\n",
    "    dic = ast.literal_eval(stripe)\n",
    "    elements = dic.items()\n",
    "    for element in elements:\n",
    "        pair = '%s,%s' % (key, element[0])\n",
    "        count = int(element[1])\n",
    "        print '%s\\t%d' % (pair, count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting reducer.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile reducer.py\n",
    "#!/usr/bin/python\n",
    "## combiner.py\n",
    "## Author: Jing Xu\n",
    "## Description: combiner code for HW3.5\n",
    "\n",
    "import sys\n",
    "import ast\n",
    "\n",
    "current_key = None\n",
    "key_dictionary = {}\n",
    "\n",
    "# input comes from STDIN\n",
    "for line in sys.stdin:\n",
    "    # parse the input we got from mapper.py\n",
    "    key, stripe = line.split('\\t', 1)\n",
    "    dic = ast.literal_eval(stripe)\n",
    "    if current_key == key:\n",
    "        #merge and add dictionary values together\n",
    "        key_dictionary = { k: key_dictionary.get(k, 0) + dic.get(k, 0) for k in set(key_dictionary) | set(dic) } \n",
    "    else:\n",
    "        for k in key_dictionary.keys(): #delete all dictionary keys with values < 100\n",
    "            if key_dictionary[k] < 100:\n",
    "                del key_dictionary[k]\n",
    "        if current_key: #filter out all pairs that have cooccurence < 100\n",
    "            # write result to STDOUT\n",
    "            print '%s\\t%s' % (current_key, key_dictionary)\n",
    "            sys.stderr.write(\"reporter:counter:Reducer1Counter,Pairs,1\\n\")\n",
    "        key_dictionary = dic\n",
    "        current_key = key\n",
    "    \n",
    "# do not forget to output the last word if needed!\n",
    "if current_key == key:\n",
    "    for k in key_dictionary.keys(): #delete all dictionary keys with values < 100\n",
    "            if key_dictionary[k] < 100:\n",
    "                del key_dictionary[k]\n",
    "    print '%s\\t%s' % (current_key, key_dictionary)\n",
    "    sys.stderr.write(\"reporter:counter:Reducer1Counter,Pairs,1\\n\")\n",
    "\n",
    "sys.stderr.write(\"reporter:counter:Reducer1Counter,Instances,1\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting reducer2.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile reducer2.py\n",
    "#!/usr/bin/python\n",
    "## reducer2.py\n",
    "## Author: Jing Xu\n",
    "## Description: reducer2 code for HW3.5\n",
    "\n",
    "import sys, Queue\n",
    "\n",
    "n_max = 50 #want top 50 largest numbers and 10 smallest numbers\n",
    "q_max = Queue.Queue(n_max) #queue for largest values\n",
    "total_buckets = 0 #total words used to calculate relative frequency\n",
    "\n",
    "for line in sys.stdin:\n",
    "    line = line.strip('\\n') #clean line\n",
    "    rec = line.split('\\t', 1) #split line\n",
    "    if rec[0] == '*,*': #if key is *,*, indicates it is the total buckets value\n",
    "        rec[1] = rec[1].strip('\\t')\n",
    "        total_buckets = int(rec[1]) #assign count as total_words\n",
    "    else:\n",
    "        first, second = rec[0].split(',')\n",
    "        rec[1] = rec[1].strip('\\t')\n",
    "        rec[1] = int(rec[1])\n",
    "        # whatever left is the biggest\n",
    "        if q_max.full():\n",
    "            q_max.get()\n",
    "        q_max.put([first, second, rec[1]])\n",
    "\n",
    "top_list = []\n",
    "        \n",
    "for i in range(n_max):\n",
    "    value = q_max.get() \n",
    "    freq = float(value[2])/float(total_buckets) #calculate relative frequency\n",
    "    value.append(freq) #append relative frequency to output \n",
    "    top_list.append(value)\n",
    "\n",
    "top_list.reverse() #reverse list so highest value is first\n",
    "    \n",
    "print '\\nTop %d product pairs:' %n_max\n",
    "for value in top_list:\n",
    "    print value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting mapreduce.sh\n"
     ]
    }
   ],
   "source": [
    "%%writefile mapreduce.sh\n",
    "## mapreduce.sh\n",
    "## Author: Jing Xu\n",
    "## Description: mapreduce bash script for HW3.5\n",
    "\n",
    "hdfs dfs -mkdir /user # create hdfs folder\n",
    "wait\n",
    "hdfs dfs -mkdir /user/jing # create hdfs folder\n",
    "wait\n",
    "hadoop fs -put ProductPurchaseData.txt /user/jing # upload local file to hdfs folder\n",
    "wait\n",
    "\n",
    "STARTTIME=$(date +%s)\n",
    "\n",
    "# hadoop command to run streaming mapreduce job\n",
    "hadoop jar /Users/JingXu/Documents/hadoop-2.6.3/share/hadoop/tools/lib/hadoop-streaming-2.6.3.jar \\\n",
    "-D mapred.job.name=\"Count Job via Streaming1\" \\\n",
    "-D mapred.map.tasks=1 \\\n",
    "-D mapred.reduce.tasks=1 \\\n",
    "-file mapper.py    -mapper mapper.py \\\n",
    "-file reducer.py   -reducer reducer.py \\\n",
    "-input /user/jing/* -output /user/jing/test-output1/\n",
    "\n",
    "wait\n",
    "\n",
    "# second hadoop command to run streaming mapreduce job\n",
    "# sort primarly by numeric descending in 2nd key, then alphabetically on 1st key\n",
    "hadoop jar /Users/JingXu/Documents/hadoop-2.6.3/share/hadoop/tools/lib/hadoop-streaming-2.6.3.jar \\\n",
    "-D mapred.job.name=\"Count Job via Streaming2\" \\\n",
    "-D mapred.map.tasks=1 \\\n",
    "-D mapred.reduce.tasks=1 \\\n",
    "-D stream.num.map.output.key.fields=2 \\\n",
    "-D mapred.output.key.comparator.class=org.apache.hadoop.mapred.lib.KeyFieldBasedComparator \\\n",
    "-D mapred.text.key.comparator.options=\"-k2n -k1\" \\\n",
    "-file mapper2.py    -mapper mapper2.py \\\n",
    "-file reducer2.py   -reducer reducer2.py \\\n",
    "-input /user/jing/test-output1/part-00000 -output /user/jing/test-output2/\n",
    "\n",
    "wait\n",
    "\n",
    "#calculate compute time for Pairs job\n",
    "ENDTIME=$(date +%s)\n",
    "echo \"It takes $(($ENDTIME - $STARTTIME)) seconds to complete this task\"\n",
    "wait\n",
    "\n",
    "hadoop dfs -cat /user/jing/test-output2/part-00000 #examine hdfs file output\n",
    "wait\n",
    "\n",
    "hdfs dfs -rmr /user/jing # remove hdfs folder\n",
    "hdfs dfs -rmr /user # remove hdfs folder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "16/02/01 16:34:05 WARN util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "16/02/01 16:34:07 WARN util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "16/02/01 16:34:08 WARN util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "16/02/01 16:34:10 WARN streaming.StreamJob: -file option is deprecated, please use generic option -files instead.\n",
      "16/02/01 16:34:10 WARN util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "packageJobJar: [mapper.py, reducer.py] [] /var/folders/zs/k144hqks281fbt0x68c_zj9m0000gp/T/streamjob5610622673548507847.jar tmpDir=null\n",
      "16/02/01 16:34:11 INFO Configuration.deprecation: session.id is deprecated. Instead, use dfs.metrics.session-id\n",
      "16/02/01 16:34:11 INFO jvm.JvmMetrics: Initializing JVM Metrics with processName=JobTracker, sessionId=\n",
      "16/02/01 16:34:11 INFO jvm.JvmMetrics: Cannot initialize JVM Metrics with processName=JobTracker, sessionId= - already initialized\n",
      "16/02/01 16:34:11 INFO mapred.FileInputFormat: Total input paths to process : 1\n",
      "16/02/01 16:34:11 INFO mapreduce.JobSubmitter: number of splits:1\n",
      "16/02/01 16:34:11 INFO Configuration.deprecation: mapred.reduce.tasks is deprecated. Instead, use mapreduce.job.reduces\n",
      "16/02/01 16:34:11 INFO Configuration.deprecation: mapred.job.name is deprecated. Instead, use mapreduce.job.name\n",
      "16/02/01 16:34:11 INFO Configuration.deprecation: mapred.map.tasks is deprecated. Instead, use mapreduce.job.maps\n",
      "16/02/01 16:34:11 INFO mapreduce.JobSubmitter: Submitting tokens for job: job_local1016413727_0001\n",
      "16/02/01 16:34:11 INFO mapred.LocalDistributedCacheManager: Localized file:/Users/JingXu/Dropbox/DataScience/W261/W261/HW3/mapper.py as file:/tmp/hadoop-JingXu/mapred/local/1454373251852/mapper.py\n",
      "16/02/01 16:34:11 INFO mapred.LocalDistributedCacheManager: Localized file:/Users/JingXu/Dropbox/DataScience/W261/W261/HW3/reducer.py as file:/tmp/hadoop-JingXu/mapred/local/1454373251853/reducer.py\n",
      "16/02/01 16:34:12 INFO mapreduce.Job: The url to track the job: http://localhost:8080/\n",
      "16/02/01 16:34:12 INFO mapred.LocalJobRunner: OutputCommitter set in config null\n",
      "16/02/01 16:34:12 INFO mapreduce.Job: Running job: job_local1016413727_0001\n",
      "16/02/01 16:34:12 INFO mapred.LocalJobRunner: OutputCommitter is org.apache.hadoop.mapred.FileOutputCommitter\n",
      "16/02/01 16:34:12 INFO mapred.LocalJobRunner: Waiting for map tasks\n",
      "16/02/01 16:34:12 INFO mapred.LocalJobRunner: Starting task: attempt_local1016413727_0001_m_000000_0\n",
      "16/02/01 16:34:12 INFO util.ProcfsBasedProcessTree: ProcfsBasedProcessTree currently is supported only on Linux.\n",
      "16/02/01 16:34:12 INFO mapred.Task:  Using ResourceCalculatorProcessTree : null\n",
      "16/02/01 16:34:12 INFO mapred.MapTask: Processing split: hdfs://localhost:9000/user/jing/ProductPurchaseData.txt:0+3458517\n",
      "16/02/01 16:34:12 INFO mapred.MapTask: numReduceTasks: 1\n",
      "16/02/01 16:34:12 INFO mapred.MapTask: (EQUATOR) 0 kvi 26214396(104857584)\n",
      "16/02/01 16:34:12 INFO mapred.MapTask: mapreduce.task.io.sort.mb: 100\n",
      "16/02/01 16:34:12 INFO mapred.MapTask: soft limit at 83886080\n",
      "16/02/01 16:34:12 INFO mapred.MapTask: bufstart = 0; bufvoid = 104857600\n",
      "16/02/01 16:34:12 INFO mapred.MapTask: kvstart = 26214396; length = 6553600\n",
      "16/02/01 16:34:12 INFO mapred.MapTask: Map output collector class = org.apache.hadoop.mapred.MapTask$MapOutputBuffer\n",
      "16/02/01 16:34:12 INFO streaming.PipeMapRed: PipeMapRed exec [/Users/JingXu/Dropbox/DataScience/W261/W261/HW3/./mapper.py]\n",
      "16/02/01 16:34:12 INFO Configuration.deprecation: mapred.task.id is deprecated. Instead, use mapreduce.task.attempt.id\n",
      "16/02/01 16:34:12 INFO Configuration.deprecation: user.name is deprecated. Instead, use mapreduce.job.user.name\n",
      "16/02/01 16:34:12 INFO Configuration.deprecation: map.input.start is deprecated. Instead, use mapreduce.map.input.start\n",
      "16/02/01 16:34:12 INFO Configuration.deprecation: mapred.task.is.map is deprecated. Instead, use mapreduce.task.ismap\n",
      "16/02/01 16:34:12 INFO Configuration.deprecation: mapred.tip.id is deprecated. Instead, use mapreduce.task.id\n",
      "16/02/01 16:34:12 INFO Configuration.deprecation: mapred.skip.on is deprecated. Instead, use mapreduce.job.skiprecords\n",
      "16/02/01 16:34:12 INFO Configuration.deprecation: mapred.task.partition is deprecated. Instead, use mapreduce.task.partition\n",
      "16/02/01 16:34:12 INFO Configuration.deprecation: map.input.length is deprecated. Instead, use mapreduce.map.input.length\n",
      "16/02/01 16:34:12 INFO Configuration.deprecation: mapred.local.dir is deprecated. Instead, use mapreduce.cluster.local.dir\n",
      "16/02/01 16:34:12 INFO Configuration.deprecation: mapred.work.output.dir is deprecated. Instead, use mapreduce.task.output.dir\n",
      "16/02/01 16:34:12 INFO Configuration.deprecation: map.input.file is deprecated. Instead, use mapreduce.map.input.file\n",
      "16/02/01 16:34:12 INFO Configuration.deprecation: mapred.job.id is deprecated. Instead, use mapreduce.job.id\n",
      "16/02/01 16:34:12 INFO streaming.PipeMapRed: R/W/S=1/0/0 in:NA [rec/s] out:NA [rec/s]\n",
      "16/02/01 16:34:12 INFO streaming.PipeMapRed: R/W/S=10/0/0 in:NA [rec/s] out:NA [rec/s]\n",
      "16/02/01 16:34:12 INFO streaming.PipeMapRed: R/W/S=100/0/0 in:NA [rec/s] out:NA [rec/s]\n",
      "16/02/01 16:34:12 INFO streaming.PipeMapRed: R/W/S=1000/0/0 in:NA [rec/s] out:NA [rec/s]\n",
      "16/02/01 16:34:12 INFO streaming.PipeMapRed: Records R/W=1216/1\n",
      "16/02/01 16:34:13 INFO mapreduce.Job: Job job_local1016413727_0001 running in uber mode : false\n",
      "16/02/01 16:34:13 INFO mapreduce.Job:  map 0% reduce 0%\n",
      "16/02/01 16:34:13 INFO streaming.PipeMapRed: R/W/S=10000/120415/0 in:10000=10000/1 [rec/s] out:120415=120415/1 [rec/s]\n",
      "16/02/01 16:34:15 INFO streaming.PipeMapRed: MRErrorThread done\n",
      "16/02/01 16:34:15 INFO streaming.PipeMapRed: mapRedFinished\n",
      "16/02/01 16:34:15 INFO mapred.LocalJobRunner: \n",
      "16/02/01 16:34:15 INFO mapred.MapTask: Starting flush of map output\n",
      "16/02/01 16:34:15 INFO mapred.MapTask: Spilling map output\n",
      "16/02/01 16:34:15 INFO mapred.MapTask: bufstart = 0; bufend = 42687930; bufvoid = 104857600\n",
      "16/02/01 16:34:15 INFO mapred.MapTask: kvstart = 26214396(104857584); kvend = 24691116(98764464); length = 1523281/6553600\n",
      "16/02/01 16:34:15 INFO mapred.MapTask: Finished spill 0\n",
      "16/02/01 16:34:15 INFO mapred.Task: Task:attempt_local1016413727_0001_m_000000_0 is done. And is in the process of committing\n",
      "16/02/01 16:34:15 INFO mapred.LocalJobRunner: Records R/W=1216/1\n",
      "16/02/01 16:34:15 INFO mapred.Task: Task 'attempt_local1016413727_0001_m_000000_0' done.\n",
      "16/02/01 16:34:15 INFO mapred.LocalJobRunner: Finishing task: attempt_local1016413727_0001_m_000000_0\n",
      "16/02/01 16:34:15 INFO mapred.LocalJobRunner: map task executor complete.\n",
      "16/02/01 16:34:15 INFO mapred.LocalJobRunner: Waiting for reduce tasks\n",
      "16/02/01 16:34:15 INFO mapred.LocalJobRunner: Starting task: attempt_local1016413727_0001_r_000000_0\n",
      "16/02/01 16:34:15 INFO util.ProcfsBasedProcessTree: ProcfsBasedProcessTree currently is supported only on Linux.\n",
      "16/02/01 16:34:15 INFO mapred.Task:  Using ResourceCalculatorProcessTree : null\n",
      "16/02/01 16:34:15 INFO mapred.ReduceTask: Using ShuffleConsumerPlugin: org.apache.hadoop.mapreduce.task.reduce.Shuffle@4c4b126\n",
      "16/02/01 16:34:15 INFO reduce.MergeManagerImpl: MergerManager: memoryLimit=333971456, maxSingleShuffleLimit=83492864, mergeThreshold=220421168, ioSortFactor=10, memToMemMergeOutputsThreshold=10\n",
      "16/02/01 16:34:15 INFO reduce.EventFetcher: attempt_local1016413727_0001_r_000000_0 Thread started: EventFetcher for fetching Map Completion Events\n",
      "16/02/01 16:34:15 INFO reduce.LocalFetcher: localfetcher#1 about to shuffle output of map attempt_local1016413727_0001_m_000000_0 decomp: 43588543 len: 43588547 to MEMORY\n",
      "16/02/01 16:34:15 INFO reduce.InMemoryMapOutput: Read 43588543 bytes from map-output for attempt_local1016413727_0001_m_000000_0\n",
      "16/02/01 16:34:15 INFO reduce.MergeManagerImpl: closeInMemoryFile -> map-output of size: 43588543, inMemoryMapOutputs.size() -> 1, commitMemory -> 0, usedMemory ->43588543\n",
      "16/02/01 16:34:15 INFO reduce.EventFetcher: EventFetcher is interrupted.. Returning\n",
      "16/02/01 16:34:15 INFO mapred.LocalJobRunner: 1 / 1 copied.\n",
      "16/02/01 16:34:15 INFO reduce.MergeManagerImpl: finalMerge called with 1 in-memory map-outputs and 0 on-disk map-outputs\n",
      "16/02/01 16:34:15 INFO mapred.Merger: Merging 1 sorted segments\n",
      "16/02/01 16:34:15 INFO mapred.Merger: Down to the last merge-pass, with 1 segments left of total size: 43588539 bytes\n",
      "16/02/01 16:34:16 INFO mapreduce.Job:  map 100% reduce 0%\n",
      "16/02/01 16:34:16 INFO reduce.MergeManagerImpl: Merged 1 segments, 43588543 bytes to disk to satisfy reduce memory limit\n",
      "16/02/01 16:34:16 INFO reduce.MergeManagerImpl: Merging 1 files, 43588547 bytes from disk\n",
      "16/02/01 16:34:16 INFO reduce.MergeManagerImpl: Merging 0 segments, 0 bytes from memory into reduce\n",
      "16/02/01 16:34:16 INFO mapred.Merger: Merging 1 sorted segments\n",
      "16/02/01 16:34:16 INFO mapred.Merger: Down to the last merge-pass, with 1 segments left of total size: 43588539 bytes\n",
      "16/02/01 16:34:16 INFO mapred.LocalJobRunner: 1 / 1 copied.\n",
      "16/02/01 16:34:16 INFO streaming.PipeMapRed: PipeMapRed exec [/Users/JingXu/Dropbox/DataScience/W261/W261/HW3/./reducer.py]\n",
      "16/02/01 16:34:16 INFO Configuration.deprecation: mapred.job.tracker is deprecated. Instead, use mapreduce.jobtracker.address\n",
      "16/02/01 16:34:16 INFO Configuration.deprecation: mapred.map.tasks is deprecated. Instead, use mapreduce.job.maps\n",
      "16/02/01 16:34:16 INFO streaming.PipeMapRed: R/W/S=1/0/0 in:NA [rec/s] out:NA [rec/s]\n",
      "16/02/01 16:34:16 INFO streaming.PipeMapRed: R/W/S=10/0/0 in:NA [rec/s] out:NA [rec/s]\n",
      "16/02/01 16:34:16 INFO streaming.PipeMapRed: R/W/S=100/0/0 in:NA [rec/s] out:NA [rec/s]\n",
      "16/02/01 16:34:16 INFO streaming.PipeMapRed: R/W/S=1000/0/0 in:NA [rec/s] out:NA [rec/s]\n",
      "16/02/01 16:34:16 INFO streaming.PipeMapRed: R/W/S=10000/0/0 in:NA [rec/s] out:NA [rec/s]\n",
      "16/02/01 16:34:21 INFO mapred.LocalJobRunner: reduce > reduce\n",
      "16/02/01 16:34:22 INFO mapreduce.Job:  map 100% reduce 69%\n",
      "16/02/01 16:34:24 INFO mapred.LocalJobRunner: reduce > reduce\n",
      "16/02/01 16:34:25 INFO mapreduce.Job:  map 100% reduce 70%\n",
      "16/02/01 16:34:27 INFO mapred.LocalJobRunner: reduce > reduce\n",
      "16/02/01 16:34:28 INFO mapreduce.Job:  map 100% reduce 71%\n",
      "16/02/01 16:34:30 INFO mapred.LocalJobRunner: reduce > reduce\n",
      "16/02/01 16:34:31 INFO mapreduce.Job:  map 100% reduce 72%\n",
      "16/02/01 16:34:31 INFO streaming.PipeMapRed: Records R/W=65198/1\n",
      "16/02/01 16:34:33 INFO mapred.LocalJobRunner: Records R/W=65198/1 > reduce\n",
      "16/02/01 16:34:34 INFO mapreduce.Job:  map 100% reduce 73%\n",
      "16/02/01 16:34:36 INFO mapred.LocalJobRunner: Records R/W=65198/1 > reduce\n",
      "16/02/01 16:34:39 INFO mapred.LocalJobRunner: Records R/W=65198/1 > reduce\n",
      "16/02/01 16:34:42 INFO mapred.LocalJobRunner: Records R/W=65198/1 > reduce\n",
      "16/02/01 16:34:45 INFO mapred.LocalJobRunner: Records R/W=65198/1 > reduce\n",
      "16/02/01 16:34:46 INFO mapreduce.Job:  map 100% reduce 74%\n",
      "16/02/01 16:34:48 INFO mapred.LocalJobRunner: Records R/W=65198/1 > reduce\n",
      "16/02/01 16:34:49 INFO mapreduce.Job:  map 100% reduce 75%\n",
      "16/02/01 16:34:51 INFO mapred.LocalJobRunner: Records R/W=65198/1 > reduce\n",
      "16/02/01 16:34:52 INFO mapreduce.Job:  map 100% reduce 76%\n",
      "16/02/01 16:34:54 INFO mapred.LocalJobRunner: Records R/W=65198/1 > reduce\n",
      "16/02/01 16:34:55 INFO mapreduce.Job:  map 100% reduce 77%\n",
      "16/02/01 16:34:56 INFO streaming.PipeMapRed: Records R/W=99003/1173\n",
      "16/02/01 16:34:56 INFO streaming.PipeMapRed: R/W/S=100000/2049/0 in:2500=100000/40 [rec/s] out:51=2049/40 [rec/s]\n",
      "16/02/01 16:34:57 INFO mapred.LocalJobRunner: Records R/W=99003/1173 > reduce\n",
      "16/02/01 16:34:58 INFO mapreduce.Job:  map 100% reduce 78%\n",
      "16/02/01 16:35:00 INFO mapred.LocalJobRunner: Records R/W=99003/1173 > reduce\n",
      "16/02/01 16:35:01 INFO mapreduce.Job:  map 100% reduce 79%\n",
      "16/02/01 16:35:03 INFO mapred.LocalJobRunner: Records R/W=99003/1173 > reduce\n",
      "16/02/01 16:35:04 INFO mapreduce.Job:  map 100% reduce 80%\n",
      "16/02/01 16:35:06 INFO mapred.LocalJobRunner: Records R/W=99003/1173 > reduce\n",
      "16/02/01 16:35:09 INFO mapred.LocalJobRunner: Records R/W=99003/1173 > reduce\n",
      "16/02/01 16:35:10 INFO mapreduce.Job:  map 100% reduce 81%\n",
      "16/02/01 16:35:12 INFO mapred.LocalJobRunner: Records R/W=99003/1173 > reduce\n",
      "16/02/01 16:35:13 INFO mapreduce.Job:  map 100% reduce 82%\n",
      "16/02/01 16:35:14 INFO streaming.PipeMapRed: Records R/W=137582/2050\n",
      "16/02/01 16:35:15 INFO mapred.LocalJobRunner: Records R/W=137582/2050 > reduce\n",
      "16/02/01 16:35:18 INFO mapred.LocalJobRunner: Records R/W=137582/2050 > reduce\n",
      "16/02/01 16:35:19 INFO mapreduce.Job:  map 100% reduce 83%\n",
      "16/02/01 16:35:21 INFO mapred.LocalJobRunner: Records R/W=137582/2050 > reduce\n",
      "16/02/01 16:35:22 INFO mapreduce.Job:  map 100% reduce 84%\n",
      "16/02/01 16:35:24 INFO mapred.LocalJobRunner: Records R/W=137582/2050 > reduce\n",
      "16/02/01 16:35:25 INFO mapreduce.Job:  map 100% reduce 85%\n",
      "16/02/01 16:35:27 INFO mapred.LocalJobRunner: Records R/W=137582/2050 > reduce\n",
      "16/02/01 16:35:28 INFO mapreduce.Job:  map 100% reduce 86%\n",
      "16/02/01 16:35:29 INFO streaming.PipeMapRed: Records R/W=175212/3109\n",
      "16/02/01 16:35:30 INFO mapred.LocalJobRunner: Records R/W=175212/3109 > reduce\n",
      "16/02/01 16:35:31 INFO mapreduce.Job:  map 100% reduce 87%\n",
      "16/02/01 16:35:33 INFO mapred.LocalJobRunner: Records R/W=175212/3109 > reduce\n",
      "16/02/01 16:35:34 INFO mapreduce.Job:  map 100% reduce 88%\n",
      "16/02/01 16:35:36 INFO mapred.LocalJobRunner: Records R/W=175212/3109 > reduce\n",
      "16/02/01 16:35:37 INFO mapreduce.Job:  map 100% reduce 89%\n",
      "16/02/01 16:35:37 INFO streaming.PipeMapRed: R/W/S=200000/4259/0 in:2469=200000/81 [rec/s] out:52=4259/81 [rec/s]\n",
      "16/02/01 16:35:39 INFO mapred.LocalJobRunner: Records R/W=175212/3109 > reduce\n",
      "16/02/01 16:35:40 INFO mapreduce.Job:  map 100% reduce 90%\n",
      "16/02/01 16:35:42 INFO mapred.LocalJobRunner: Records R/W=175212/3109 > reduce\n",
      "16/02/01 16:35:45 INFO mapred.LocalJobRunner: Records R/W=175212/3109 > reduce\n",
      "16/02/01 16:35:46 INFO mapreduce.Job:  map 100% reduce 91%\n",
      "16/02/01 16:35:48 INFO mapred.LocalJobRunner: Records R/W=175212/3109 > reduce\n",
      "16/02/01 16:35:49 INFO mapreduce.Job:  map 100% reduce 92%\n",
      "16/02/01 16:35:50 INFO streaming.PipeMapRed: Records R/W=238907/5503\n",
      "16/02/01 16:35:51 INFO mapred.LocalJobRunner: Records R/W=238907/5503 > reduce\n",
      "16/02/01 16:35:52 INFO mapreduce.Job:  map 100% reduce 93%\n",
      "16/02/01 16:35:54 INFO mapred.LocalJobRunner: Records R/W=238907/5503 > reduce\n",
      "16/02/01 16:35:55 INFO mapreduce.Job:  map 100% reduce 94%\n",
      "16/02/01 16:35:57 INFO mapred.LocalJobRunner: Records R/W=238907/5503 > reduce\n",
      "16/02/01 16:35:58 INFO mapreduce.Job:  map 100% reduce 95%\n",
      "16/02/01 16:36:00 INFO mapred.LocalJobRunner: Records R/W=238907/5503 > reduce\n",
      "16/02/01 16:36:01 INFO mapreduce.Job:  map 100% reduce 96%\n",
      "16/02/01 16:36:03 INFO streaming.PipeMapRed: R/W/S=300000/7922/0 in:2803=300000/107 [rec/s] out:74=7922/107 [rec/s]\n",
      "16/02/01 16:36:03 INFO mapred.LocalJobRunner: Records R/W=238907/5503 > reduce\n",
      "16/02/01 16:36:04 INFO mapreduce.Job:  map 100% reduce 97%\n",
      "16/02/01 16:36:05 INFO streaming.PipeMapRed: Records R/W=313004/7923\n",
      "16/02/01 16:36:06 INFO mapred.LocalJobRunner: Records R/W=313004/7923 > reduce\n",
      "16/02/01 16:36:07 INFO mapreduce.Job:  map 100% reduce 98%\n",
      "16/02/01 16:36:09 INFO mapred.LocalJobRunner: Records R/W=313004/7923 > reduce\n",
      "16/02/01 16:36:10 INFO mapreduce.Job:  map 100% reduce 99%\n",
      "16/02/01 16:36:12 INFO streaming.PipeMapRed: MRErrorThread done\n",
      "16/02/01 16:36:12 INFO streaming.PipeMapRed: mapRedFinished\n",
      "16/02/01 16:36:12 INFO mapred.Task: Task:attempt_local1016413727_0001_r_000000_0 is done. And is in the process of committing\n",
      "16/02/01 16:36:12 INFO mapred.LocalJobRunner: Records R/W=313004/7923 > reduce\n",
      "16/02/01 16:36:12 INFO mapred.Task: Task attempt_local1016413727_0001_r_000000_0 is allowed to commit now\n",
      "16/02/01 16:36:12 INFO output.FileOutputCommitter: Saved output of task 'attempt_local1016413727_0001_r_000000_0' to hdfs://localhost:9000/user/jing/test-output1/_temporary/0/task_local1016413727_0001_r_000000\n",
      "16/02/01 16:36:12 INFO mapred.LocalJobRunner: Records R/W=313004/7923 > reduce\n",
      "16/02/01 16:36:12 INFO mapred.Task: Task 'attempt_local1016413727_0001_r_000000_0' done.\n",
      "16/02/01 16:36:12 INFO mapred.LocalJobRunner: Finishing task: attempt_local1016413727_0001_r_000000_0\n",
      "16/02/01 16:36:12 INFO mapred.LocalJobRunner: reduce task executor complete.\n",
      "16/02/01 16:36:13 INFO mapreduce.Job:  map 100% reduce 100%\n",
      "16/02/01 16:36:13 INFO mapreduce.Job: Job job_local1016413727_0001 completed successfully\n",
      "16/02/01 16:36:13 INFO mapreduce.Job: Counters: 39\n",
      "\tFile System Counters\n",
      "\t\tFILE: Number of bytes read=87184510\n",
      "\t\tFILE: Number of bytes written=131294373\n",
      "\t\tFILE: Number of read operations=0\n",
      "\t\tFILE: Number of large read operations=0\n",
      "\t\tFILE: Number of write operations=0\n",
      "\t\tHDFS: Number of bytes read=6917034\n",
      "\t\tHDFS: Number of bytes written=166520\n",
      "\t\tHDFS: Number of read operations=15\n",
      "\t\tHDFS: Number of large read operations=0\n",
      "\t\tHDFS: Number of write operations=4\n",
      "\tMap-Reduce Framework\n",
      "\t\tMap input records=31101\n",
      "\t\tMap output records=380821\n",
      "\t\tMap output bytes=42687930\n",
      "\t\tMap output materialized bytes=43588547\n",
      "\t\tInput split bytes=107\n",
      "\t\tCombine input records=0\n",
      "\t\tCombine output records=0\n",
      "\t\tReduce input groups=12012\n",
      "\t\tReduce shuffle bytes=43588547\n",
      "\t\tReduce input records=380821\n",
      "\t\tReduce output records=12012\n",
      "\t\tSpilled Records=761642\n",
      "\t\tShuffled Maps =1\n",
      "\t\tFailed Shuffles=0\n",
      "\t\tMerged Map outputs=1\n",
      "\t\tGC time elapsed (ms)=24\n",
      "\t\tTotal committed heap usage (bytes)=630194176\n",
      "\tMapper1Counter\n",
      "\t\tInstance=1\n",
      "\t\tStripes=349720\n",
      "\tReducer1Counter\n",
      "\t\tInstances=1\n",
      "\t\tPairs=12012\n",
      "\tShuffle Errors\n",
      "\t\tBAD_ID=0\n",
      "\t\tCONNECTION=0\n",
      "\t\tIO_ERROR=0\n",
      "\t\tWRONG_LENGTH=0\n",
      "\t\tWRONG_MAP=0\n",
      "\t\tWRONG_REDUCE=0\n",
      "\tFile Input Format Counters \n",
      "\t\tBytes Read=3458517\n",
      "\tFile Output Format Counters \n",
      "\t\tBytes Written=166520\n",
      "16/02/01 16:36:13 INFO streaming.StreamJob: Output directory: /user/jing/test-output1/\n",
      "16/02/01 16:36:14 WARN streaming.StreamJob: -file option is deprecated, please use generic option -files instead.\n",
      "16/02/01 16:36:15 WARN util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "packageJobJar: [mapper2.py, reducer2.py] [] /var/folders/zs/k144hqks281fbt0x68c_zj9m0000gp/T/streamjob7757838846904717631.jar tmpDir=null\n",
      "16/02/01 16:36:15 INFO Configuration.deprecation: session.id is deprecated. Instead, use dfs.metrics.session-id\n",
      "16/02/01 16:36:15 INFO jvm.JvmMetrics: Initializing JVM Metrics with processName=JobTracker, sessionId=\n",
      "16/02/01 16:36:15 INFO jvm.JvmMetrics: Cannot initialize JVM Metrics with processName=JobTracker, sessionId= - already initialized\n",
      "16/02/01 16:36:16 INFO mapred.FileInputFormat: Total input paths to process : 1\n",
      "16/02/01 16:36:16 INFO mapreduce.JobSubmitter: number of splits:1\n",
      "16/02/01 16:36:16 INFO Configuration.deprecation: mapred.reduce.tasks is deprecated. Instead, use mapreduce.job.reduces\n",
      "16/02/01 16:36:16 INFO Configuration.deprecation: mapred.output.key.comparator.class is deprecated. Instead, use mapreduce.job.output.key.comparator.class\n",
      "16/02/01 16:36:16 INFO Configuration.deprecation: mapred.text.key.comparator.options is deprecated. Instead, use mapreduce.partition.keycomparator.options\n",
      "16/02/01 16:36:16 INFO Configuration.deprecation: mapred.job.name is deprecated. Instead, use mapreduce.job.name\n",
      "16/02/01 16:36:16 INFO Configuration.deprecation: mapred.map.tasks is deprecated. Instead, use mapreduce.job.maps\n",
      "16/02/01 16:36:16 INFO mapreduce.JobSubmitter: Submitting tokens for job: job_local1144968182_0001\n",
      "16/02/01 16:36:16 INFO mapred.LocalDistributedCacheManager: Localized file:/Users/JingXu/Dropbox/DataScience/W261/W261/HW3/mapper2.py as file:/tmp/hadoop-JingXu/mapred/local/1454373376452/mapper2.py\n",
      "16/02/01 16:36:16 INFO mapred.LocalDistributedCacheManager: Localized file:/Users/JingXu/Dropbox/DataScience/W261/W261/HW3/reducer2.py as file:/tmp/hadoop-JingXu/mapred/local/1454373376453/reducer2.py\n",
      "16/02/01 16:36:16 INFO mapreduce.Job: The url to track the job: http://localhost:8080/\n",
      "16/02/01 16:36:16 INFO mapred.LocalJobRunner: OutputCommitter set in config null\n",
      "16/02/01 16:36:16 INFO mapreduce.Job: Running job: job_local1144968182_0001\n",
      "16/02/01 16:36:16 INFO mapred.LocalJobRunner: OutputCommitter is org.apache.hadoop.mapred.FileOutputCommitter\n",
      "16/02/01 16:36:16 INFO mapred.LocalJobRunner: Waiting for map tasks\n",
      "16/02/01 16:36:16 INFO mapred.LocalJobRunner: Starting task: attempt_local1144968182_0001_m_000000_0\n",
      "16/02/01 16:36:16 INFO util.ProcfsBasedProcessTree: ProcfsBasedProcessTree currently is supported only on Linux.\n",
      "16/02/01 16:36:16 INFO mapred.Task:  Using ResourceCalculatorProcessTree : null\n",
      "16/02/01 16:36:16 INFO mapred.MapTask: Processing split: hdfs://localhost:9000/user/jing/test-output1/part-00000:0+166520\n",
      "16/02/01 16:36:16 INFO mapred.MapTask: numReduceTasks: 1\n",
      "16/02/01 16:36:16 INFO mapred.MapTask: (EQUATOR) 0 kvi 26214396(104857584)\n",
      "16/02/01 16:36:16 INFO mapred.MapTask: mapreduce.task.io.sort.mb: 100\n",
      "16/02/01 16:36:16 INFO mapred.MapTask: soft limit at 83886080\n",
      "16/02/01 16:36:16 INFO mapred.MapTask: bufstart = 0; bufvoid = 104857600\n",
      "16/02/01 16:36:16 INFO mapred.MapTask: kvstart = 26214396; length = 6553600\n",
      "16/02/01 16:36:16 INFO mapred.MapTask: Map output collector class = org.apache.hadoop.mapred.MapTask$MapOutputBuffer\n",
      "16/02/01 16:36:16 INFO streaming.PipeMapRed: PipeMapRed exec [/Users/JingXu/Dropbox/DataScience/W261/W261/HW3/./mapper2.py]\n",
      "16/02/01 16:36:16 INFO Configuration.deprecation: mapred.task.id is deprecated. Instead, use mapreduce.task.attempt.id\n",
      "16/02/01 16:36:16 INFO Configuration.deprecation: user.name is deprecated. Instead, use mapreduce.job.user.name\n",
      "16/02/01 16:36:16 INFO Configuration.deprecation: map.input.start is deprecated. Instead, use mapreduce.map.input.start\n",
      "16/02/01 16:36:16 INFO Configuration.deprecation: mapred.task.is.map is deprecated. Instead, use mapreduce.task.ismap\n",
      "16/02/01 16:36:16 INFO Configuration.deprecation: mapred.tip.id is deprecated. Instead, use mapreduce.task.id\n",
      "16/02/01 16:36:16 INFO Configuration.deprecation: mapred.skip.on is deprecated. Instead, use mapreduce.job.skiprecords\n",
      "16/02/01 16:36:16 INFO Configuration.deprecation: mapred.task.partition is deprecated. Instead, use mapreduce.task.partition\n",
      "16/02/01 16:36:16 INFO Configuration.deprecation: map.input.length is deprecated. Instead, use mapreduce.map.input.length\n",
      "16/02/01 16:36:16 INFO Configuration.deprecation: mapred.local.dir is deprecated. Instead, use mapreduce.cluster.local.dir\n",
      "16/02/01 16:36:16 INFO Configuration.deprecation: mapred.work.output.dir is deprecated. Instead, use mapreduce.task.output.dir\n",
      "16/02/01 16:36:16 INFO Configuration.deprecation: map.input.file is deprecated. Instead, use mapreduce.map.input.file\n",
      "16/02/01 16:36:16 INFO Configuration.deprecation: mapred.job.id is deprecated. Instead, use mapreduce.job.id\n",
      "16/02/01 16:36:16 INFO streaming.PipeMapRed: R/W/S=1/0/0 in:NA [rec/s] out:NA [rec/s]\n",
      "16/02/01 16:36:16 INFO streaming.PipeMapRed: R/W/S=10/0/0 in:NA [rec/s] out:NA [rec/s]\n",
      "16/02/01 16:36:16 INFO streaming.PipeMapRed: R/W/S=100/0/0 in:NA [rec/s] out:NA [rec/s]\n",
      "16/02/01 16:36:16 INFO streaming.PipeMapRed: R/W/S=1000/0/0 in:NA [rec/s] out:NA [rec/s]\n",
      "16/02/01 16:36:17 INFO streaming.PipeMapRed: Records R/W=9646/1\n",
      "16/02/01 16:36:17 INFO streaming.PipeMapRed: R/W/S=10000/81/0 in:NA [rec/s] out:NA [rec/s]\n",
      "16/02/01 16:36:17 INFO streaming.PipeMapRed: MRErrorThread done\n",
      "16/02/01 16:36:17 INFO streaming.PipeMapRed: mapRedFinished\n",
      "16/02/01 16:36:17 INFO mapred.LocalJobRunner: \n",
      "16/02/01 16:36:17 INFO mapred.MapTask: Starting flush of map output\n",
      "16/02/01 16:36:17 INFO mapred.MapTask: Spilling map output\n",
      "16/02/01 16:36:17 INFO mapred.MapTask: bufstart = 0; bufend = 30700; bufvoid = 104857600\n",
      "16/02/01 16:36:17 INFO mapred.MapTask: kvstart = 26214396(104857584); kvend = 26209060(104836240); length = 5337/6553600\n",
      "16/02/01 16:36:17 INFO mapred.MapTask: Finished spill 0\n",
      "16/02/01 16:36:17 INFO mapred.Task: Task:attempt_local1144968182_0001_m_000000_0 is done. And is in the process of committing\n",
      "16/02/01 16:36:17 INFO mapred.LocalJobRunner: Records R/W=9646/1\n",
      "16/02/01 16:36:17 INFO mapred.Task: Task 'attempt_local1144968182_0001_m_000000_0' done.\n",
      "16/02/01 16:36:17 INFO mapred.LocalJobRunner: Finishing task: attempt_local1144968182_0001_m_000000_0\n",
      "16/02/01 16:36:17 INFO mapred.LocalJobRunner: map task executor complete.\n",
      "16/02/01 16:36:17 INFO mapred.LocalJobRunner: Waiting for reduce tasks\n",
      "16/02/01 16:36:17 INFO mapred.LocalJobRunner: Starting task: attempt_local1144968182_0001_r_000000_0\n",
      "16/02/01 16:36:17 INFO util.ProcfsBasedProcessTree: ProcfsBasedProcessTree currently is supported only on Linux.\n",
      "16/02/01 16:36:17 INFO mapred.Task:  Using ResourceCalculatorProcessTree : null\n",
      "16/02/01 16:36:17 INFO mapred.ReduceTask: Using ShuffleConsumerPlugin: org.apache.hadoop.mapreduce.task.reduce.Shuffle@5350266d\n",
      "16/02/01 16:36:17 INFO reduce.MergeManagerImpl: MergerManager: memoryLimit=333971456, maxSingleShuffleLimit=83492864, mergeThreshold=220421168, ioSortFactor=10, memToMemMergeOutputsThreshold=10\n",
      "16/02/01 16:36:17 INFO reduce.EventFetcher: attempt_local1144968182_0001_r_000000_0 Thread started: EventFetcher for fetching Map Completion Events\n",
      "16/02/01 16:36:17 INFO reduce.LocalFetcher: localfetcher#1 about to shuffle output of map attempt_local1144968182_0001_m_000000_0 decomp: 33372 len: 33376 to MEMORY\n",
      "16/02/01 16:36:17 INFO reduce.InMemoryMapOutput: Read 33372 bytes from map-output for attempt_local1144968182_0001_m_000000_0\n",
      "16/02/01 16:36:17 INFO reduce.MergeManagerImpl: closeInMemoryFile -> map-output of size: 33372, inMemoryMapOutputs.size() -> 1, commitMemory -> 0, usedMemory ->33372\n",
      "16/02/01 16:36:17 INFO reduce.EventFetcher: EventFetcher is interrupted.. Returning\n",
      "16/02/01 16:36:17 INFO mapred.LocalJobRunner: 1 / 1 copied.\n",
      "16/02/01 16:36:17 INFO reduce.MergeManagerImpl: finalMerge called with 1 in-memory map-outputs and 0 on-disk map-outputs\n",
      "16/02/01 16:36:17 INFO mapred.Merger: Merging 1 sorted segments\n",
      "16/02/01 16:36:17 INFO mapred.Merger: Down to the last merge-pass, with 1 segments left of total size: 33348 bytes\n",
      "16/02/01 16:36:17 INFO reduce.MergeManagerImpl: Merged 1 segments, 33372 bytes to disk to satisfy reduce memory limit\n",
      "16/02/01 16:36:17 INFO reduce.MergeManagerImpl: Merging 1 files, 33376 bytes from disk\n",
      "16/02/01 16:36:17 INFO reduce.MergeManagerImpl: Merging 0 segments, 0 bytes from memory into reduce\n",
      "16/02/01 16:36:17 INFO mapred.Merger: Merging 1 sorted segments\n",
      "16/02/01 16:36:17 INFO mapred.Merger: Down to the last merge-pass, with 1 segments left of total size: 33348 bytes\n",
      "16/02/01 16:36:17 INFO mapred.LocalJobRunner: 1 / 1 copied.\n",
      "16/02/01 16:36:17 INFO streaming.PipeMapRed: PipeMapRed exec [/Users/JingXu/Dropbox/DataScience/W261/W261/HW3/./reducer2.py]\n",
      "16/02/01 16:36:17 INFO Configuration.deprecation: mapred.job.tracker is deprecated. Instead, use mapreduce.jobtracker.address\n",
      "16/02/01 16:36:17 INFO Configuration.deprecation: mapred.map.tasks is deprecated. Instead, use mapreduce.job.maps\n",
      "16/02/01 16:36:17 INFO streaming.PipeMapRed: R/W/S=1/0/0 in:NA [rec/s] out:NA [rec/s]\n",
      "16/02/01 16:36:17 INFO streaming.PipeMapRed: R/W/S=10/0/0 in:NA [rec/s] out:NA [rec/s]\n",
      "16/02/01 16:36:17 INFO streaming.PipeMapRed: R/W/S=100/0/0 in:NA [rec/s] out:NA [rec/s]\n",
      "16/02/01 16:36:17 INFO streaming.PipeMapRed: R/W/S=1000/0/0 in:NA [rec/s] out:NA [rec/s]\n",
      "16/02/01 16:36:17 INFO streaming.PipeMapRed: MRErrorThread done\n",
      "16/02/01 16:36:17 INFO streaming.PipeMapRed: Records R/W=1335/1\n",
      "16/02/01 16:36:17 INFO streaming.PipeMapRed: mapRedFinished\n",
      "16/02/01 16:36:17 INFO mapred.Task: Task:attempt_local1144968182_0001_r_000000_0 is done. And is in the process of committing\n",
      "16/02/01 16:36:17 INFO mapred.LocalJobRunner: 1 / 1 copied.\n",
      "16/02/01 16:36:17 INFO mapred.Task: Task attempt_local1144968182_0001_r_000000_0 is allowed to commit now\n",
      "16/02/01 16:36:17 INFO output.FileOutputCommitter: Saved output of task 'attempt_local1144968182_0001_r_000000_0' to hdfs://localhost:9000/user/jing/test-output2/_temporary/0/task_local1144968182_0001_r_000000\n",
      "16/02/01 16:36:17 INFO mapred.LocalJobRunner: Records R/W=1335/1 > reduce\n",
      "16/02/01 16:36:17 INFO mapred.Task: Task 'attempt_local1144968182_0001_r_000000_0' done.\n",
      "16/02/01 16:36:17 INFO mapred.LocalJobRunner: Finishing task: attempt_local1144968182_0001_r_000000_0\n",
      "16/02/01 16:36:17 INFO mapred.LocalJobRunner: reduce task executor complete.\n",
      "16/02/01 16:36:17 INFO mapreduce.Job: Job job_local1144968182_0001 running in uber mode : false\n",
      "16/02/01 16:36:17 INFO mapreduce.Job:  map 100% reduce 100%\n",
      "16/02/01 16:36:17 INFO mapreduce.Job: Job job_local1144968182_0001 completed successfully\n",
      "16/02/01 16:36:17 INFO mapreduce.Job: Counters: 35\n",
      "\tFile System Counters\n",
      "\t\tFILE: Number of bytes read=72722\n",
      "\t\tFILE: Number of bytes written=630334\n",
      "\t\tFILE: Number of read operations=0\n",
      "\t\tFILE: Number of large read operations=0\n",
      "\t\tFILE: Number of write operations=0\n",
      "\t\tHDFS: Number of bytes read=333040\n",
      "\t\tHDFS: Number of bytes written=2660\n",
      "\t\tHDFS: Number of read operations=13\n",
      "\t\tHDFS: Number of large read operations=0\n",
      "\t\tHDFS: Number of write operations=4\n",
      "\tMap-Reduce Framework\n",
      "\t\tMap input records=12012\n",
      "\t\tMap output records=1335\n",
      "\t\tMap output bytes=30700\n",
      "\t\tMap output materialized bytes=33376\n",
      "\t\tInput split bytes=107\n",
      "\t\tCombine input records=0\n",
      "\t\tCombine output records=0\n",
      "\t\tReduce input groups=1335\n",
      "\t\tReduce shuffle bytes=33376\n",
      "\t\tReduce input records=1335\n",
      "\t\tReduce output records=52\n",
      "\t\tSpilled Records=2670\n",
      "\t\tShuffled Maps =1\n",
      "\t\tFailed Shuffles=0\n",
      "\t\tMerged Map outputs=1\n",
      "\t\tGC time elapsed (ms)=12\n",
      "\t\tTotal committed heap usage (bytes)=582483968\n",
      "\tShuffle Errors\n",
      "\t\tBAD_ID=0\n",
      "\t\tCONNECTION=0\n",
      "\t\tIO_ERROR=0\n",
      "\t\tWRONG_LENGTH=0\n",
      "\t\tWRONG_MAP=0\n",
      "\t\tWRONG_REDUCE=0\n",
      "\tFile Input Format Counters \n",
      "\t\tBytes Read=166520\n",
      "\tFile Output Format Counters \n",
      "\t\tBytes Written=2660\n",
      "16/02/01 16:36:17 INFO streaming.StreamJob: Output directory: /user/jing/test-output2/\n",
      "It takes 128 seconds to complete this task\n",
      "DEPRECATED: Use of this script to execute hdfs command is deprecated.\n",
      "Instead use the hdfs command for it.\n",
      "\n",
      "16/02/01 16:36:18 WARN util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "\t\n",
      "Top 50 product pairs:\t\n",
      "['DAI62779', 'ELE17451', 1592, 0.05118806469245362]\t\n",
      "['FRO40251', 'SNA80324', 1412, 0.04540046943828173]\t\n",
      "['DAI75645', 'FRO40251', 1254, 0.04032024693739751]\t\n",
      "['FRO40251', 'GRO85051', 1213, 0.039001961351725026]\t\n",
      "['DAI62779', 'GRO73461', 1139, 0.03662261663612103]\t\n",
      "['DAI75645', 'SNA80324', 1130, 0.03633323687341243]\t\n",
      "['DAI62779', 'FRO40251', 1070, 0.03440403845535513]\t\n",
      "['DAI62779', 'SNA80324', 923, 0.029677502331114755]\t\n",
      "['DAI62779', 'DAI85309', 918, 0.029516735796276648]\t\n",
      "['ELE32164', 'GRO59710', 911, 0.029291662647503297]\t\n",
      "['FRO40251', 'GRO73461', 882, 0.02835921674544227]\t\n",
      "['DAI62779', 'DAI75645', 882, 0.02835921674544227]\t\n",
      "['DAI62779', 'ELE92920', 877, 0.02819845021060416]\t\n",
      "['FRO40251', 'FRO92469', 835, 0.026848011317964052]\t\n",
      "['DAI62779', 'ELE32164', 832, 0.026751551397061188]\t\n",
      "['DAI75645', 'GRO73461', 712, 0.022893154560946594]\t\n",
      "['DAI43223', 'ELE32164', 711, 0.022861001253978972]\t\n",
      "['DAI62779', 'GRO30386', 709, 0.02279669464004373]\t\n",
      "['ELE17451', 'FRO40251', 697, 0.022410854956432268]\t\n",
      "['DAI85309', 'ELE99737', 659, 0.021189029291662647]\t\n",
      "['DAI62779', 'ELE26917', 650, 0.020899649528954053]\t\n",
      "['GRO21487', 'GRO73461', 631, 0.02028873669656924]\t\n",
      "['DAI62779', 'SNA45677', 604, 0.019420597408443457]\t\n",
      "['ELE17451', 'SNA80324', 597, 0.019195524259670107]\t\n",
      "['DAI62779', 'GRO71621', 595, 0.019131217645734864]\t\n",
      "['DAI62779', 'SNA55762', 593, 0.01906691103179962]\t\n",
      "['DAI62779', 'DAI83733', 586, 0.01884183788302627]\t\n",
      "['ELE17451', 'GRO73461', 580, 0.018648918041220538]\t\n",
      "['GRO73461', 'SNA80324', 562, 0.01807015851580335]\t\n",
      "['DAI62779', 'GRO59710', 561, 0.01803800520883573]\t\n",
      "['DAI62779', 'FRO80039', 550, 0.01768431883219189]\t\n",
      "['DAI75645', 'ELE17451', 547, 0.017587858911289025]\t\n",
      "['DAI62779', 'SNA93860', 537, 0.01726632584161281]\t\n",
      "['DAI55148', 'DAI62779', 526, 0.016912639464968973]\t\n",
      "['DAI43223', 'GRO59710', 512, 0.01646249316742227]\t\n",
      "['ELE17451', 'ELE32164', 511, 0.016430339860454647]\t\n",
      "['DAI62779', 'SNA18336', 506, 0.01626957332561654]\t\n",
      "['ELE32164', 'GRO73461', 486, 0.015626507186264106]\t\n",
      "['DAI85309', 'ELE17451', 482, 0.01549789395839362]\t\n",
      "['DAI62779', 'FRO78087', 482, 0.01549789395839362]\t\n",
      "['DAI62779', 'GRO94758', 479, 0.015401434037490756]\t\n",
      "['GRO85051', 'SNA80324', 471, 0.015144207581749784]\t\n",
      "['DAI62779', 'GRO21487', 471, 0.015144207581749784]\t\n",
      "['ELE17451', 'GRO30386', 468, 0.015047747660846917]\t\n",
      "['FRO85978', 'SNA95666', 463, 0.01488698112600881]\t\n",
      "['DAI62779', 'FRO19221', 462, 0.014854827819041188]\t\n",
      "['DAI62779', 'GRO46854', 461, 0.014822674512073567]\t\n",
      "['DAI43223', 'DAI62779', 459, 0.014758367898138324]\t\n",
      "['ELE92920', 'SNA18336', 455, 0.014629754670267838]\t\n",
      "['DAI88079', 'FRO40251', 446, 0.014340374907559243]\t\n",
      "rmr: DEPRECATED: Please use 'rm -r' instead.\n",
      "16/02/01 16:36:20 WARN util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "16/02/01 16:36:21 INFO fs.TrashPolicyDefault: Namenode trash configuration: Deletion interval = 0 minutes, Emptier interval = 0 minutes.\n",
      "Deleted /user/jing\n",
      "rmr: DEPRECATED: Please use 'rm -r' instead.\n",
      "16/02/01 16:36:22 WARN util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "16/02/01 16:36:22 INFO fs.TrashPolicyDefault: Namenode trash configuration: Deletion interval = 0 minutes, Emptier interval = 0 minutes.\n",
      "Deleted /user\n"
     ]
    }
   ],
   "source": [
    "!./mapreduce.sh"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Single computer dual core, 1 mapper, 1 reducer**\n",
    "\n",
    "**It takes 128 seconds to complete this task**\n",
    "\n",
    "**Mapper1Counter**\n",
    "\n",
    "    Instance=1\n",
    "\tPairs=349720\n",
    "\n",
    "**Reducer1Counter**\n",
    "\n",
    "\tInstances=1\n",
    "\tPairs=12012\n",
    "\n",
    "**Top 50 product pairs:**\n",
    "\n",
    "['DAI62779', 'ELE17451', 1592, 0.05118806469245362]\t\n",
    "['FRO40251', 'SNA80324', 1412, 0.04540046943828173]\t\n",
    "['DAI75645', 'FRO40251', 1254, 0.04032024693739751]\t\n",
    "['FRO40251', 'GRO85051', 1213, 0.039001961351725026]\t\n",
    "['DAI62779', 'GRO73461', 1139, 0.03662261663612103]\t\n",
    "['DAI75645', 'SNA80324', 1130, 0.03633323687341243]\t\n",
    "['DAI62779', 'FRO40251', 1070, 0.03440403845535513]\t\n",
    "['DAI62779', 'SNA80324', 923, 0.029677502331114755]\t\n",
    "['DAI62779', 'DAI85309', 918, 0.029516735796276648]\t\n",
    "['ELE32164', 'GRO59710', 911, 0.029291662647503297]\t\n",
    "['FRO40251', 'GRO73461', 882, 0.02835921674544227]\t\n",
    "['DAI62779', 'DAI75645', 882, 0.02835921674544227]\t\n",
    "['DAI62779', 'ELE92920', 877, 0.02819845021060416]\t\n",
    "['FRO40251', 'FRO92469', 835, 0.026848011317964052]\t\n",
    "['DAI62779', 'ELE32164', 832, 0.026751551397061188]\t\n",
    "['DAI75645', 'GRO73461', 712, 0.022893154560946594]\t\n",
    "['DAI43223', 'ELE32164', 711, 0.022861001253978972]\t\n",
    "['DAI62779', 'GRO30386', 709, 0.02279669464004373]\t\n",
    "['ELE17451', 'FRO40251', 697, 0.022410854956432268]\t\n",
    "['DAI85309', 'ELE99737', 659, 0.021189029291662647]\t\n",
    "['DAI62779', 'ELE26917', 650, 0.020899649528954053]\t\n",
    "['GRO21487', 'GRO73461', 631, 0.02028873669656924]\t\n",
    "['DAI62779', 'SNA45677', 604, 0.019420597408443457]\t\n",
    "['ELE17451', 'SNA80324', 597, 0.019195524259670107]\t\n",
    "['DAI62779', 'GRO71621', 595, 0.019131217645734864]\t\n",
    "['DAI62779', 'SNA55762', 593, 0.01906691103179962]\t\n",
    "['DAI62779', 'DAI83733', 586, 0.01884183788302627]\t\n",
    "['ELE17451', 'GRO73461', 580, 0.018648918041220538]\t\n",
    "['GRO73461', 'SNA80324', 562, 0.01807015851580335]\t\n",
    "['DAI62779', 'GRO59710', 561, 0.01803800520883573]\t\n",
    "['DAI62779', 'FRO80039', 550, 0.01768431883219189]\t\n",
    "['DAI75645', 'ELE17451', 547, 0.017587858911289025]\t\n",
    "['DAI62779', 'SNA93860', 537, 0.01726632584161281]\t\n",
    "['DAI55148', 'DAI62779', 526, 0.016912639464968973]\t\n",
    "['DAI43223', 'GRO59710', 512, 0.01646249316742227]\t\n",
    "['ELE17451', 'ELE32164', 511, 0.016430339860454647]\t\n",
    "['DAI62779', 'SNA18336', 506, 0.01626957332561654]\t\n",
    "['ELE32164', 'GRO73461', 486, 0.015626507186264106]\t\n",
    "['DAI85309', 'ELE17451', 482, 0.01549789395839362]\t\n",
    "['DAI62779', 'FRO78087', 482, 0.01549789395839362]\t\n",
    "['DAI62779', 'GRO94758', 479, 0.015401434037490756]\t\n",
    "['GRO85051', 'SNA80324', 471, 0.015144207581749784]\t\n",
    "['DAI62779', 'GRO21487', 471, 0.015144207581749784]\t\n",
    "['ELE17451', 'GRO30386', 468, 0.015047747660846917]\t\n",
    "['FRO85978', 'SNA95666', 463, 0.01488698112600881]\t\n",
    "['DAI62779', 'FRO19221', 462, 0.014854827819041188]\t\n",
    "['DAI62779', 'GRO46854', 461, 0.014822674512073567]\t\n",
    "['DAI43223', 'DAI62779', 459, 0.014758367898138324]\t\n",
    "['ELE92920', 'SNA18336', 455, 0.014629754670267838]\t\n",
    "['DAI88079', 'FRO40251', 446, 0.014340374907559243]\t"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
